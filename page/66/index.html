<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/66/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/66/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Introduction-to-Azure-Cosmos-DB-23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Introduction-to-Azure-Cosmos-DB-23/" class="post-title-link" itemprop="url">AZ-204-Introduction-to-Azure-Cosmos-DB-23</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:18:10" itemprop="dateCreated datePublished" datetime="2022-11-14T11:18:10-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 00:42:40" itemprop="dateModified" datetime="2022-11-15T00:42:40-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Introduction-to-Azure-Cosmos-DB-23/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Introduction-to-Azure-Cosmos-DB-23/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Greetings. Welcome to Cloud Academy’s course on Microsoft Cosmos DB. I’m delighted to have you join me on what is bound to be an educational and delightful adventure into the world of database technology. First, I’ll let you know a little bit about myself before I get into the course outline. My name is Jonathan. I’m one of the course developers with Cloud Academy. I work professionally as a technical consultant specializing in DevOps, data engineering, and security. Long ago in another life, I was a public school teacher, so I love educating people and I’m thrilled to be doing it again, only now, with technology. </p>
<p>This course is designed to be very practical. It’s meant for technology professionals, DevOps engineers, data architects, CTOs, etc, with the goal of helping them get a solid understanding of Cosmos DB in a short time. So, what exactly are the prerequisites then for a course like that? Well, we’re going to be doing a pretty deep dive on the Cosmos DB architecture and its varying data models. For that reason, we recommend that people have some basic knowledge of database technologies. You should also have at least a passing familiarity with <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a> and cloud hosting generally. So, when I talk about how an Azure service can talk to Cosmos DB, I’m going to assume that the students know what that means at a high level. In short, if you have any technical experience working with cloud providers and databases, you’ll probably be fine. If this is your first time learning about databases, then maybe you’ll struggle a bit.</p>
<p>So, what are the exact learning objectives for a course like this? Well, I’ve narrowed it down to three big ones that will guide each section. Objective one, is to ensure that the student gains a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/intro-azure-cosmos-db/">basic understanding of the Cosmos DB</a> technology, including its feature set and design philosophy and a little bit of history too. Objective two, is to teach students how to use Cosmos DB, via its APIs, Microsoft CLI tools, and the Azure web console. Finally, objective three, is to give students a practical understanding of how to integrate Cosmos DB with other Azure services with the goal of creating a working app. So in short, what is Cosmos DB, how do we use it, and then concrete example. So, lastly, I would like to encourage everyone to leave feedback. Email <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> if you have any questions, comments, suggestions, or concerns. We always appreciate people taking the time to provide feedback. So now without further ado, let’s get started.</p>
<h1 id="Introduction-to-Cosmos-DB"><a href="#Introduction-to-Cosmos-DB" class="headerlink" title="Introduction to Cosmos DB"></a>Introduction to Cosmos DB</h1><p>The fact that you’re here with me taking this course implies that you know, or at least care a little bit about data based technology. If that’s the case, then you know there are a lot of different products out there. There are relational databases like PostgreSQL and MySQL as well. You have no sequel systems like MongoDB or key value stores, collum databases, things like Cassandra, numerous enterprise offerings from big companies. Things like Amazon’s DynamoDB. So with such a crowded ecosystem, the first question anyone would rightly ask about Cosmos DB is what makes it so special? Before we try to tackle that question, let’s talk a little history.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/introduction/">Cosmos DB</a> is actually an extension of Microsoft’s DocumentDB which was released back in 2014. Microsoft had been developing Cosmos DB since 2010 and in 2017, it officially migrated all DocumentDB users to Cosmos DB. Cosmos DB supports the same SQL-like API as DocumentDB as well as several other APIs and features. Now according to <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a>‘s own documentation, Cosmos DB is best described as a globally distributed multi-model database. Now the two descriptive terms there are key. </p>
<p>Globally distributed and multi-model, that should already give you an idea of the problems Cosmos DB is meant to solve. For those of you familiar with CAP theory, C-A-P CAP theory, you could infer that availability is one of the chief guarantees. This is critical because achieving reliable database performance across multiple geographic regions is very difficult, especially for relational databases. So what do we mean when we say that Cosmos DB is a multi-model database? When we use the term model when talking about databases, we’re referring to how the data is actually stored and what sort of APIs are used for clients to read and write data. So for example, Cassandra has a certain API and storage paradigm that makes it more suitable for some use cases than others. MongoDB and PostgreSQL have their own models. So what does it mean to say that Cosmos DB is multi-model? Well, as you might guess, this simply means that with Cosmos DB, you have the flexibility to use a variety of different APIs for different use cases. In fact, the available models for Cosmos DB come from well known database technologies. Cosmos DB supports Table API, Cassandra, SQL, MongoDB, and Gremlin. </p>
<p>There are STKs available in multiple languages for clients. Cosmos DB is sort of like DynamoDB in that it is a database as a service. That is, you don’t install it on your own server and configure it like you would with Cassandra or Postgres. Rather, what you do is you sign up for access through Azure, and behind the scenes Microsoft handles the scaling. Microsoft Azure offers 99.999% uptime SLA to ease concerns about entrusting your precious data to them. Still, it is important to keep in mind the relative lack of control you have if you rely on Cosmos DB. It isn’t like a typical database system where you can SSH to the server and tune things the way you want. This is understandably a deal breaker for some people. On the flip side, as a former DynamoDB user, I can say that it can be a really big boon for a small startup to have the reliability and performance guarantees of a large company like Microsoft backing your app. </p>
<p>It lets you focus on your business logic and scale very quickly. So, that’s it in a nutshell. Cosmos DB is a powerful database as a service system designed to support a variety of data models and work across multiple geographic regions. In the next lesson, we’ll dive into the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/cosmos-db-features-and-capabilities/">feature set</a> and talk about how companies actually might use Cosmos DB. See you there.</p>
<h1 id="Cosmos-DB-Features-and-Capabilities"><a href="#Cosmos-DB-Features-and-Capabilities" class="headerlink" title="Cosmos DB Features and Capabilities"></a>Cosmos DB Features and Capabilities</h1><p>For this section we’re gonna focus on Cosmos DB’s unique capabilities. We are not going to cover literally every single thing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/introduction/">Cosmos DB</a> can do. There is a lot of overlap with other database technologies and quite frankly if you just want a feature list you’re better off reading their documentation, linked below. Instead, in this lesson, we’re going to focus on the six most important and compelling capabilities unique to Cosmos DB. Those six features are, in order, global distribution of data, serverless architecture, multi-model support, throughput consistency guarantees, partitioning, and security. Let’s start by talking about Cosmos DB multi-region support. This is one of the main reasons enterprises choose to use Cosmos DB. The fact that it is designed from the ground up to support access patterns from all over the planet. With over 50 geographic locations for its data centers, Cosmos DB users can ensure minimal latency for their users. What’s more, new locations are regularly added each year. Multi-region logic is deeply integrated into the Cosmos DB service. As the user, you can associate as many geographic regions with your data as you want.</p>
<p>You can tune consistency levels for read and write operations, to improve availability or data precision. You can set entire regions as read only, write only, or read-write. Furthermore, you get built-in failover that lets your set priority lets you set priorities for each region, so you can decide what happens if one of your US data centers goes down, for example. You can plan for exactly which regions take precedence and how you will recover. In section two, we’ll go into how all of these geolocation features are used via their Cosmos DB rest API and web console. The next key thing to introduce is Cosmos DB’s serverless architecture. As described previously, Cosmos DB is an example of a database as a service. You do not set up database servers and manage them, instead you just get an endpoint for your app to utilize. The currency for making use of the Cosmos DB endpoint is known as request units. This will determine how much you pay and what sort of performance guarantees you can get. The larger your data, the more frequent your queries, the more indexing you do, the more consistency you demand, the larger the number of request units you will need. The nice thing about this system is it greatly simplified your data layer. You don’t need to think about memories, CPU, hardware provisioning, OS optimization, updates and patches, SSL certs, et cetera, et cetera. All of this operational overhead of managing a database is gone. The time saved alone could translate into more than enough cost savings to offset the cost of needed request units. Cosmos DB’s serverless architecture also ensures strong SLAs. You get a guarantee of 99.999% uptime, far better than what a typical tech company achieves on their own. You’ll also get first order integration with other <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> services and great support. So let’s move on and talk about the multi-model data support. </p>
<p>This I think is perhaps Cosmos DB’s most intriguing feature. Cosmos DB offers APIs for Cassandra, Gremlin, MongoDB, SQL, and Table Key-Value API. This means that with a single Cosmos DB account, you can run multiple database engines. So if a portion of your data is best suited for Cassandra, you can set up Cassandra key spaces. And then if a subset of your data needs a document paradigm, you can use Mongo. If you happen to need a graph database and a relational database as well, you can add them as well using Gremlin and SQL. This means you have the flexibility of using the right data model for the job. It means you can easily migrate an existing heterogeneous data architecture into Cosmos DB with little hassle. Now there are some important trade-offs and restrictions that come from using multiple APIs. We’ll dig into that in later sections. Next, let’s talk about throughput and consistency. As far as the cap theorem goes, Cosmos DB is very strong on partition tolerance and availability. Like Cassandra, consistency is tunable and throughput is a function of your request units and consistency settings. Cosmos DB features five different consistency settings. </p>
<p>In order from strongest to weakest guarantees, they are strong, bounded staleness, session, consistent prefix, and eventual. If you need to ensure that always the most recent data is read, choose the strong consistency level. It ensures that no reads are processed until rights are completed durably by a quorum of replicas. With bounded staleness, you get a configurable level of consistency. Reads will lag behind writes by either an adjustable time interval or a number of item revisions. Then there’s the session consistency level. This gives you a read your own writes guarantee suitable for scenarios where you need guarantees at the level of individual clients. It’s considerably cheaper than bounded staleness and strong consistency levels, but you get no consistency guarantee outside of individual client session. And then lastly you have the consistent prefix and eventual consistency levels. Both of these guarantee that your data will eventually converge to the most recently written. With the consistent prefix, at least you get an additional guarantee that data will never be out of order. So even if you don’t get the most recent data on read, you can at least be sure that you’re not skipping over data inadvertently. Both of these consistency levels allow for fast throughput and are relatively inexpensive. The more inconsistency you can tolerate, the more you can save money on request unit usage. </p>
<p>Let’s now talk about partitioning and indexing a bit. In Cosmos DB, there are physical partitions that compromise compute hardware resources. SSD storage, CPU, memory, logical partitions, which are a subset of the physical ones. In other words, a physical partition may be made up of several logical partitions. The basic abstraction for sets of data is a container. A container in Cosmos DB can span multiple physical partitions and will be responsible for storing your collections, graphs, SQL tables, et cetera. Every document in Cosmos DB is uniquely identifiable by the combination of its partition key and row key. The partition key, specifically, acts as a logical partition for your data and helps to create boundaries to enable cosmos DB to map data to specific physical resources. In Cosmos DB, the data for a single logical partition must reside on a single physical partition. When designing collections of data, two critical things to think about are partition key and indexing. On the latter point, Cosmos DB automatically indexes all of your data. However it’s possible to create custom indexing policies that let you tune trade offs between query throughput and consistency. Now regarding partition keys however, you’ll have to think carefully about the nature of your data to decide on a proper partition key. People coming from the Cassandra world will have some good intuition here. </p>
<p>The main thing you want is a column with high cardinality and a large variety of values to help distribute your workloads evenly. See Azure’s best documentation for more details. We will cover both of the indexing and partition key selection in more depth in section three of this course when we get into the practical application. And finally let’s talk a little about security. Now as a cloud-based service, Cosmos DB has many of the same security considerations as any other provider. You need to control who has access to your Azure account, who has credentials to use the API and ensure that sensitive data is properly isolated. The nice thing about Cosmos DB though is that it has very sane defaults when it comes to security. Without taking any action at all, Cosmos DB encrypts all data both at rest and in transit. Cosmos DB supports HTTPS, TLS for all client to server interactions. It also includes two different types of credentials for different use cases, master keys, and resource tokens. </p>
<p>The former are useful for administrators that need to make significant changes, while the latter can be used by clients with narrower needs. So there you have it, consider this your crash course into the world of Cosmos DB. For a more detailed breakdown, please take a look at the Cosmos DB documentation, it will answer many of your questions. Our goal in the following two sections is to go beyond the documentation and get you to actually use Cosmos DB on your own. In section two, we’ll dig into how to set up and utilize Cosmos DB futures that were described here and in section three we’ll walk through <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/introduction-to-using-cosmos-db/">creating an actual software service</a>. Good luck and see you there.</p>
<h1 id="Introduction-to-Using-Cosmos-DB"><a href="#Introduction-to-Using-Cosmos-DB" class="headerlink" title="Introduction to Using Cosmos DB"></a>Introduction to Using Cosmos DB</h1><p>In this section, we will get a little more hands-on with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/introduction/">Cosmos DB</a>. We’re gonna learn how to actually start using Cosmos DB via the web console in various APIs. This is not a lab, so you’re not required to actually follow all of the stops. However, if you have access to an <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> account, I would suggest setting up Cosmos DB and actually trying some of what we cover on your own. We’ll start with just the initial setup via the web console. We will create a Cosmos DB account, select a data model API, and show how to manage our data via the browser. This will be a good opportunity to review several key <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/cosmos-db-features-and-capabilities/">features</a>. Next, we’ll explain how to use libraries for languages like Python and .NET to interact with our Cosmos DB storage. We will also cover using CLI tools, Powershell, and the REST API for Cosmos DB. By the time you’re done with this section, you should have a theoretical knowledge, enough to dive in and do some real work. So if you’re ready, let’s <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/getting-started-with-cosmos-db/">get started</a>.</p>
<h1 id="Getting-Started-with-Cosmos-DB"><a href="#Getting-Started-with-Cosmos-DB" class="headerlink" title="Getting Started with Cosmos DB"></a>Getting Started with Cosmos DB</h1><p>Section two, part two, Getting Started with Cosmos DB. The first and most obvious prerequisite for using Cosmos DB is having a Microsoft Azure account. Once that’s ready, you can go ahead and create a Cosmos DB account. This is as simple as clicking Create a Resource in the top left of the web UI and then clicking on Databases or just starting a search for Cosmos DB by typing that in.</p>
<p>You will then be prompted to input some basic information about your new cosmos DB resource, a name to uniquely identify it with your account, as well as a resource group name, a region, and potentially an option for replica failover region. Also, most important, you’ll have to select your API type. This is what’s gonna define the data model as we said and recall that you can select from MongoDB, Cassandra, SQL, and Gremlin and Azure table.</p>
<p>Now, once everything is filled in click create and wait for a few minutes. It’ll validate and your account will be generated. Now, remember at this point you have a Cosmos DB account resource that’s ready to use but there’s no actual database at this point. What we wanna do next is to use the web console to define a database and then a collection or a table to which we could write some data.</p>
<p>Now before we walk through the steps to do that it’s important to clarify some of the nomenclature here. Different database engines use different terms to describe database schema. So for example, Cassandra uses terms like keyspaces and column families while Mongo uses terms like collections and documents, and in SQL generally, you have a database that has tables and then a row within a table to describe an individual item and that’s kind of your hierarchy.</p>
<p>So depending on the API we pick, we’ll actually see different terms used in the web UI. Now, using the core SQL API we’re gonna see terms like database and container as our general terms. The latter container, this refers to the units of scalability and are analogous to MongoDB collections. So, the process for actually setting up our database in the web console goes something like this.</p>
<p>In the Cosmos DB dashboard we wanna select the Data Explorer option on the left menu. Now we could also start by selecting Add Container in the top menu; however, going through the data explorer makes what we’re doing a little bit more transparent and obvious.</p>
<p>So, in the data explorer UI, we can do all sorts of typical database tasks such as executing queries and changing configuration. Now once you’re in the data explorer page simply click on the New Database button to define your first database. All you need to fill in is a name and a number of RUs, request units, and remember that the RU value will define the kind of throughput performance you can expect.</p>
<p>Now once you have a database, its name should appear in the left side menu, the one that’s to the right of the leftmost menu with the other Cosmos DB options. So click on the little three dots next to your database name and you’ll get an option there to create a container.</p>
<p>So you give your collection a name, you give it an ID, and you give it a partition key, which should be optional, and then you’re done. You’ve got a database and you’ve got a container within your database. Now, that’s just the basic setup and with that clarified let’s step back for a minute and try to fully understand how request units work, because this determines our throughput and it determines our cost to a large extent. </p>
<p>How many request units should we set for our account? What is the formula? Azure offers some really good general advice in the <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-gb/azure/cosmos-db/request-units">documentation</a>. So of course, be sure to check out the <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-gb/azure/cosmos-db/request-units">links</a>. They also offer a nice request unit calculator tool where you can upload a sample JSON document and input some parameters such as reads and deletes, updates per second, kind of your expected use case, and then the calculator will spit out an estimated optimal value for how many RUs you need.</p>
<p>So, without using that, the basic formula for determining RUs is to multiply the number of desired reads per second by the average item size in kilobytes and then perform the same calculation for desired writes per second and then add those two numbers.</p>
<p>So, for example, if your item size is two kilobytes on average and you need 300 reads per second and 500 writes per second you would get 600 and 1,000 by multiplying those two numbers by two, two kilobytes. And therefore in that scenario you add those two numbers together, you should opt for roughly 1,600 RUs, okay?</p>
<p>Now, there are a few important other variables to consider here; document indexing, complexity of query patterns, and critically the desired consistency level. Now remember there’s five; strong, bounded staleness, session, consistent prefix, and eventual. All of these parameters will have an impact on the proper RU setting. So for this reason we strongly recommend taking a look at the documentation and testing out the RU calculator but do keep in mind the formula above. And that’s basically it.</p>
<p>That’s it for this lesson. Congrats, you now know how to set up the Cosmos DB account, the endpoint, and also calculate RUs, all of this using the GUI. You can create your basic config, you can execute queries, you can add arbitrary data. You’re ready to handle basic maintenance for things like request units and other config. Very magical stuff, but of course we can’t stop there. Unless you have a very unusual app, you probably want software to handle querying your database instead of humans. So, in the next lesson, we will learn all about the Cosmos DB API and how you can get computers to do all of this stuff for you. See ya there, space cowboy.</p>
<h1 id="Cosmos-DB-API"><a href="#Cosmos-DB-API" class="headerlink" title="Cosmos DB API"></a>Cosmos DB API</h1><p>So we’ve got our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/introduction/">Cosmos DB</a> account set up. We can select a desired data API, create databases, write arbitrary data, and execute queries all from the browser. We’re ready to administer our data intelligently. Swell right? Well, in all likelihood you’ll not be doing all of your database reading and writing manually via GUI. You need applications and tools to be able to talk to Cosmos DB. You may want your web app to serve requests to Cosmos DB or you may have devops scripts that have to execute Cosmos DB queries. This is where the Cosmos DB API comes into play. The API documentation is divided by data model, meaning there are separate docs for a Cassandra API, MongoDB, Gremlin, SQL, Azure Table. When you select one you will get information for how to set up your development environment depending on your language choice. Cosmos DB has strong support for many languages including .NET, Java, Python, Ruby, JavaScript. The best way to get started is to look through the SDK documentation for the language of your choice. So for example, let’s say we wanted to use Cosmos DB’s SQL API with, oh I don’t know, how about Java. </p>
<p>The documentation will link us to a GitHub repo for the Java Cosmos DB SDK. There you’ll find a nice README and walkthrough to help you set up the SDK with your Java IDE and begin writing code that could interact with Cosmos DB. Now in some cases you won’t use a Cosmos DB SDK specifically, but rather a driver meant for whatever data model API you selected. So, for example, if we go through the Cosmos DB Python quick start guide for a Cassandra API server, the setup documentation links to the Python Cassandra driver GitHub repo. The Python code you write for Cosmos DB in this instance will be just like Python code for a Cassandra app. You will import the Python Cassandra driver and use its methods to connect, write, and query your Cosmos DB. This flexibility, I think, is one of the most powerful things about Cosmos DB. What it does is it makes it so that if you already have expertise in one database technology, like Mongo or SQL, you can leverage that and often reuse the same code in SDKs. The only new overhead is ensuring your applications have the right permissions and authorization to actually connect to your Cosmos DB account. We’ll drill down about that in section three when we build a real service. Furthermore, with Cosmos DB we have more than just SDKs, and in the web console we have CLI tools and a REST API and Powershell. </p>
<p>Using the Azure CLI tool we can do all of the things we did with the web console in the previous lessons. Now see the links below for the complete command reference. We’re not gonna go through every command. There is also a nice set of BASH scripts in that link that make use of the CLI tool to set up and manage your databases. They have them throughout the Azure documentation. With Powershell we can do everything from the CLI as well as scripting. So if you’re comfortable writing Powershell code you can do many of the same things as you would in BASH scripts, only it’ll probably require fewer lines of code. And then finally you have the REST API. This lets you do everything your own way with any programming language that can make HTTP requests or you can just use curl, if it pleases you. So as we can see from this section, Cosmos DB is both very powerful and very flexible. We can do most of our setup, configuration, and maintenance from the web console or using command line tools. When it comes time for our software to read and write data we have options for many languages and workarounds for unusual use cases. All of this surely sounds great, and by now you have enough theoretical understanding to dive in, build your own little app with Cosmos DB. However, to help ensure you start strong in section three we’re gonna walk through <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/app-creation-with-cosmos-db/">building an application</a>, using <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> services, and Cosmos DB. It’s gonna be a blast, see you there.</p>
<h1 id="Introduction-to-Creating-an-App-with-Cosmos-DB"><a href="#Introduction-to-Creating-an-App-with-Cosmos-DB" class="headerlink" title="Introduction to Creating an App with Cosmos DB"></a>Introduction to Creating an App with Cosmos DB</h1><p>By now, you should have a pretty strong theoretical understanding of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/introduction/">Cosmos DB</a>. We walked through its feature set, explained its <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/azure-cosmos-db-api/">API</a>, demonstrated its tooling and generally covered how the technology can serve as a backend for a variety of use cases. What we have not done is actually tried to build something so that we can see all this cool stuff in action. In this final section of the course, we’re gonna do just that. We’re gonna stop being polite and start getting real. </p>
<p>This isn’t a Cloud Academy lab, so you won’t actually need to create anything in a <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a> account to follow along. Instead, we’ll just very carefully cover step-by-step how to create a working application backend using Cosmos DB. Now, to keep things simple, we’re not gonna create a frontend. The app we create will be a simple event processing system. It’ll take in event data using Azure Event Hub, transmit the events to an Azure Function where we can perform transformations, and then finally, save everything to Cosmos DB. The setup for Azure Event Hub and Azure Functions will be minimal. We will include some screenshots and explanation, but we won’t focus too much on those systems since they’re out of scope for this class. </p>
<p>There are other classes if you wanna dig into those tools. You’re welcome to skip those short lessons if you don’t feel like they’re of use to you. The meat of this section will be the creating our app backend and validating our app lessons where we will walk through the Cosmos DB endpoint setup and then run some test data through <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/azure-event-ingestion/">Event Hub</a> to see if it actually persisted in Cosmos DB. Again, you don’t actually need to do any of this yourself, but if you have access to an Azure account, it’s a great way to reinforce all that we have covered. So if you’re ready, let’s get to it.</p>
<h1 id="Event-Ingestion-in-Azure"><a href="#Event-Ingestion-in-Azure" class="headerlink" title="Event Ingestion in Azure"></a>Event Ingestion in Azure</h1><p>Okay. Welcome to our Cloud Academy demo for this course on Cosmos DB in Azure. And now for this demo section, there’ll be multiple videos and we’re going to put together a backend application using Azure Event Hubs as your functions and as your Cosmos DB.</p>
<p>So for this first video, we’re gonna start by creating an Event Hub. Now what you see here on the screen, this is my personal account. There’s some test infrastructure here. That’s pretty recent. And we’re gonna run through how to create this piece, the Event Hub.</p>
<p>Now, if we just click Event Hubs, we’ll see here it takes us first to this section, which these are namespaces. You have to first create a namespace for your Event Hub. So if you click on it here, we have this Bethune namespace. This is my last name. And then there is a bethune Event Hub, and we can create click plus to create an additional Event Hub within that. But let’s start from scratch.</p>
<p>Let’s say we don’t have any namespace at all. We can go through the, create a resource menu here. And if you want, you can actually just type Event Hubs if you can’t find it and you can click create, and it’ll take you to the wizard and it will tell you first to create a namespace.</p>
<p>Now there’s some information you fill out here. We’ll stick with pay as you go, resource group, we have an existing one, but let’s say we wanna create a new one. We’ll call it, let’s create a new one and we’ll call it bethune2 and so we have a new resource group that will be for this namespace. And then the namespace name we’ll also just call that bethune2.</p>
<p>We wanna pick a region for the data center. I think East US is fine. We’ll stick with the standard pricing tier we can go with, let’s say one consumer group is enough for us and throughput units one is fine for a test. And then we can just go to review and create.</p>
<p>Now after you click review and create, it’s not actually gonna create it right away, it’s gonna just do validation. So don’t navigate away, it’s just done the validation piece. So we know that this configuration is okay, and then we have to click create. And now it’s actually deploying the infrastructure for our Event Hub namespace. That doesn’t take very long, but we have to, we’ll give it a minute and we’ll refresh. Okay.</p>
<p>So eventually your deployment for the Event Hub namespace will complete. It’ll get to this point and we can just click on, go to resource and we can see here at bethune2 Event Hub namespace, and we can navigate there from the home menu as well.</p>
<p>If we click on Event Hubs and we’ll see there’s the additional namespace right there, it’s still in status activating, but that’ll be done very soon. So at this point, we don’t have an actual Event Hub yet. We just have a namespace. So to create an Event Hub, we click on the namespace bethune2 that we created and we want to do we wanna add an Event Hub we do plus Event Hub. And this is pretty simple as well. We just have to give it a name. We’ll call this as well bethune2 partition count of two is fine. Just for test purposes. We don’t need to worry about message retention or capture at this point.</p>
<p>So we’ll just click create and that’ll trigger the validation. And it’ll take a second, but eventually, we’ll see spin up right there bethune2. Down there, we can see the Event Hub. These create very, very quickly, but after you’ve done that, congrats, you’re done. You have your ready-to-use Event Hub right here. No messages or traffic yet, but we’ll get to that soon enough.</p>
<h1 id="Creating-our-App-Backend-with-Cosmos-DB"><a href="#Creating-our-App-Backend-with-Cosmos-DB" class="headerlink" title="Creating our App Backend with Cosmos DB"></a>Creating our App Backend with Cosmos DB</h1><p>Okay, so now in this next demonstration, we’re gonna go through the process of setting up an Azure Cosmos DB account. Now that we have a new event hub, namespace, and event hub, we need a backend to save data. And in the final section, we’ll do. In the section after this, we’ll talk about Azure functions, something to actually process data.</p>
<p>So we have here a sample account, bethune. Now I’ll walk through the steps of how we created it. Similarly, we can do the Create a Resource wizard if we want, and we can just search for Cosmos DB. And so we click on create, and again, we just go, to the fill in the information. We’re gonna go through the same resource group we just created before, bethune2, we’ll say pay as you go and a name for the account. Let’s just keep it as bethune2.</p>
<p>API, this is very important. As you know, Cosmos DB can support many different data structures and API types. So we can do Cassandra. We can do Gremlin. We’re going to just stick with the SQL for this demo. We’ll keep it, keep it simple.</p>
<p>We’ll do the provision throughput capacity mode here. There’s a serverless version in preview mode as of this recording but we’ll stick with the provision for the location. Let’s keep it in the same place as our event, hub US East. And then there’s some additional configuration options here. We don’t really need any of this stuff. Multi-region geo-redundancy. This is, if you want a greater, greater resilience and more robust deployment, we don’t really need to worry about this stuff.</p>
<p>So review and create, and as before it’ll do a validation first. So it’s valid. There’s nothing wrong with this config. We then click create, and this will just take a moment to deploy. So the account is in progress and it’s important to remember once this is done, you don’t have a database yet. You just have the account. So you have to actually create the actual schema, the collections and, you know, DB instances manually, or you can actually have your software created. You can have an Azure function, or you could submit a SQL query. There’s many ways to, to create the schema afterward, but here we can see a deployment is in progress. </p>
<p>Usually only takes about a minute or so, and we can see, well, if we click refresh, we’ll see if it’s making any progress. Alright, well, let’s, let’s give this a sec. We’ll come back when this is done.</p>
<p>Okay. So it took a few minutes longer than I thought, but it did eventually complete. We have here our Azure Cosmos DB bethune2 account, which successfully finished. So what we can do is now actually look at the data, although there’s no data there, I’ll, I’m gonna show you how we can create a database, which will be our, our storage for our application.</p>
<p>So there’s the bethune1 here, which was done for test purposes. This is bethune2 account that we just created. Now, if we go into it, there’s nothing there right now. So to create a database within the account, there’s a lot of ways you can do that. You can get with the API, you can get with it with the query. We’re going to go through the Data Explorer piece here. And in this window, you can see, they give us a few options. You can create a container, which is a sort of a fixed storage and then throughput unit. Or you can go through here to create the database directly, which will give us our initial schema. And remember, this is all SQL API.</p>
<p>So what we’ll do is we’ll, we’ll just create a database to start. We’ll give it a unique ID to call it. We’ll call it the database bethunetest, just to give it a slightly different name. You know, the, the throughput is fine. We can pick the default. We don’t really need to worry about scaling or anything right now. So we’ll just click okay for that. And in a few seconds we should have our database backend. Good to go. Fetching. Okay. There it is.</p>
<p>Now that’s coming up and we can see there’s no data in there. Now, if we go to this older one that does have data, we can see what it would look like. What, what we should hope to get. This one, bethune, has working data in it. I believe it’s under the bethune1 collection. Yeah, here we go.</p>
<p>So you can see here items. If you do have data, you’ll see, you know, a separate collection and you’ll see some items that will load. So this one has one unit there and this one, oh, this one actually has more of than this one under Bethune has more items, these message items here. So we’ll know that we’ve succeeded with our app. When we can go back to this one here that we just created, bethune2, and we should see a collection and items.</p>
<p>So in order to do that, we need to create an answer function, to process some data. And then we’re going to send some messages that will be captured by the event hub processed by the function and stored in our Cosmos DB backend. So next up is, Azure function, see you there.</p>
<h1 id="Azure-Functions-and-Cosmos-DB"><a href="#Azure-Functions-and-Cosmos-DB" class="headerlink" title="Azure Functions and Cosmos DB"></a>Azure Functions and Cosmos DB</h1><p>Okay, so now that we have a working Cosmos DB service and backend running and we have an Event Hub for receiving events and messages, the next thing we need is a Azure Function. That’s gonna be our actual service for processing the data we receive and we have an example here called bethune, and we’ll dig into the code and a little bit but before we do that, I wanna show how it is we set up an Azure Function app.</p>
<p>As with a lot of Azure services, there’s two parts essentially. There’s the shell, the Function app. And then there’s the code itself. The function that’s actually running. So as before we can just go through the create a resource wizard or Function app. We click Create. Pay as you go is fine. We want an existing resource group. We’ll put it in bethune2. We’ll name it bethune2 as well.</p>
<p>Our runtime stack, this is important. We’re gonna do Node.js ‘cause we wanna do a JavaScript app. And how we publish it here, we’re gonna just publish it as code. You can also save as a Docker Container. Version 12 is fine. Most recent default, Node.js version from the time of this recording. And we’ll stick to US East for our region.</p>
<p>There’s some additional configuration here. We go through our hosting. It’s gonna need a storage account so we’ll just, there isn’t an existing one in this namespace, in this account so we’ll create a new one by default. We’ll just call it new storage account, bethune, whatever, that’s fine. Operating system, Windows is okay. We’ll stick with the Windows default. And our plan type is serverless. We don’t need to pick one of the options, other options here.</p>
<p>There’s an app service plan and premium plan. If you know you need, you know, more storage and more resources. And then monitoring we can accept, well actually we’ll go with no. We don’t need to enable application insights. That’s more if we need more deep level granular monitoring. You can add that if you’d like, but for this tutorial we won’t need it. And for tags, we’re not gonna add any tags. That’s fine.</p>
<p>So we’ll go ahead and review and create. And as always, it’ll do a validation first. Config seems to be okay so we click Create and we’ll just give this a few moments and we will have our bethune2 Function app. Right? There we go. Deployment in progress, let’s give that a minute. Okay, all right.</p>
<p>So eventually the deployment will create, will complete and it shouldn’t take more than a minute or so. So once it’s finished, we can just click Go to resource, or we can find it from the dashboard. And you’ll see we now have a bethune2 Function app, and this is our sort of our container for our serverless Azure Function, our actual code.</p>
<p>So right now there’s not really any code executing, there’s just this account. So what we have to do is click on Functions and here is where we can go ahead and create our event processor. So we’ll click on Add, and this will take us through a pretty simple wizard to set something up.</p>
<p>Now, when you click New Function here, it’s gonna ask for the type of function, there’s a number of templates here. HTTP trigger, Timer trigger. These are the conditions that will determine when the code executes. So there’s many types of triggers here. For our purposes, we’re interested in this one, the Azure Event Hub trigger.</p>
<p>So with an Azure Event Hub trigger, what’ll happen is the code for the Azure Function will execute whenever a message or an event is received by that event hub. So we’ll go ahead and we’ll call this EventHubTrigger2, and then we will come up with some connection. We’ll have to create a new connection here.</p>
<p>So in order to create a connection, you’d need an existing event hub. Now fortunately we have an existing one. We just created the bethune2 event hub a little while ago. So we’ll use that. It’ll put in the rest of the config automatically. We’ll call it bethune2 connection. We’ll use the route manage access the route manage shared access key policy for that. Click OK. And now we have an event hub trigger for the event hub we created.</p>
<p>Event hub name, we’ll go ahead and call this bethune2 and we’ll give it the default consumer group. As you know, event hubs can have multiple consumer groups. So we click Create Function and this will, again, it’s not gonna give us the code for the function itself, but it will give us the configuration for the trigger. And that’s key for getting everything to be wired together properly. So just give this a second.</p>
<p>We can refresh it after a while and we should see, boom, there it is. There is our function EventHubTrigger2. And if we click on that, we can see here that the basic configuration, there’s a code here and there’s the integration.</p>
<p>Now the integration, we’ll start with that. This is defined by our trigger, by the type of function. Then we selected Event Hub. So we can see here eventHubsMessages is our trigger. And then this is where the actual function code is. I’ll come to that in a second here. But if we look at the trigger here, this is the configuration that we just set. And we can actually look at this as code in a little bit.</p>
<p>If you go to Code + Test, we can see. So here’s some default JavaScript code that’ll give us a message if we run it, we’ll try that in a second. But there’s another bit of code, this is function.json. This code is defined by the integration that we just looked at, which was the Azure Event Hub integration.</p>
<p>So we see here it’s a bindings config type eventHubTrigger. name, eventHubMessages, direction in. So this is, you know, if a message comes into the event hub, it’ll trigger this. Event Hub name, bethune2. The one we created and here’s our connection. Cardinality, many. I’ll come back to that in a bit, it deals with whether we’re dealing with individual messages or arrays of objects. And we care about the default consumer groups. So this function.json file is very important. This is our configuration for integrating with Event Hub.</p>
<p>Now the code, the actual JavaScript code is in this index.js file. And this doesn’t really do anything too fancy. If we look through it line by line, we can see it’s, it’s basically just logging some text here, context.log JavaScript eventhub trigger function called for message. It’ll print out this message object, and then context.log processed message giving us the message.</p>
<p>There’s a little for-loop here. It’ll go through every message in this object. It’s assuming that it’s receiving an array of messages. See, there’s that cardinality thing. It’s assuming that there’s multiple messages here. So we can do a test run of this if we’d like. We can just do Test and Run and click Run, and we can see it’ll connect to our function. And if we click Run we’ll see, it should just output some basic message text.</p>
<p>Okay, give that a moment. Welcome, we’re now connected to our streaming and we’ll do a quick run. So there’s 202 accepted, that’s good. And there it is, there’s our output right there. The same as the code, right? Processed message, message, right. Process to message, test message. That’s all it was setting as input, this test message here. And then we saw JavaScript eventhub trigger called, right. This is the output there and so it succeeded.</p>
<p>So we now have a working event hub triggered Azure Function with some default to JavaScript code. So now how do we integrate this with Cosmos DB? Well that’s our next challenge. Okay, so we have our function set up. We have some basic JavaScript code. We have our Event Hub trigger. So now we need to integrate with Cosmos DB and in order to do that, there’s a little bit more configuration we need to set up.</p>
<p>So what we’re gonna do is we’re gonna go back home and we’re gonna go back to our Cosmos DB account, bethune2, and we’re gonna go into the Data Explorer, and we’re gonna set up a container that will give us a collection within the database that we can use.</p>
<p>Now here in the Azure Function configuration, the main thing we need to do, if we go here we can say that, that was not set. We’ll go back to the integration and we can see here there’s no outputs defined. This is the main goal. We have to define an output so that the function knows to save its results or its output somewhere, and that’s going to be Cosmos DB.</p>
<p>Now what we need to do is we need to create a container within our bethune test DB. So we’ll click New Container. We’re gonna use this existing database and the container ID will be bethunecontainer. We’ll just call it bethune, we’ll call it bethune2container, why not. And then we will also give this a partition key. Let’s just call this testkey1, simple enough. So we have a partition key. This will help us ensure things work the right way. So click OK. And very quickly we will have a collection that we can use for storing the output of our Azure Function. All right, perfect.</p>
<p>So that’s already set and good to go. And if we click on the collection we’ll see there’s no data in it. There’s no items there. And hopefully if we do a test, we’ll see items show up. So let’s go back to our event hub and our Azure Function here, and we will add an output for this new collection.</p>
<p>Now the output, we have to pick Cosmos DB, that’s the output type we want. And then we get a few interesting bits of configuration here. This output document, that’s the parameter that we’re gonna care about in the actual code. And then the database name. We wanna use this bethune test database so we’ll put that in. And the collection we want to use would be the same as this container name, bethune2container.</p>
<p>One of the nice things is there’s this feature here called if true, create if doesn’t exist, basically. It means that if you don’t have this container or if you don’t have this database or collection existing, then the function will create it for you. So we can leave that as yes, you can leave it as no if you don’t want it automatically doing that. But what we have to also do is create a connection to the Cosmos DB account.</p>
<p>What we’ll do is we’ll click New and want to connect to the bethune2 database. We click OK. And we’ll have this connection here. Actually they’re already is an existing connection we can use, but in any event, in this part here, as long as you already have a Cosmos DB account, you should be able to just find, fill it in here and put in your connection. And then for the partition key, the partition key we had before here in this container we saw was testkey1. So we can literally just copy that and paste it and click OK.</p>
<p>So now we have an output. Now we actually have an output for our function. So how can we test this? How can we make sure that it works appropriately? So we’re not gonna test the event hub piece. We just want to test that the function can output into Cosmos DB. Well, there’s a very simple way to test that.</p>
<p>Okay, so now that we have the output defined for Cosmos DB, in order to test that it works, we have to make two changes to our function set up here. We’re gonna make a change to the JavaScript code, and we’re gonna make a change to the event hub integration piece. So the integration piece here, we can change it by just going through the wizard. You can also change the JSON which I’ll show in a little bit, but basically we wanna go to this cardinality section and change this to one.</p>
<p>Now cardinality tells the code what type of object to expect, whether we’re dealing with a one-to-one or many to one relationship. What we want is to deal with a single message object. We don’t want an array of messages to iterate through. So we change that to one, that we click Save, and then we’re gonna go, and we’re going to edit our JavaScript code, we can click here. And actually before I go into the JavaScript code, I’ll just show you the change in the function.json, we can see that this code has changed.</p>
<p>We’ve added this output section for the Cosmos DB integration. We have the parameter name for the output document here for what will actually go into Cosmos DB. We have the config for the database name, the collection, create if not exist. The string for the connection and then the partition key and the type direction. So this has all changed. And of course here, the cardinality has changed.</p>
<p>It now says one instead of many. So, you know, we can update our integration config here either using the wizard by clicking on it and we have the little boxes or we can edit this function.json file and save it. So we go to our JavaScript code, index.json, you can see that the problem here with this code is that it’s expecting an array, it wants to do a for-loop for each message on this eventHubMessages object, it wants to log them all. And what we’re gonna do is instead update the code to expect a single object, a single thing instead of an array. So we’ll just paste in some different codes so you don’t have to hear me type it all out. And really all we’ve changed is we’re still doing the logging here, but there’s no longer a for-loop.</p>
<p>So we’re taking in a single object. We have this processed output. And then this line is important. We have here the output document parameter. This is what’s gonna go into Cosmos DB. So we call this output document and we call a JSON.stringify to make sure we get a string. And this is the thing that will go into Cosmos DB. So what we’ll do is we will save this and we will do a quick test run to see if it works.</p>
<p>So we’ll click Test. Now if we go into Cosmos DB now we can see in the bethune2container collection. If we click on items there’s nothing there, right? There’s no items for us now. So if our code works, we should see something show up there. So we have a little silly message there, a test message banana face. We know for sure that that’s us. And so we just run, click Run here and cross fingers. Hopefully we should see this execute without any failures. And there we go.</p>
<p>Okay it executed. And now let’s see if this actually went into Cosmos DB, we’ll click on Items. And there it is, there’s our message, Test Message banana face. So we know that our function works in terms of the output piece. We know that we can have the Azure function connect to Cosmos DB and save messages in a collection of our choice in the database of our choice.</p>
<p>Now the last thing we need to do is test the event hub piece. We wanna see the whole thing work end to end. We want to see a message go into event hub, and then that trigger the Azure function. And then after that function is triggered, it should then save it into Cosmos DB. We know this Cosmos DB part works, but we wanna see the event hub part as well.</p>
<p>So that’s what we’ll do in the last section of this demo. It should be fun.</p>
<h1 id="Validating-Our-App"><a href="#Validating-Our-App" class="headerlink" title="Validating Our App"></a>Validating Our App</h1><p>Okay, so now we’re coming into the final section of our demo. We’re going to do an end-to-end validation of our app. What we did before with Cosmos DB, we were able to send a test message through the Azure function dashboard, and we were able to see it persisted in Cosmos DB. So that’s good, but what we really want to see is that the Event Hub works is able to capture a message and then that will trigger the Azure function, and then it will be persisted in Cosmos DB.</p>
<p>So we wanna see all three steps. Now, in order to do that we have to send a message to our Event Hub. And there’s a lot of ways we can do this. If you’re familiar with PowerShell or command line, if you can use a Python Repl, you can write some code. you could send a curl command with the right authentication and all that. But these aren’t the most user-friendly approaches if you are not, if you don’t have experience in programming.</p>
<p>So if you’re not an experienced programmer, then I would say the easiest thing to do is to just use Visual Studio and the SampleSender library that you could get from Microsoft GitHub. So we’ll give you <a target="_blank" rel="noopener" href="https://github.com/Azure/azure-event-hubs/tree/master/samples/DotNet/Microsoft.Azure.EventHubs/SampleSender">links</a> for that, downloading and installing Visual Studio is not too difficult. You can do it right, obviously on Windows works great or other platforms. And then the SampleSender library, you’ll just download that from GitHub. And once you have it, you’re just gonna open it, you’re gonna go into you’re gonna wanna open a project solution yeah that’s correct. You can look for an SLN file and that should just pop up in SampleSender directory. And you’re gonna want here SampleSender.SLN, okay. And once you have that, you’re ready to go. You’re basically ready to send your messages, although actually sorry, you’re not quite ready to go.</p>
<p>You have to change two things here. You have to put in the right name for your Event Hub and you have to put in the right authentication, the right connection string. This is the credential it’s using it. And right now it has the old ones for my previous test. So we have to get the correct name and connection string. So we have to figure out where can we get that from.</p>
<p>Well, you can get it from the dashboard and I’ll show you how to do that in one sec. Okay, so in order to get the connection string, we’re gonna go into the dashboard and I’ll show you how that’s, how you obtain it. First, we can change this to bethune2, because we know already the Event Hub name is bethune2, that’s in the dashboard.</p>
<p>If we click on bethune2 namespace, we will see here, this is the Event Hub itself bethune2. Now that connection string, in order to get that, we need to have a policy, a connection policy that will generate that credential. So in here where it says shared access policies, there’s none by default. What we can do is we click add, and then this will let us create a policy. We’ll just call it default. And we’ll say that this is for sending and for listening, we don’t need to give it management permission. I mean we can, it doesn’t really matter. This is just for a test, but for this one, we’ll go ahead and click create, we’ll call it default and we’ll see it should generate those connection strings if we… Okay yeah already created it.</p>
<p>So we click here and then here are the strings that we need. There’s a primary key, secondary key, connection string, a secondary key here. We care about this one connection string, primary key. We wanna copy this to our clipboard. And then we go into the code and we’re gonna get rid of this one here. We don’t need this anymore. This is the old one, and then we’ll just paste in the new one and there you go. And so now we have our credentials set properly.</p>
<p>So the last thing to do is so we’ll save it. We’ll make sure that this is all set there. The last thing we have to do is actually run this code and we should see the messages appear in the Cosmos DB backend.</p>
<p>Now just to show that they’re not already there as a some kind of slight of hand trick here, we’ll go into Cosmos DB now. We’ll go into our Data Explorer and we should see as it connects, if we go into the collection, we should see that there’s only maybe one or two items from when we ran the tests with the Azure function.</p>
<p>So if we go here and then go to this container, we click on items. It’s only loading that one silly test message we have here, this banana face thing. So there’s nothing else there. Cause this is selecting everything, it’s not finding anything. So if we run this code and it succeeds we should see more messages propagate.</p>
<p>So, let’s see if it works. We click on this green arrow here to run the code samplecenter, and it should start spinning everything up. We’ll see a terminal pop-up and there we go. All right, our code is executing and it should send exactly 100 messages. They’re all just small string that says the message and the number. And then if everything is working properly, these messages will hit the Event Hub trigger the Azure function, and then the function output will put them into Cosmos DB.</p>
<p>So we’ll go ahead and click anything here. Continuous X out I suppose and then we’ll close this. Now one thing we can do to validate the traffic is we can actually look at the Event Hub logs as well. We can go here to bethune2 and look at this bethune2 Event Hub. And this will tell us if any traffic or messages are going through.</p>
<p>Now it’ll take a while for them to show up. We don’t see it right away. We can also look at the, at the Azure function. We can see the logs for the Azure function. We can see if it’s executing at all. So we can go to our bethune2 function app here, and we can see if we go to our functions, we can see here this one, and we can look at the monitor for it. Well actually we don’t need the insights monitor. We can actually this, we didn’t configure application insights so we can just go to the activity log and it should tell us if anything is happening.</p>
<p>So we’re not seeing anything yet, but let’s check Cosmos DB. Let’s see if there’s any activity there. So we will go ahead and reload that and give it a moment. Close this, oh sorry one sec. Let’s go back to that Cosmos DB and we’ll see if we have any messages coming through. Oh, I hope this works all right, we’re connecting to Cosmos DB and we’ll go to our bethune test database container. We’ll click on items, drum roll please. And there it is, there are all the messages that we sent.</p>
<p>So the messages are now in Cosmos DB, which means that the function executed properly. Now we’re not seeing them in the activity logs here. It might take a minute or so for it to show up because we don’t have any advanced insights configured just yet. Actually yes, we are seeing it now.</p>
<p>So if we look at the overview, we could see this spike in activity for the function app. This shows us that the function executed, and then we may see something similar in the Event Hub. If we look at throughput, we might we should see requested messages. Well, it might take a moment for that to propagate, but the most important thing is here are the actual messages are there.</p>
<p>So now we know that the code worked successfully. Our Cosmos DB, Event hub, and Azure function app is working as designed. So congrats if you’re able to get this working on your own and thanks for playing along, cheers.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Congratulations, you made it. Give yourself a pat on the back, because it’s been a long and tough ride. We went through a lot of pretty dense material, so before we pop the champagne bottles, let’s take a minute to briefly review what we have accomplished. By completing this course, you should now have a working knowledge of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/introduction/">Cosmos DB</a>. You should understand the basic service and its features, you should know how to make use of it with an Azure account, and you should have a pretty solid idea of how to integrate Cosmos DB with a real-world application.</p>
<p>Recall our three learning objectives. Number one, the student will have a basic understanding of the Cosmos DB technology, including its feature set and design philosophy. We covered that in section one where we talked about Cosmos DB’s history, its unique capabilities, and the general architecture and design. Number two, the student will know how to use Cosmos DB via its APIs, CLI tools, and the Azure web console. Section two went over this pretty thoroughly. We walked through setting up Cosmos DB in the web console using Azure data explorer. We also talked about how to start coding with Cosmos DB SDKs and how to use both PowerShell and Azure’s CLI tool with Cosmos DB. And then number three, students will have a practical understanding of how to integrate Cosmos DB with other <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> services with the goal of creating a working app. This was section three where we introduced a few other Azure services and explained how to make them work with Cosmos DB. We walked through creating an app backend and even showed how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/validating-app-cosmos-db/">validate everything</a>. You should now be ready to work with Cosmos DB, both at work and in your side projects. </p>
<p>Now, remember, practice makes perfect. The best way to really solidify your knowledge is to actually go build something, so get out there and make some magic. Now that you’re done, I’d like to invite you to send any feedback you have about the course to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. We greatly appreciate your comments, questions, and suggestions. Congratulations again on fighting through the whole course, and good luck in your future endeavors.</p>
<h2 id="5Getting-Started-with-Cosmos-DB"><a href="#5Getting-Started-with-Cosmos-DB" class="headerlink" title="5Getting Started with Cosmos DB"></a>5<strong>Getting Started with Cosmos DB</strong></h2><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-gb/azure/cosmos-db/request-units">Cosmos DB Request Units</a></p>
<h2 id="11Validating-Our-App"><a href="#11Validating-Our-App" class="headerlink" title="11Validating Our App"></a>11<strong>Validating Our App</strong></h2><p><a target="_blank" rel="noopener" href="https://github.com/Azure/azure-event-hubs/tree/master/samples/DotNet/Microsoft.Azure.EventHubs/SampleSender">Microsoft GitHub repo</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Lab-Working-with-Azure-Storage-Using-PowerShell-22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Lab-Working-with-Azure-Storage-Using-PowerShell-22/" class="post-title-link" itemprop="url">AZ-204-Lab-Working-with-Azure-Storage-Using-PowerShell-22</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:17:39 / Modified: 11:17:40" itemprop="dateCreated datePublished" datetime="2022-11-14T11:17:39-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Lab-Working-with-Azure-Storage-Using-PowerShell-22/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Lab-Working-with-Azure-Storage-Using-PowerShell-22/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata-21/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata-21/" class="post-title-link" itemprop="url">AZ-204-Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata-21</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:15:42" itemprop="dateCreated datePublished" datetime="2022-11-14T11:15:42-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 00:33:16" itemprop="dateModified" datetime="2022-11-15T00:33:16-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata-21/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata-21/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata"><a href="#Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata" class="headerlink" title="Setting and Retrieving Azure Blob Storage Properties and Metadata"></a>Setting and Retrieving Azure Blob Storage Properties and Metadata</h1><p>Welcome to “Setting and Retrieving Azure Blob Storage Properties and Metadata”. I’m Guy Hummel. To get the most from this course, you should already have some knowledge of Azure Blob Storage. You should also have some experience with programming, especially C#.</p>
<p>In this short course, I’ll explain how to manage blob properties and metadata using the Azure Portal and the Azure Storage client libraries for .NET.</p>
<p>So what’s the difference between blob properties and metadata? Properties are automatically created by Azure, while metadata is user-defined. That is, metadata is optional, and you only create it if you need it.</p>
<p>Let’s have a look at some properties. I’m in the Azure Portal, and I’m looking at a blob container in a storage account. These are the blobs in the container. To see the system properties for one of them, you right-click on the blob and select “Properties”.</p>
<p>Some of the properties are pretty straightforward, such as Last Modified, Creation Time, and Size. You’ll notice that these properties can’t be modified, which makes sense because there’s only one true value for each of them. Some of the properties will change if you change the blob itself. For example, if you add more data to the blob, then its size will increase, or if you move the blob from the Hot tier to the Cool tier, then the access tier property will change.</p>
<p>You can see that there are a handful of properties that you can change, though. The first one only applies if this blob is configured to be cached in the Azure Content Delivery Network. The rest of them are things like content type and content language. Azure doesn’t necessarily know what these should be set to, so it allows you to set them. For example, you could set the content language to “en-us”, which means US English. Then, you’d click “Save”.</p>
<p>There are actually a few more you can set using the portal even though it doesn’t look like you can. These three lease properties can be set indirectly by clicking the “Acquire lease” button. If you apply a lease to a blob, then the blob can’t be modified by anyone else until the lease has expired. It’s a good way to prevent two people or programs from making conflicting changes to a blob.</p>
<p>When we click “Acquire lease”, it changes the lease status to “Locked”, the lease state to “Leased”, and the lease duration to “Infinite”. The duration is infinite because we didn’t set a time when the lease should expire. Of course, we weren’t given the option to set an expiration time, so we didn’t have a choice. To set one, we’d need to use the Azure Storage client libraries. To end the lease, we can click “Break lease”. Now, these three properties have changed again.</p>
<p>Okay, down here we can see a Metadata section. It’s empty because, as I mentioned, metadata is optional. Suppose we wanted to set a category for each of our documents. Then we’d create a key called “category”, and we’d set the value to the appropriate category for the document, such as “finance”. Then we’d click “Save”, and our key&#x2F;value pair would show up in the metadata list.</p>
<p>Using the Azure Portal to read and write blob properties and metadata is fine if you only need to change a few blobs, but if you need to make a lot of changes, then it would be easier to write a script. You could do that using either the Azure command-line interface or Azure PowerShell. If you have an application that needs to manage blob properties and metadata, then you can use Azure Storage client libraries. They’re available for a number of languages, but I’ll show you an example from Microsoft’s documentation that uses the .NET version.</p>
<p>This is an example C# method called SetBlobPropertiesAsync. It shows how to set the ContentType and ContentLanguage properties. First, it retrieves the existing properties by calling the GetPropertiesAsync method. It does this because when it creates a new BlobHttpHeaders class, it needs to set all of the properties for the blob, so it needs to know what to set the other properties to. By the way, even though it’s called BlobHttpHeaders, it just means BlobProperties.</p>
<p>First, it sets the ContentType to “text&#x2F;plain” and the ContentLanguage to “en-us”, which is the whole purpose of this method. Then it sets the remaining properties to the values it retrieved at the beginning. If it didn’t set these properties, then their existing values would be wiped out, so it has to set them.</p>
<p>Now it calls the SetHttpHeadersAsync method to actually set the properties of the blob to the values in the BlobHttpHeaders class that was created above. Then there’s a catch block to say what to do if the operation fails.</p>
<p>Okay, now how about setting blob metadata? It’s pretty similar. Here’s an example method called AddBlobMetadataAsync. First, it creates a Dictionary object called “metadata” that you can put key&#x2F;value pairs in.</p>
<p>Then it shows you two different ways to add a key&#x2F;value pair. The first is to use the Add method and pass it the key and value. In this example, it’s setting a key called “docType” to the value “textDocuments”, but you could set these to whatever you want because they’re user-defined.</p>
<p>The second way is to use this key&#x2F;value syntax where the key, called “category” in this example, is in square brackets, and the value (“guidance”, in this example) comes after the equal sign.</p>
<p>When you’re done adding key&#x2F;value pairs to the dictionary, you call the SetMetadataAsync method and pass it the dictionary object that contains the metadata. Finally, there’s a catch block to handle errors.</p>
<p>Now, I’ll show you one more code sample. This one shows you how to read metadata from a blob. Interestingly, you use the GetPropertiesAsync method, which seems weird considering we’re trying to get the metadata, not the properties. Well, this method actually gets both the properties and the metadata. We already saw how to read the properties from this in the first example, but to retrieve the metadata, you use a foreach loop to read all of the key&#x2F;value pairs from the Metadata dictionary.</p>
<p>And that’s it for setting and retrieving properties and metadata in Azure Blob Storage. Please give this course a rating, and if you have any questions or comments, please let us know. Thanks!</p>
<h2 id="1Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata"><a href="#1Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata" class="headerlink" title="1Setting and Retrieving Azure Blob Storage Properties and Metadata"></a>1<strong>Setting and Retrieving Azure Blob Storage Properties and Metadata</strong></h2><p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-properties-metadata#set-and-retrieve-properties">Manage blob properties and metadata with .NET</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20/" class="post-title-link" itemprop="url">AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:14:57" itemprop="dateCreated datePublished" datetime="2022-11-14T11:14:57-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 00:32:02" itemprop="dateModified" datetime="2022-11-15T00:32:02-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Configuring-Azure-Blob-Storage-Lifecycle-Management"><a href="#Configuring-Azure-Blob-Storage-Lifecycle-Management" class="headerlink" title="Configuring Azure Blob Storage Lifecycle Management"></a>Configuring Azure Blob Storage Lifecycle Management</h1><p>Welcome to “Configuring Azure Blob Storage Lifecycle Management”. I’m Guy Hummel. To get the most from this course, you should already have some knowledge of Azure Blob Storage. In this short course, I’ll explain how you can save money by setting up lifecycle management policies to automatically move your blobs to less expensive access tiers when certain conditions are met.</p>
<p>First, let’s review Azure Blob Storage access tiers. The Hot tier is designed for blobs that will be accessed frequently. This is the default tier. It has the most expensive storage costs but the lowest access costs.</p>
<p>The Cool tier has lower storage costs but higher access costs. That’s why you should only put infrequently accessed blobs in the Cool tier. If you were to put frequently accessed blobs in this tier, then the access costs would very quickly outweigh the lower storage costs. Also, you have to leave data in the Cool tier for at least 30 days. If you delete it or move it to a different tier in less than 30 days, you’ll have to pay an early removal penalty.</p>
<p>The Archive tier has the lowest storage costs and the highest access costs. It’s intended for data that you rarely need to access, such as long-term backups. To avoid an early deletion penalty, you need to leave data in the Archive tier for at least 180 days. It’s significantly different from the hot and cool tiers because you can’t access your data right away when you need it. That’s because it uses offline storage. So you have to wait for your data to be rehydrated before you can access it. This process can take up to 15 hours.</p>
<p>Since there’s a big difference in cost between the different tiers, it’s a good idea to set up an automated system to move your data between tiers when the time is right. For example, you might have monthly reports that get accessed frequently in the first month after they’re created, are accessed much less frequently for the next 11 months, and are rarely accessed after that but need to be retained for a total of 10 years for compliance reasons.</p>
<p>In this case, you might want to create a policy that:</p>
<ul>
<li>Moves these reports from the Hot tier to the Cool tier after 30 days,</li>
<li>Moves them from the Cool tier to the Archive tier when they’re 12 months old,</li>
<li>And deletes them from the Archive tier after they’re 10 years old.</li>
</ul>
<p>Notice that I said 12 months instead of 11 months for the second one because the blobs would have already been in the Hot tier for 1 month before being moved to the Cool tier, so they would be 12 months old. I’ll show you two different ways to set up these rules: using the Azure portal and using the command line.</p>
<p>First, let’s use the portal. I’ve already created a storage account and a blob container. In the menu on the left, select “Lifecycle management”. This is where we create the rules we want. Notice that it says lifecycle management is only available for general-purpose v2 accounts and blob storage accounts. Since most storage accounts are general-purpose v2 accounts these days, you probably won’t need to worry about having the right account type.</p>
<p>Now click “Add a rule”. Let’s call it “LifecycleForReports”. We’ll leave the rule scope as “Apply rule to all blobs in your storage account”. We’ll leave the blob type as just “Block blobs”. Append blobs are used for files that frequently have new data appended to them, which likely wouldn’t be the case for our monthly reports.</p>
<p>You might not be familiar with the blob subtype. If you turn on blob versioning, then a copy of the blob will be saved every time the blob is modified. That way, you could retrieve a previous version of the blob if you needed to. Snapshots are similar except that you create them manually. If you don’t use either versioning or snapshots, then you can just leave “Base blobs” checked.</p>
<p>Okay, now we can click “Next” and define the rule. A rule needs to contain one or more conditions. The first condition we want to have is to move blobs to Cool storage after 30 days, so let’s change this to “If base blobs were created more than 30 days ago, then move to cool storage. Then we click “Add conditions” again to add the next one, which is to move blobs from Cool storage to Archive storage when they’re 12 months old. So, we’ll put 365 days since they were created.</p>
<p>Finally, we’ll click “Add conditions” again, and this time, we’ll say that after 3,650 days (which is 10 years if you don’t count leap years), then delete the blob.</p>
<p>Now we click “Add” and we’re done. However, this lifecycle policy won’t go into effect immediately. Azure only runs policies once a day, so it can take up to 24 hours before any actions triggered by a new policy will take place.</p>
<p>This example was pretty simple, but there are some other features we can use to customize it. First, we could create a rule that only applies to certain blobs. To do this, we’d need to create a filter. Technically, when we set the blob type to only block blobs, that was a filter, but we can also create a filter that looks at the name of a blob.</p>
<p>For example, suppose we want a lifecycle rule to only apply to blobs that start with the word “report”, then we’d select “Limit blobs with filters”. Now there’s a tab called “Filter set”. There are two types of filters we can apply: blob prefix and blob index match. The second one is more complicated and requires tagging your blobs before it’ll work. Blob prefix is the one we need for our example. We just need to type “report” in this field. And we have to click the Update button. Now the rule will apply to any blobs that start with those letters.</p>
<p>Here’s another change we can make. Suppose you don’t want a blob to be moved to the Cool tier until a certain number of days after the last time it was accessed rather than since it was created. You might have noticed that this wasn’t an option in the list of possible conditions, so how would we do it? First, we have to check the “Enable access tracking” box. Then, when we go back to the first condition, the list includes “Last accessed”, so we can select it. And click “Update”.</p>
<p>This visual interface makes it very easy to create lifecycle rules, but if you had to apply lifecycle policies to lots of different storage accounts, it would probably be faster to create a standard configuration and apply it using the command line. Then you wouldn’t have to keep pointing and clicking in the interface over and over again.</p>
<p>To do this, you need to create a JSON file that contains your rules. But this can be a bit of a daunting task, so there’s a shortcut you can use. Once you’ve created one or more rules in this interface, you can go to the Code View tab, and there’s the JSON code you need.</p>
<p>The rule definition is divided into two sections: the rule actions and the rule filters. For each of the conditions we created, it shows the action to take and the condition required to take that action. Here’s the one for moving blobs to the Cool tier. Here’s the one for moving to the Archive tier, and here’s the one for deleting blobs. In the filters section, it shows the blob type filter and the prefix match filter that we set.</p>
<p>If we wanted to change anything, we could actually edit it right here and click the Save button, which would apply the updated policy to this storage account. But in most cases, we’d probably want to download the code, modify it, and use the command line to apply it to other storage accounts.</p>
<p>So, we’ll click “Download”. Then, to avoid having to install the Azure command-line utility on your desktop, we can upload the JSON file to the Cloud Shell and run the command there. So, I’ll open the Cloud Shell. Then I’ll select “Upload”. Here’s the file we downloaded. It’s called “policy.json”.</p>
<p>I’m not going to modify anything, but I’ll show you what command you’d use to apply this lifecycle policy to a storage account. I put the command in the transcript below in case you want to try this yourself.</p>
<p>It starts with “az storage account management-policy create”. Then you specify the account name. My storage account is called “camonthlyreports”. Then you type “–policy” and the name of the JSON file. And finally, you tell it the resource group where your storage account resides. I called mine “camonthlyreportsrg”. Now hit Enter. It might not be obvious from this output, but it successfully applied the policy to that storage account. </p>
<p>And that’s it for Blob Storage lifecycle management. Please give this course a rating, and if you have any questions or comments, please let us know. Thanks!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer-19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer-19/" class="post-title-link" itemprop="url">AZ-204-Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer-19</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:14:08" itemprop="dateCreated datePublished" datetime="2022-11-14T11:14:08-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 00:30:58" itemprop="dateModified" datetime="2022-11-15T00:30:58-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer-19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer-19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer"><a href="#Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer" class="headerlink" title="Managing Azure Storage With AzCopy and Azure Storage Explorer"></a>Managing Azure Storage With AzCopy and Azure Storage Explorer</h1><p>Welcome to Managing Azure Storage with AzCopy and Azure Storage Explorer. I’m Guy Hummel. To get the most from this course, you should already have some knowledge of Azure Storage. I’ll show you two different ways of working with Azure Storage. If you prefer a graphical user interface, then you can use Azure Storage Explorer, which is an application that runs on Windows, MacOS, and Linux. If you’re comfortable with the command line, then you can use AzCopy. Let’s start with Azure Storage Explorer. First you need to download it from Microsoft. The URL for this page is in the transcript below. Select your operating system and click the ‘Download’ button. On a Mac, you just need to open the zip file and then run the Storage Explorer application. On Windows and Linux, you need to run an installation program. The first thing you’ll need to do is sign into your Azure Storage account. Unless you’re using a special Azure environment, leave this set to Azure and click ‘Next’.</p>
<p>It opens a browser tab and asks you to authenticate. I’m already logged in, so once I select my account, I’m authenticated and I can close this tab and go back to Storage Explorer. Over here, you can choose your subscription and your storage account. I have an account called camonthlyreports and a blob container in it called monthly-reports. When I click on the container, it shows me the blobs in it. I don’t have any blobs in this container which is why nothing shows up here. I’ll upload some files into it so we have something to work with. You can either upload a whole folder or individual files. If I were to choose folder, then it would create a virtual folder inside the container, which isn’t what I want in this case. So, I’m going to upload the files that are in the folder. I can select all of the files by holding down the ‘Shift’ key when I click the bottom one. That way they’ll all get uploaded at the same time.</p>
<p>Now all of the files are in the container. If you right click on one of the files, you can see that there are lots of options for what we can do with it. For example, we could copy it, or delete it, or change its access tier, such as if we wanted to move it to the cool tier. I’ll show you how to copy a blob to another container. First, let’s create a second container so we have a place to copy the blob to. If we right-click on blob containers, there’s an option to create a blob container. Let’s call it Copies. Now, if we go back to the first container, we can right-click on a file and select ‘Copy’. It’s not obvious that we did anything, but if we go to the second container, we can click the ‘Paste’ button and it’ll copy that blob into this container. You might have noticed something interesting down here. It says ‘Copy AzCopy Command to Clipboard’. It turns out that Azure Storage Explorer uses the AzCopy command when it needs to copy blobs. And if you want to see the actual command, you can click here.</p>
<p>Now it’s on the clipboard, so we have to paste it somewhere to see it. I’ll paste it to a text editor. It kind of looks like gibberish, doesn’t it? Well it’s not as bad as it looks. The basic command is just ‘azcopy copy’ then the URL of the source and the URL of the destination. So, why do you have to put ‘copy’ after ‘azcopy’? That’s because the AzCopy command can also do other things besides just copying files, so the name of the utility is a bit misleading. For example, if you wanted to delete a blob, you’d start the command with azcopy remove. Okay, so why are the URLs so long? That’s because the URL contains a shared access signature, or SAS. It’s the part after the question mark. A SAS gives access to resources in a storage account for a limited amount of time. You can generate a SAS yourself if you want to give someone access to a particular resource for, let’s say, the next week. But Storage Explorer generated a SAS for the source and a SAS for the destination for you.</p>
<p>You don’t have to use them though, because there’s another way to provide authentication. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-azure-active-directory-3142/introduction/">Azure Active Directory</a>. If you have an Azure account that’s authorized to access the source and destination, then you don’t need to use shared access signatures. There’s something very important you need to know to get this to work though. Even if you’re the owner of the source and destination storage accounts, you could still get a permission error. You’re probably thinking, what? How is that possible? Well, it turns out that your Azure Active Directory account needs to have the Storage Blob Data Contributor role for the source and destination storage accounts if you want to copy blobs using AzCopy. In this example, I’m using the same storage account for both the source and the destination, so I only need to give myself the Storage Blob Data Contributor role for that storage account.</p>
<p>To see whether you have that role assigned to you or not, go into the storage account, select ‘Access Control’, and then click ‘View my access’. You can see that I have that role assigned to me, but if you don’t then click on the role assignments tab and assign it to yourself. Or if you don’t have permission to do that, then ask an administrator to do it for you. If that would be a problem, then you could use shared access signatures instead. Okay, now I’ll show you how to run the AzCopy command. I could install AzCopy on my desktop, but since it’s already installed in Cloud Shell, let’s use that instead. But we still need to do one more thing first. Type ‘azcopy login’ to authenticate. Then go to this URL and copy and paste this code. Then choose your Azure account and click ‘Continue’. All right, now we can finally try to copy a blob. First type azcopy copy, then put in the URL of the source blob.</p>
<p>This is the name of the storage account, this is the name of the container, and this is the name of the blob. Note that if we were including a shared access signature at the end of the URL, we’d need to put quotes around the whole thing, but since we’re not using a SAS, we don’t need to. Now put in the URL of the destination. If we wanted to give the blob copy a different name than the original one, we type a different name here. We could even leave the blob name out and just give the URL of the container where we want to copy the blob, but it doesn’t hurt to give the name. In the AzCopy command we got from Azure Storage Explorer, there were some options at the end. The first one was overwrite&#x3D;prompt. This means that if the command will overwrite a blob in the destination, then it should ask whether you actually want to overwrite it before doing so.</p>
<p>Since we’re copying the same blob that we already copied using Storage Explorer, this option will matter for our example. If we don’t include this option, then it’ll overwrite the blob by default. Let’s include this option so we can see what the prompt looks like. The next one is s2s-preserve-access-tier&#x3D;false. This tells it not to ensure that the access tier of the destination blob is the same as the one for the source blob. By default, this option is true. For our example, there’s no good reason to set this to false, so we can leave this option out entirely. The next one is include-directory-stub&#x3D;false. This is kind of an obscure option and it’s false by default anyway, so we can leave it out. The next one is recursive, which means that if you’re copying a folder from your local file system, it will recursively copy its subfolders as well. That’s not what we’re doing, so we can leave this out too. The last one is log-level&#x3D;INFO.</p>
<p>This refers to how much information is sent to the AzCopy log file and INFO is the default, so we don’t need to include this one either. Okay, let’s run it. There’s the prompt we were expecting that’s asking if we want to overwrite the blob. To say yes, type y. And it’s done. If you want to see all of the options that are available, you can type azcopy copy -h. But it’s easier to read if you go to this web page. Now after seeing that AzCopy is more difficult to use than Azure Storage Explorer, you might be wondering why you’d even use AzCopy. After all, Storage Explorer actually runs AzCopy under the hood, so you don’t have to use it yourself. Well, if you need to automate your copying activities, then you can write scripts that contain AzCopy commands. It’s also possible to do it by calling the Azure Storage Client Libraries in your code. They’re available for a variety of languages such as .NET, Java, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/python-beginners/hello-world/">Python</a>.</p>
<p>But if you want to put something together quickly, it’s much easier to use AzCopy. However, Azure Storage Explorer is better than AzCopy for general storage management because it supports more operations and storage types than AzCopy does. You can use it to manage not only blobs but also queues, tables, file shares also known as Azure Files, and Data Lake Storage Gen2. The Azure Storage Client Library support all of the storage types too, of course, but you have to write code to use them, so they’re not meant for doing storage management manually. AzCopy supports blobs, file shares, and Data Lake Storage Gen2, but it doesn’t support queues or tables, so it’s not as robust as Storage Explorer. And that’s it for AzCopy and Azure Storage Explorer. Please give this course a rating, and if you have any questions or comments, please let us know. Thanks.</p>
<h2 id="1Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer"><a href="#1Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer" class="headerlink" title="1Managing Azure Storage With AzCopy and Azure Storage Explorer"></a>1<strong>Managing Azure Storage With AzCopy and Azure Storage Explorer</strong></h2><p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/products/storage/storage-explorer">Azure Storage Explorer</a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/azure/storage/common/storage-ref-azcopy-copy">Azcopy Copy</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Knowledge-Check-Introduction-to-Azure-Storage-18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Knowledge-Check-Introduction-to-Azure-Storage-18/" class="post-title-link" itemprop="url">AZ-204-Knowledge-Check-Introduction-to-Azure-Storage-18</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:13:27 / Modified: 19:32:32" itemprop="dateCreated datePublished" datetime="2022-11-14T11:13:27-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Knowledge-Check-Introduction-to-Azure-Storage-18/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Knowledge-Check-Introduction-to-Azure-Storage-18/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><object data="2.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Introduction-to-Azure-Storage-17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Introduction-to-Azure-Storage-17/" class="post-title-link" itemprop="url">AZ-204-Introduction-to-Azure-Storage-17</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:12:45" itemprop="dateCreated datePublished" datetime="2022-11-14T11:12:45-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 00:29:10" itemprop="dateModified" datetime="2022-11-15T00:29:10-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Introduction-to-Azure-Storage-17/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Introduction-to-Azure-Storage-17/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Hi there. Welcome to Introduction to Azure Storage. My name is Thomas Mitchell and I’ll be taking you through this course.</p>
<p>I’m an Azure instructor at Cloud Academy and I have over 25 years of IT experience, several of those with cloud technologies. If you have any questions about this course, feel free to connect with me on LinkedIn, or send an email to <a href="mailto:&#115;&#117;&#112;&#x70;&#111;&#114;&#116;&#64;&#99;&#x6c;&#x6f;&#117;&#x64;&#97;&#x63;&#x61;&#x64;&#x65;&#x6d;&#121;&#46;&#x63;&#111;&#x6d;">&#115;&#117;&#112;&#x70;&#111;&#114;&#116;&#64;&#99;&#x6c;&#x6f;&#117;&#x64;&#97;&#x63;&#x61;&#x64;&#x65;&#x6d;&#121;&#46;&#x63;&#111;&#x6d;</a>.</p>
<p>This course is intended for those who wish to learn about the basics of Microsoft Azure storage.</p>
<p>We’re going to start things off with an introduction to azure storage. You’ll learn about the core storage services in Azure and about the different storage account types that are available. You also get to see a demonstration that shows you how to create a storage account in Microsoft Azure.</p>
<p>We’ll then take a look at the different storage services in a little more detail. We will start with blob storage, where you will learn what it is and what it offers. Next, will dive into Azure Files.</p>
<p>After covering Azure Files, we’ll dive into Azure queues. You’ll learn what Azure queues are and how to create a queue.</p>
<p>After learning about Azure queues, you’ll learn about Azure tables, where you will learn why and when to use Azure table storage.</p>
<p>We’ll wrap things up by taking an introductory look at Azure disks. </p>
<p>By the time you finish this course, you should have a solid understanding of the storage basics of Microsoft Azure.</p>
<p>We’d love to get your feedback on this course, so please give it a rating when you’re finished. If you’re ready to learn about Azure storage, let’s get started.</p>
<h1 id="Core-Storage-Services-in-Azure"><a href="#Core-Storage-Services-in-Azure" class="headerlink" title="Core Storage Services in Azure"></a>Core Storage Services in Azure</h1><p>Hello and welcome to core storage services in Azure. In this lesson, we will take a look at some of the core storage offerings, including blob storage, Azure Files, queue storage, table storage, and disk storage. However, before we get into the separate storage services, let’s take a look at some of the benefits of Azure storage.</p>
<p>Because the Azure storage platform is designed for virtually all modern-day data storage requirements, it was built to offer several key benefits. First and foremost, all Azure storage services are durable and highly available. Built-in redundancy keeps data safe in the event of underlying hardware failures within the Azure infrastructure. In addition to this redundancy, you can also cross multiple data centers and even multiple geographical regions. This provides protection against natural disasters or local failures at the data center level.</p>
<p>Because all data that gets written to Azure storage accounts is encrypted automatically, it is inherently secure as well. Azure also offers the ability to maintain fine-grained control over data access. </p>
<p>Azure storage offerings are also scalable and widely accessible. As I mentioned earlier, the Azure storage platform is designed to support all modern-day data storage requirements. That being the case, it’s designed to be massively scalable. You can access Azure data from anywhere in the world over HTTP or HTTPS. There are also numerous client libraries available for Azure storage in numerous languages. This means you can access Azure storage using things like .NET, Java, Python, PHP, and many others.</p>
<p>You can also access Azure storage through PowerShell scripting, through the Azure CLI, through the Azure portal, and through Azure Storage Explorer.</p>
<p>So now that you understand some of the key benefits of the core Azure storage services, let’s take an introductory look at each of them.</p>
<p>Let’s start with Azure blobs.</p>
<p>Azure blob storage provides object storage for the cloud. This offering has been optimized to support massive amounts of unstructured data. Unstructured data is data that does not fit a specific data model. For example, text and binary data would be the type of unstructured data that falls under blob storage.</p>
<p>Azure Files is essentially a fully managed file share system available in the cloud. You can access Azure Files through the typical SMB protocol. You can mount Azure file shares from Windows, Linux, and MacOS machines that reside both on-prem and in the cloud. You can even cash Azure file shares on Windows servers, using the Azure file sync service. This helps make data more accessible for remote offices.</p>
<p>Azure queue storage is a special type of storage service. It’s not meant for storing files. Instead, it’s designed for storing large numbers of messages. These messages are used in communications between the different components of a distributed application. Such messages can be accessed from anywhere in the world through authenticated calls via HTTP or HTTPS. Each queue message can be up to 64 KB in size. A typical queue can contain millions of messages.</p>
<p>Azure table storage is a storage offering intended for the storage of structured NoSQL data. It offers a key&#x2F;attribute store and a schema-less design. The schema-less design of Azure table storage allows you to more easily adapt the data to the needs of your business or application.</p>
<p>Lastly, let’s take a look at Azure managed disks. Azure managed disks are essentially block-level storage volumes. These storage volumes, which are managed by Azure, are used, to provide storage capabilities for virtual machines. A managed disk is much like a physical desk that you would see in an on-prem server. However, it’s virtualized. There are several types of disks available in Azure. They include ultra disks, premium SSD disks, standard SSD disks and standard HDD disks. We will touch on these in more detail later on.</p>
<p>So, to wrap things up for this lesson, let’s review. There are essentially five different core storage services available in Microsoft Azure. They include Azure blobs, Azure Files, Azure queues, Azure tables, and Azure disks. Each of these storage services are accessed through an Azure storage account. Over the next several lessons, we will dive into storage accounts and into the details of each core storage service in Microsoft Azure.</p>
<h1 id="Storage-Account-Types-in-Azure"><a href="#Storage-Account-Types-in-Azure" class="headerlink" title="Storage Account Types in Azure"></a>Storage Account Types in Azure</h1><p>Hello and welcome to storage account types in Microsoft Azure. In this lesson, we are going to take a look at the different types of storage accounts that are available when it comes time to provision storage in your Azure subscription.</p>
<p>Before we get into the different types of storage accounts. Let’s talk a little bit about what a storage account is and what it’s used for. A storage account in Azure can be viewed as the container, so to speak, that houses all of your Azure storage data objects. A storage account can host blobs, Azure Files, queues, tables, and disks.</p>
<p>When you provision a storage account. You are asked to provide a unique name for that storage account. This is necessary because data within a storage account is accessible from anywhere in the world. That being the case, the storage account namespace must be unique across the Azure landscape.</p>
<p>There are several different types of storage accounts available. Each type offers different features, and each has a different pricing model.</p>
<p>The first type of storage account is the general-purpose V2 account. A general-purpose V2 storage account is a basic storage account that can be used, to host blobs, files, queues, and tables. Microsoft recommends using the general-purpose V2 storage account for most scenarios that require Azure storage.</p>
<p>The general-purpose V1 storage account is similar to the V2 account. This legacy type account can also host blobs, files, queues, and tables. While a general-purpose V1 account offers similar functionality to the V2 accounts, Microsoft recommends using the general-purpose V2 account instead. This tells me that the general-purpose V1 storage account will probably go away at some point in the future.</p>
<p>The next type of storage account we are going to take a look at here is the block blob storage account. Block blob storage accounts offer premium performance for block blobs and append blobs. You would typically use a block blob storage account for situations where high transaction rates are in play. Block blob storage accounts are also a good choice for scenarios that require low storage latency.</p>
<p>File storage accounts are exactly what the name says they are. They are files only storage accounts. Because they feature high-performance characteristics, Microsoft recommends using these kinds of accounts for enterprise applications or for high-performing applications.</p>
<p>The last storage account type touch on here is the blob storage account. The blob storage account is a legacy account that is used for blob only storage. Microsoft actually recommends that, instead of using blob storage accounts, you use general-purpose V2 accounts. Like the general-purpose V1 accounts, I suspect the blob storage accounts, because of their legacy status, will likely go away sometime in the future.</p>
<p>The table that you see on your screen right now shows each of the different types of storage accounts, along with their key features. As you can see on your screen. Every storage account type is encrypted using storage service encryption, or SSE, for data that is at rest.</p>
<p>I should point out that archive storage and blob level tiering only support block blobs.</p>
<p>Another key point I should mention here is that zone redundant storage and Geo zone redundant storage are only available for standard general-purpose V2 accounts, block blob accounts, and file storage accounts in certain regions.</p>
<p>The premium performance that you see listed for general-purpose V2 and general-purpose V1 accounts is only available for disk storage and page blobs. Premium performance for block blobs and append blobs is only available for block blob accounts. Also, just as importantly, premium performance for files is only available on file storage accounts.</p>
<p>For more information on the different storage accounts available in Microsoft Azure, visit the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview">https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview</a></p>
<h1 id="DEMO-Creating-a-Storage-Account"><a href="#DEMO-Creating-a-Storage-Account" class="headerlink" title="DEMO: Creating a Storage Account"></a>DEMO: Creating a Storage Account</h1><p>Hi everyone and welcome back. In this brief demonstration, I’m going to show you how to create a storage account using the Azure portal. Now, every storage account needs to belong to a resource group. Now a resource group, as you may know is a logical container that groups all of your Azure resources.</p>
<p>Now when we create our storage account here, we’re going to have two options, we can create a storage account and then as part of that process, we can create the new resource group or we can select an existing resource group. What we’re going to do here is add this new storage account to an existing resource group.</p>
<p>So on the screen here, I’m logged into my Azure portal and I’m at the home page. To create our general purpose V2 storage account, what we’ll do here is go up to the hamburger here and select all services. And then from here, we can search for storage accounts. And we’ll select it from the results page here. And we can see in my existing subscription here, I already have a storage account called VMLab diag 601.</p>
<p>What we’re going to do here is create a new one by clicking Add. And then from here, we need to provide some basic information, we need to specify the subscription that’s going to host the storage accounts. We need to select the resource group we want to deploy the storage account into or we can create a new one here. And then of course, we need to give our storage account a name, tell Azure which locations going into and then the performance of the storage account.</p>
<p>We can see that standard storage accounts are backed by magnetic drives and they are the cheapest cost per gigabyte. We can see in this popup, that standard storage accounts are good for applications that need bulk storage or where you’re accessing the data within that storage account infrequently. The premium storage accounts are backed by solid state drives. You use these when you need a consistent low latency performance.</p>
<p>Now you will notice here that a premium performance storage account can only be used with Azure virtual machine disks and they are best for IO intensive applications like databases. Another important item to note here is that virtual machines that use premium storage for all of their discs, do qualify for a 99.9% SLA even if they’re not running within an availability set so that’s pretty important to remember.</p>
<p>For this exercise, we’ll just use a standard storage account here and then the account kind here is where we can select what type of storage account to deploy. Here, we have three options storage V2, storage which is basically the legacy general purpose V1 and Blob storage.</p>
<p>Now in the replication dropdown, we can specify the redundancy that we need for our storage whether it be locally-redundant, zone-redundant, geo-redundant, read-access geo-redundant, geo zone-redundant or read access geo zone-redundant. Instead of going through each of these individually, you can visit the URL that you see on your screen for more information about each of these types of redundancy options. And lastly, we have the access tier here.</p>
<p>Now the hot access tier is really used for data that’s accessed frequently. The cool access tier is typically used for infrequently access data. We can also see that there is a another option and it’s the archive access tier but that can only be set at the Blob level and not on the actual storage account level.</p>
<p>So let’s go ahead and see here. We’ll deploy our storage accounts and we only have one subscription here, our labs subscription. So we’ll leave that set and then we have a couple of different resource groups here. So I’ll just deploy this into my VMLab resource group. And then we need to give our storage account a name and this name must be unique across all storage accounts in Azure, it needs to be unique across the landscape. The name can be as short as three characters but as long as 24 and it can only contain lowercase letters and numbers. So let’s call this test9878storage.</p>
<p>We get the green checkbox which means we’re good and we’ll deploy into the Central US region, that’s all we really need for this demonstration. And we’re going to deploy a general purpose V2 and we’ll accept the default read-access geo-redundant storage. We’ll do the same thing for the access tier.</p>
<p>Now, when we click Next for Networking, we can specify our network connectivity requirements, including the connectivity method and any kind of network routing or routing preferences. Now we can see here we have three options, we have a public endpoint for all networks, public end point for selected and private end points.</p>
<p>Essentially, storage accounts have a public endpoint that’s accessible through the internet. That’s what this public endpoint would be. Now, if we select public endpoint, what we do is we’re enabling that public endpoint to all networks, that’s why we have all networks here. If we select the second option here, we can see we then allow a public endpoint but then we can select which networks can access this storage account. </p>
<p>So that allows us to segment our traffic and block certain traffic. And then we have private endpoint. Now you would create a private end point to allow only private connections to this storage accounts. Now, what this would do is assign a private IP address from the virtual network that we select and it would take it and assign it to these storage accounts. And then as a result, all traffic between that virtual network and the storage account would be secured over a private link.</p>
<p>For this exercise here, we’re just going to go public and the only option we have here for routing is Microsoft network routing defaults and that’s because the combination of the storage account kind performance and replication along with location does not support internet routing which we probably wouldn’t do anyway.</p>
<p>We’ll go into data protection and then we can see, we have the Blob soft delete option, we have the file share soft delete option here and then we have versioning but versioning is not offered for this storage account due to the type of storage account combined with the subscription replication and location options.</p>
<p>So we’ll leave these at their defaults. We’ll click Next through to advanced. Now here’s where we could configure some advanced features. We could configure the secure transfer which essentially enhances the security of the storage account by only allowing the requests to that storage account by secure connections.</p>
<p>When you enable the Blob public access, the storage accounts Blobs can be read publicly without needing to share an account key or even a shared access signature. So your Blobs are wide open. We have the TLS versioning. Now this large file shares option here, turning this on provides file share support for a maximum of a hundred terabytes.</p>
<p>Now you’ll notice here that large file share storage accounts don’t have the ability to convert to geo-redundant storage offerings. And down here, we have some Data Lake Storage Gen2 options. We’ll leave these other defaults, we’ll click next for tags, we’re not going to do any tagging here. This just allows us to categorize our resources and then we’ll go ahead and click Next to Review and Create. And at this point we can see our validation has passed, we can review our configuration and then we can go ahead and click Create. And what this will do is deploy our general V2 storage account. And we can go ahead and click, Go to Resource and we are now in our test9878 storage account.</p>
<p>So that is how you walk through the process of creating a storage account in the Microsoft Azure portal.</p>
<h1 id="An-Introduction-to-Blob-Storage"><a href="#An-Introduction-to-Blob-Storage" class="headerlink" title="An Introduction to Blob Storage"></a>An Introduction to Blob Storage</h1><p>Hello and welcome to blob storage. In this lesson, we are going to take a closer look at blob storage and when you should use it.</p>
<p>Azure Blob storage, as I mentioned earlier in this course, is an object storage solution in Azure. It’s optimized to allow the storage of massive amounts of unstructured data, including text and binary data.</p>
<p>You would typically use blob storage to host images and documents that you wish to serve up to a web browser. Think images on a website. Blob storage is also used when you wish to stream video or audio or to store log files. Organizations will also often use blob storage to store backup data, archive data, and data that needs to be analyzed by some on-prem process or Azure-hosted process.</p>
<p>To access objects in blob storage, you can use HTTP or HTTPS. You can also access blob storage objects through the Azure storage rest API, through Azure PowerShell, through the Azure CLI, or through an Azure storage client library such as Java, PHP, .NET, or several others.</p>
<p>There are three types of resources that you should be familiar with when discussing blob storage. They include the storage account that hosts the blob storage, containers within the storage account, and the blobs within those containers.</p>
<p>The image on your screen depicts the relationship between these resources.</p>
<p>The storage account creates the unique namespace in Azure that you use to access your data. When you access your data, you’ll use a combination of the storage account name and the Azure Storage blob endpoint that you are trying to access. Together, these two pieces form the base address for the objects that reside in the storage account.</p>
<p>For example, the URL that you see on your screen would be used to access blob storage in a storage account called MyStorage:</p>
<p><a target="_blank" rel="noopener" href="http://mystorage.blob.core.windows.net/">http://mystorage.blob.core.windows.net</a></p>
<p>Containers within a storage account are used to organize the blobs within the account. You can view containers like directories in a file system. You can create an unlimited number of containers within a storage account, and each container can store an unlimited number of blobs.</p>
<p>There are actually three types of blobs that Azure storage supports. They include block blobs, append blobs, and page blobs. Block blobs can contain up to about 190.7TiB of text and binary data. They are called block blobs because they consist of blocks of data that can be managed individually.</p>
<p>Append blobs are similar to block blobs insofar as they, too, consist of blocks of data. However, unlike block blobs, append blobs are optimized for append operations. This makes append blobs a good choice for logging data from virtual machines.</p>
<p>Page blobs are used to store random access files up to 8 TiB in size. You would typically use page blobs to store VHD files, which would serve as disks for Azure virtual machines. </p>
<p>To learn more about blob storage, visit the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction">https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction</a> </p>
<h1 id="DEMO-Uploading-a-Block-Blob-to-Azure"><a href="#DEMO-Uploading-a-Block-Blob-to-Azure" class="headerlink" title="DEMO: Uploading a Block Blob to Azure"></a>DEMO: Uploading a Block Blob to Azure</h1><p>Hello and welcome back. In this demonstration, we are going to create a container in our storage account. And then what we’re going to do is upload a block blob to that storage account. Now, before we upload our block blob to our storage account, we need to create a container.</p>
<p>The container is what will host our blob. So on the screen here, we have my test9878storage account. To create my container here, I simply select the containers chiclet here. And I can see I have no containers configured yet. So what we’ll do here, we’ll create a container.</p>
<p>Now the container name must be lowercase. So we’re gonna call it mycontainer. And we’ll create it. So now we have our container created. We can see that the default access level for that container is private. There’s no anonymous access to it. With our container created, we can upload our block blob to our storage.</p>
<p>To upload our block blob, we simply select our container and then we click upload here. And then in the right pane here, we can select the file we want to upload. And we’re just going to upload this, mytextdocument.text. And then we have an overwrite if it already exists. And then some advanced features here. None of these are required. We just get to specify what kind of authentication type, block size, the access tier and any folders we want to upload to.</p>
<p>For this demonstration, we’re just going right into the container. And then we’ll go ahead and click upload. And we can now see my text document listed in our container. We’ll close this out. And with that, we now have our container created and we were able to upload a block blob or file to our container.</p>
<p>Now, if we wanted to download our block blob, we simply right click it, and then we could select download here. So with that, you now know how to create a container in your blob storage and you know how to upload and download block blobs to, and from it.</p>
<h1 id="An-Introduction-to-Azure-Files"><a href="#An-Introduction-to-Azure-Files" class="headerlink" title="An Introduction to Azure Files"></a>An Introduction to Azure Files</h1><p>Hello and welcome to an intro to Azure Files. In this lesson, we are going to take a look at what Azure Files is and talk a little bit about what it offers.</p>
<p>Azure Files is an offering that makes file shares available in the cloud. It’s a fully managed solution that supports access to these cloud-based file shares via the industry-standard server message block protocol, or SMB.</p>
<p>You can mount Azure file shares from cloud deployments and on-prem deployments of not only Windows machines, but also Linux, and Mac OS machines. You can also use the Azure file sync service with Azure Files to cache your Azure file shares on Windows servers that are located close to your users. By leveraging Azure file shares with Azure file sync, you can speed data access for your end users. </p>
<p>Organizations will often use Azure Files to replace on-prem file servers or to supplement them. While earlier iterations of Azure Files were not a good replacement for on-prem file servers, this is no longer the case. Because popular operating systems like Windows, Linux, and Mac OS can mount Azure file shares, Azure Files can now completely replace traditional on-prem file servers and even NAS devices. As a matter of fact, the release of Azure Files AD Authentication means Azure file share permissions can even be controlled through on-prem active directories.</p>
<p>Azure Files is also helpful when lifting and shifting applications to the cloud. This is especially true for applications that require file shares to store application data and user data.</p>
<p>Because Azure Files are fully managed, you can create as many file shares as you need without worrying about hardware management and OS installation. This means you also have no need for OS patching or security upgrades.</p>
<p>You can also use familiar PowerShell commands and Azure CLI commands to create, mount, and manage Azure file shares. They can also be created and managed through the Azure portal and through Azure Storage Explorer.</p>
<p>Because Azure Files are built to be resilient, you no longer need to worry about file server upgrades or local power outages and network issues that typically affect access to on-prem file shares.</p>
<p>Join me in the next lesson, where I will show you how to create an Azure file share using the Azure portal.</p>
<h1 id="DEMO-Creating-an-Azure-File-Share"><a href="#DEMO-Creating-an-Azure-File-Share" class="headerlink" title="DEMO: Creating an Azure File Share"></a>DEMO: Creating an Azure File Share</h1><p>Hello and welcome back. Now, once we have a storage account created, we can go ahead and create file shares if we’re going to use the Azure file service. What I’m going to do here is show you how to create a file share within an Azure Storage account.</p>
<p>Now on the screen, you can see I’m in the storage account and I’m on the overview page. Now, from this overview page, to create a file share, I simply navigate to the file shares tab. The file shares tab allows us to see any file shares we’ve already created. And as you can see on the screen, we have none.</p>
<p>To create a new file share, we simply click the file share link here, and then we can give our file share a name and a quota. So what we’ll do here, we’ll call this file share MyShare. Now we can see the little exclamation point here and this is telling me that the file share name can only be lowercase letters, numbers, and hyphens. So we’ll go ahead and rename this myshare in lowercase letters.</p>
<p>Now, if I hover over the quota icon here, we can see that we can set a quota essentially up to five terabytes that limits the total size of this file share. We are not going to set a quota here, so we’ll go ahead and click create. And now we can see that our share is created and that we have the default quota. Since we didn’t specify one, it’s just giving us the entire size that’s available to us for the share.</p>
<p>If we select the share here, we can click the connect link here and this will give us information on how to connect to this share from Windows, Linux, or MacOS. We can also upload files directly to it, and we can add a directory, so let’s call this marketing, and now we have a directory within our myshare file share called marketing. So you can build a hierarchy of your file shares within your Azure portal.</p>
<p>So with that, you know how to create a file share in Microsoft Azure using the Azure portal.</p>
<h1 id="An-Introduction-to-Azure-Queues"><a href="#An-Introduction-to-Azure-Queues" class="headerlink" title="An Introduction to Azure Queues"></a>An Introduction to Azure Queues</h1><p>Hello and welcome to Azure queues. In this brief lecture. We are going to take an introductory look at Azure queues and what they are used for. </p>
<p>The Azure queues storage service is designed to store large numbers of messages. Now these messages aren’t the type that you would normally think of. We’re not talking about emails or anything like that. Instead, these messages are used to facilitate communication between the components of distributed applications. When using Azure queue storage, you can access these messages from anywhere in the world through authenticated calls via HTTP or HTTPS.</p>
<p>The Azure queue service is comprised of several components. These include the URL format, a storage account, a queue, and messages.</p>
<p>To access a queue, you must do so through a specific URL format. The URL for a specific queue will include the storage account name and the queue name. For example, the URL that you see on your screen would be used to access a queue called images to process in a storage account called mystorageaccount.</p>
<p>Speaking of storage accounts, all access to virtually all Azure storage services is provided through a storage account. You can view the storage account as the overarching container that hosts your Azure storage.</p>
<p>The queue itself is actually a set of messages. When you name a queue, you must use all lowercase letters.</p>
<p>And last but not least we have messages. Messages in any format can be up to 64 kB. As I mentioned previously, these messages are used to facilitate communication between the different components of a distributed application.</p>
<p>Join me in the next lesson where I will show you how to create a queue in Microsoft Azure.</p>
<h1 id="DEMO-Creating-a-Queue"><a href="#DEMO-Creating-a-Queue" class="headerlink" title="DEMO: Creating a Queue"></a>DEMO: Creating a Queue</h1><p>Welcome back. In this quick demonstration, I want to show you how to create a queue in Microsoft Azure using the Azure portal. Now, since this isn’t a course on using queues, we’re not going to get into all the details of how to use it, but I wanted to at least show you how to create a queue.</p>
<p>On the screen here, you can see I’m logged into my Azure portal. I’m at the home page and I’m logged in as an admin. To create a queue service, what I need to do is browse to my storage accounts. And then from the overview page of my storage accounts, I simply click on the queues box here.</p>
<p>Now from the queues page, I can see what existing queues I have set up. I can also see what authentication method I’m using. Now to create my queue, it’s a pretty straightforward process. I simply click the plus queue button up here and then give my queue a name.</p>
<p>Now my name for the queue needs to be all lowercase. It must start with a letter or a number, and it can only include letters, numbers, or hyphens. So I’m just going to call this myqueue. And then we’ll click okay here. And then we can see here myqueue is created and it gives me the URL to access that queue. And that’s really all there is to creating a queue.</p>
<p>Now, before we go, I’ll just show you quickly how to add a message to that queue through the portal. And to do that, I simply click on my queue here and then add a message. Now what I’ll do here, I’ll just add some kind of message text here. And we can set the expiration. I’ll just leave the defaults there.</p>
<p>Now because Microsoft recommends encoding binary data, this checkbox here for encode the message body in Base64 is checked by default. So we’ll leave it there and we’ll go ahead and click okay. And now we can see the message in our queue. If we select the message, we can look at some of the message properties. We can see the body of the message, when that message was inserted into the queue and when it expires. </p>
<p>Now, if we wanted to dequeue this message or remove it from the queue, we simply click the dequeue message. Now since this is the first and only message in my queue, it’s asking me if I’m sure I want to remove the first message in the queue. We’ll go ahead and click yes. And now we can see, we have no more messages in our queue.</p>
<p>So that’s the long and short of it on how to create a queue, how to add a message to a queue, and how to dequeue that message.</p>
<h1 id="An-Introduction-to-Azure-Table-Storage"><a href="#An-Introduction-to-Azure-Table-Storage" class="headerlink" title="An Introduction to Azure Table Storage"></a>An Introduction to Azure Table Storage</h1><p>Hello and welcome to Azure table storage. In this lesson, we will take a look at what table storage is and what it’s used for. </p>
<p>Azure table storage is a storage service offered by Microsoft that allows you to store structured no SQL data in the cloud. It provides a key&#x2F;attribute store with a schema-less design. When I say schema-less here, what I mean is that the data does not conform to a rigid schema. In other words, it doesn’t conform to specific data types that a typical relational database would conform to. </p>
<p>Organizations will often use table storage to store flexible databases that include things like user data for Web applications or maybe even address books or device information. Table storage allows you to store all kinds of entities in a table. Storage accounts can contain as many tables as you need up to the capacity limits of the storage account itself. </p>
<p>Azure table storage, which is actually a NoSQL datastore, often comes into play when an organization needs to store large amounts of structured data. It’s perfect for scenarios that require the storage of structured non-relational data. For example, table storage would be a good choice if you need to store terabytes of structured data required for web scale applications. If you have datasets that don’t require complex joins, foreign keys, or stored procedures, table storage would be a good choice, assuming those data sets can be denormalized for fast access.</p>
<p>Leveraging Azure table storage allows you to store and query massive sets of structured nonrelational data.</p>
<p>Table storage consists of several different components. The URL format for accessing Azure table storage can be seen on your screen.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://&lt;storage account&gt;.table.core.windows.net/&lt;table&gt;</span><br></pre></td></tr></table></figure>

<p>Notice the URL includes the storage account name and the table name. It’s the inclusion of these values that makes the URL address unique.</p>
<p>The storage account is another component that helps comprise the Azure table storage offering. The function of the Azure storage account is simple. Access to all Azure storage, including table storage, is provided through a storage account.</p>
<p>The third piece of Azure table storage is the table itself. All a table is is a collection of entities. What makes an Azure table storage table different from a typical relational database table is the fact that it doesn’t enforce a schema on the entities within it. This means that a single table can, and often does, contain all kinds of entities with many different sets of properties.</p>
<p>Speaking of entities, the entity is the fourth component that makes up the Azure table storage offering. The best way to describe an entity is that it is similar to a typical database row. It’s essentially a set of properties. Each entity in Azure storage can be up to 1 MB in size.</p>
<p>The properties, which make up the fifth component, are name-value pairs. You can include up to 252 properties for each entity. There are also three system properties associated with each entity as well. These include a partition key, a row key, and a timestamp.</p>
<p>The image on your screen depicts the relationship among each of these components. As you can see the storage account holds the tables. Each table consists of the entities. Each entity consists of many different properties. The URL that we mentioned earlier is how you access all of this stuff.</p>
<p>In the next lesson, I will show you how to create an Azure storage table using the Azure portal.</p>
<h1 id="DEMO-Creating-an-Azure-Storage-Table"><a href="#DEMO-Creating-an-Azure-Storage-Table" class="headerlink" title="DEMO: Creating an Azure Storage Table"></a>DEMO: Creating an Azure Storage Table</h1><p>Welcome back. In this brief demonstration, we are going to walk through the process of creating an Azure Storage table in the Azure portal. As you can see here, I’m logged into my portal as my admin, and I’m on the homepage. We have my storage account here, test9878storage. This obviously is a prerequisite to create a table.</p>
<p>So what we’ll do to begin the process of creating our storage table is select our storage account. And then from the overview page, we can select tables. Now from this table storage page, we can see I have no tables defined yet.</p>
<p>Now, one thing I didn’t mention in the lecture is that there is a premium table experience and it’s not necessarily table storage per se so I didn’t cover it in the course, but this premium table experience is offered through Cosmos DB. And if I open this up here, it takes me out to Azure Cosmos DB and what Azure Cosmos DB is is a globally distributed database service.</p>
<p>Now, what Cosmos DB offers is a premium Azure table API. And so that’s why we didn’t get into it because it’s not really table storage as part of the Azure Storage service offering. But just keep that in mind that there is a premium experience for Azure table API if it comes up in an exam somewhere.</p>
<p>So let’s go back to our test9878storage here. So to create our table, we simply click plus table and we’ll give our table a name. I’ll just call it MyTable and we’ll OK it. And from there, we can see our new table has been created along with a URL to access that table.</p>
<p>If we click the ellipsis here for the context menu, we can configure access policies or delete the table altogether. Now, since this is an introduction to Azure Storage, we’re not going to get into table design and actually doing things with the table storage. I just wanted to introduce you, hence the name of the course, to this storage option so you have an idea of what it is, where you access it and how you create a table.</p>
<p>So with that, we’ll call it a wrap and I’ll see you in the next lesson.</p>
<h1 id="Introduction-to-Azure-Managed-Disks"><a href="#Introduction-to-Azure-Managed-Disks" class="headerlink" title="Introduction to Azure Managed Disks"></a>Introduction to Azure Managed Disks</h1><p>Hello and welcome to Azure managed disks. In this lesson, we’re going to take a look at the benefits of managed disks and at the types of encryption you can use with managed disks. Let’s start by identifying what a managed disk is in Azure. An Azure managed disk is similar to a physical disk that you would find in a physical on-prem server, but it’s virtualized. It’s a block level storage volume that’s used with Azure VMs and is managed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a>.</p>
<p>Creating a managed disk in Microsoft Azure is as simple as specifying the size of the disk, and the type of the disk you want to use. When you provision a managed disk, you have a choice of several different types. You can deploy ultra disks, premium SSD disks, standard SSD disks, and standard HDD disks. We will actually look at these types in a little more detail in the next lesson. But for now, we’re just going to look at the benefits of using managed disks.</p>
<p>For starters, managed disks are designed for 99.999% availability. That’s five nines of availability. To achieve this level of availability, there are three replicas of the data stored on each managed disk. This type of durability protects you from not only one, but two failures of disk replicas.</p>
<p>Managed disks make it easy to deploy and scale VMs. Microsoft Azure allows up to 50,000 VM disks of a specific type per region in each subscription. This allows you to create thousands of virtual machines in one subscription. Because Azure supports so many disks, you can create VM scale sets that include up to 1000 VMs per set, provided you use a marketplace image. </p>
<p>I should point out that managed disks are integrated with both availability sets and availability zones. The integration with availability sets ensures that VM disks within an availability set are isolated from one another. This protects your applications from a single point of failure within an Azure datacenter. Availability zone integration protects applications from entire Azure datacenter failures.</p>
<p>Since Azure backup supports the backup and restore of managed disks, you can use Azure backup to create backup jobs to protect your data. This makes VM restores a snap. I should mention, however, at the time of this course publication, Azure backup supports disk sizes up to 32 terabytes.</p>
<p>Through Azure role-based access control, or RBAC, you can specify granular access control for managed disks. You can assign specific permissions for managed disks to your users.</p>
<p>Lastly, Azure managed disks make it easier to upload your on-prem VMs to Azure because you can use direct upload to transfer your VHD files to Azure managed disks. There are far fewer steps to uploading your VHDs than there used to be.</p>
<p>There are two types of encryption that you can use with managed disks. They include server-side encryption, or SSE, and Azure disk encryption, or ADE.</p>
<p>Server-side encryption is performed by the Azure storage service, and is enabled by default for all managed disks. This type of encryption provides encryption at rest for your data. Server-side encryption is also enabled by default for snapshots and images in regions where managed disks are available.</p>
<p>Azure disk encryption is enabled on the OS and data disks of a VM. Using Azure disk encryption, you can encrypt the OS and data disks for a virtual machine, including managed disks. On Windows VMs, the disks are encrypted using bit locker technology. While on Linux VMs, the disks are encrypted using DM-crypt technology.</p>
<p>There are three disk roles in Azure. These roles include data disks, OS disks, and temporary disks.</p>
<p>Data disks are managed disks that you attach to a virtual machine. They’re used to store applications and other sorts of data that you need. When you attach a data disk to a VM, it’s registered as a SCSI drive. You can assign a drive letter to a data disk just like any other physical disk in a physical server. Data disks have a max capacity of 32 terabytes, and the number of data disks that you can attach to a virtual machine will be determined by the size of the virtual machine itself.</p>
<p>OS disks are pretty self-explanatory. When you deploy a virtual machine, it’s deployed with a single OS disk attached. The OS disk, as you may have guessed, hosts the VM’s operating system and boot volume. The max capacity of an OS disk is four terabytes.</p>
<p>Temporary disks are probably the most misunderstood of the three disk types. Every VM contains a temporary disk. I should mention, however, that the temporary disk is not a managed disk. The temporary disk is not intended for storage of important data. Instead, temporary disks are used to host things like page files and swap files. Data that is stored on a temporary disk is often lost during maintenance events and when a VM is redeployed. The temporary disk is assigned the drive letter of D on Windows machines by default. On Azure Linux VMs, the temporary disk is &#x2F;dev&#x2F;sdb.</p>
<h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><p>Congratulations. You’ve come to the end of Introduction to Azure Storage. Let’s review what you’ve learned.</p>
<p>We kicked things off with an introduction to azure storage. You learned about the core storage services in Azure and about the different storage account types that are available. You also got to see a demonstration that showed you how to create a storage account in Microsoft Azure.</p>
<p>We then took a look at the different storage services in a little more detail. We started with blob storage, where you learned what it is and what it offers. Next, we dove into Azure Files.</p>
<p>After covering Azure Files, we got into Azure queues, where you learned what Azure queues are and how to create a queue.</p>
<p>After learning about Azure queues, you learned about Azure tables. You learned why they are used and when to use Azure them.</p>
<p>We wrapped up with Azure disks. </p>
<p>At this point, you should have a solid understanding of the storage offerings in Microsoft Azure.</p>
<p>To learn more about Microsoft Azure storage options, you can, and should, read Microsoft’s published documentation. You should also keep an eye out for new courses on Cloud Academy, because we’re always publishing new ones. Be sure to give this course a rating, and if you have any questions or comments, please let us know. Thanks for watching and happy learning.</p>
<h2 id="3Storage-Account-Types-in-Azure"><a href="#3Storage-Account-Types-in-Azure" class="headerlink" title="3Storage Account Types in Azure"></a>3<strong>Storage Account Types in Azure</strong></h2><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview">Azure Storage Accounts Overview</a></p>
<h2 id="5An-Introduction-to-Blob-Storage"><a href="#5An-Introduction-to-Blob-Storage" class="headerlink" title="5An Introduction to Blob Storage"></a>5<strong>An Introduction to Blob Storage</strong></h2><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction">Azure Blob Storage Overview</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Lab-Integrate-Services-with-Azure-Function-Apps-16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Lab-Integrate-Services-with-Azure-Function-Apps-16/" class="post-title-link" itemprop="url">AZ-204-Lab-Integrate-Services-with-Azure-Function-Apps-16</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:12:15 / Modified: 11:12:16" itemprop="dateCreated datePublished" datetime="2022-11-14T11:12:15-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Lab-Integrate-Services-with-Azure-Function-Apps-16/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Lab-Integrate-Services-with-Azure-Function-Apps-16/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Introduction-to-Azure-Functions-15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Introduction-to-Azure-Functions-15/" class="post-title-link" itemprop="url">AZ-204-Introduction-to-Azure-Functions-15</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:11:30" itemprop="dateCreated datePublished" datetime="2022-11-14T11:11:30-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 00:12:24" itemprop="dateModified" datetime="2022-11-15T00:12:24-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Introduction-to-Azure-Functions-15/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Introduction-to-Azure-Functions-15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Aloha, I’m David Gaynes. And I have been developing .NET and cloud software for more than 20 years. I spent some time at Microsoft, and I’ve written a book or two in the past as well. This course is intended for software developers who want to learn how to implement <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/azure-functions-overview/">Azure Functions</a> as a part of their cloud software design. You don’t need any specific background to get something out of the course, but it will certainly help if you understand event driven programming, what servers and APIs are in general and if you have experience with C#, JSON and how to create a project in Visual Studio.</p>
<p>Azure Functions are little bits of your application logic that live in the cloud. You can think of them in the same way that you think of any API endpoint, meaning you can get to an <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> Function using HTTP. You just call them whenever you need to run that logic. The course will include how to activate, or what we call trigger, your Azure Functions, how to pass data to and from them and also how to tie different Azure Functions together.</p>
<p>Note that this course covers Azure Functions 2.0 and up. As is typical with Microsoft, there were several breaking changes from V1 to V2. So if you have used Azure Functions V1 in the past, then, obviously, there are some places where the material in this course won’t match what you are used to.</p>
<p>For any additional help, just email <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. And please rate my course when you’re finished. Thanks! And on we go.</p>
<h1 id="Azure-Functions-Overview"><a href="#Azure-Functions-Overview" class="headerlink" title="Azure Functions Overview"></a>Azure Functions Overview</h1><p>You would typically use <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Functions</a> to run bits of your overall application logic on-demand in the same way that you use any other HTTP-accessible endpoint. Just as with any other API endpoint or library function, you create a serverless <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> Function to do some compute work as a part of a larger logic landscape. You can configure Azure Functions and pass data to and from them using a wide range of architectures.</p>
<p>As I mentioned, Azure Functions are just HTTP endpoints that you can call in the same way that you call any other API endpoint from within your code or other process. However, instead of residing on a standard web server that is operational 24&#x2F;7&#x2F;365, your Azure Function is available, quote, on demand whenever you call it. The endpoint is not provisioned into the cloud until it is called. As a result, because your cloud services charge you per minute for any resource in use, an Azure Function is typically less costly to operate than a typical API endpoint web server that’s running and available all the time.</p>
<p>And when do you use Azure Functions? Well, Azure Functions have <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/introduction-to-triggers/">triggers</a> that cause them to execute. So they are especially useful when responding to events such as data coming or going, timers going off, or even HTTP requests coming in. If you have some code that you want to execute based only on a particular event, one that may or may not occur regularly, Azure Functions can be a solution.</p>
<p>As I mentioned, triggers for your Azure Function can include timers, data operations, and standard HTTP calls or webhooks. Triggers are created in a configuration file that is a companion to your Azure Function. As we’ll see when we write our first Azure Function, the trigger type you choose actually defines your project.</p>
<p>Azure Functions connect to your data stores and resources through the use of bindings. Input bindings provide any needed data to your Azure Function and output bindings receive any results. Bindings are created in the same configuration file I mentioned that is the companion to your Azure Function when you publish it.</p>
<p>We will cover the implementation of triggers and bindings in the first part of the course.</p>
<p>Because <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> Functions employ the on-demand services model I mentioned earlier, they are typically stateless. Azure Functions are serverless meaning there is no host server to hold or store information between calls. Your Azure Function is provisioned into the cloud when it needs to run and de-provisioned immediately afterwards so there is no concept of state connected to it.</p>
<p>However, there is an extension to standard Azure Functions called Durable Functions that does allow you to manage state&#x2F;coordination of different Azure Functions. It’s a very useful feature in a serverless environment. We will cover the use of Durable Functions in the second part of the course.</p>
<h1 id="Introduction-to-Triggers"><a href="#Introduction-to-Triggers" class="headerlink" title="Introduction to Triggers"></a>Introduction to Triggers</h1><p>Triggers are how your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Function</a> starts, so it seems the best place for us to start as well. A trigger defines how your function is invoked. There are several kinds of triggers you can employ to run your function when desired. In general, they have three primary types, including HTTP&#x2F;Webhooks, timers, and data operations.</p>
<p>So for HTTP trigger, your Azure Function fires whenever an HTTP request hits that endpoint.</p>
<p>For a generic webhook trigger, your function fires when a webhook HTTP request, hits that endpoint from any service that supports webhooks, such as Stripe or Twilio.</p>
<p>A GitHub webhook trigger in specific means that your function fires when an event occurs in your GitHub repository, such as create, delete, download, fork, push, pull requests, commit comment or any of the other several dozen GitHub events.</p>
<p>For a timer trigger your function is called at intervals, we’ll actually be creating a timer trigger in our exercise.</p>
<p>A queue trigger means your function fires when a new message comes into your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> queue storage. The message contents are then passed on as input to the function. We will also be doing a queue trigger in our exercise.</p>
<p>For a service bus trigger, your function fires when a new message arrives from a service bus queue or topic. The message contents are then passed in as input to your function.</p>
<p>A BLOB trigger means that your function fires when a new or updated BLOB is detected, and then the BLOB contents are passed to your function as input or an event hub trigger your function fires when any events are delivered to an Azure Event Hub.</p>
<p>To define those triggers and other bindings for your Azure Function, you create a function.json file that creates the configuration metadata for the function itself. Here you can see an example of a function.json file.</p>
<p>You can see within it that a function can only have a single trigger binding. As we’ll discuss later, it can have multiple input slash output bindings. All bindings and triggers will have a direction property and note that for triggers, the direction is always in.</p>
<p>Here you can see examples of several common types of trigger bindings used for implementation of those that we just listed. This code would all be found in your function.json file, typically auto generated by Visual Studio, but you do have direct access to edit it when you desire.</p>
<p>So you can see httpTrigger with an authorization level, a timerTrigger with a schedule, a blobTrigger with the file path to the blob, a queueTrigger with the name of the queue, and so on all of the details necessary for your binding to enable itself to be properly connected to whichever resource you are binding to.</p>
<p>The easiest way to create an Azure Function, and to see all of this is using Visual Studio. You just write your functions, starting with the templates provided, and then publish in the usual way to your Azure Cloud. Note that because any Azure Function can have exactly only one trigger as specified, the trigger type you intend to use is what differentiates your Azure Functions when you <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/demo-creating-a-new-function/">create them</a> in Visual Studio, so let’s take a look at that.</p>
<h1 id="DEMO-Creating-a-New-Function"><a href="#DEMO-Creating-a-New-Function" class="headerlink" title="DEMO: Creating a New Function"></a>DEMO: Creating a New Function</h1><p>To begin, we’ll be creating the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Functions</a> equivalent of Hello World. Although, in this case, we’ll actually be logging to our console. So, just use your create a new project template in the typical way to create an Azure Function project. If you don’t see it immediately in your list of projects, you can always use the filter control at the top by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a>, and you’ll find it quickly.</p>
<p>We’re gonna go ahead and name our function according to the primary type of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/introduction-to-triggers/">trigger</a> that we intend to use, which in this case for simplicity’s sake, we’ll just go ahead and make a timer. So, we’ll call our project, AzureFunctionTimer. And although we will be using a couple of other types of timers throughout the project, including an HTTP trigger, which is the kind that you would want to use to access your function as an API endpoint.</p>
<p>As we discussed earlier, we’ll also be touching on Queue trigger, but, as I mentioned for the moment for simplicity’s sake, we’ll start with a Timer trigger that runs on a simple interval. The interval that we’ll specify here will be the default value of five minutes, and we’re also going to want to be engaging our default development storage emulator provided by Visual Studio and Azure, which is the AzureWebJobsStorage account. That’s going to allow us to interact with our local Blob storage, Queue storage, anything that we might need as Azure storage without having to provision any live resources into the cloud, which would incur charges and require a live Azure account.</p>
<p>So, for development work, we can use the free local storage emulator provided by Visual Studio. We’ll go ahead and create that function, and let Visual Studio do its magic for us. And when it’s finished doing so, you can see that we have a base function signature for an Azure function, and that it includes the necessary trigger attribute applied to the first argument, which in this case is the Timer trigger with the interval specified in part of its constructor argument.</p>
<p>That attribute applies to the TimerInfo variable, myTimer. And, in this function, all we’re going to be doing is logging information that lets us know, yes, that function ran. To do so, we’re going to use the dependency injection ILogger variable log. If you’re not familiar with dependency injection, don’t worry about it for right now. It just means that Visual Studio and Azure have some built-in functionality that you can access by using the right types of arguments in your function’s signature, things like logging and configuration.</p>
<p>So, in this particular case, we’re gonna go ahead and use logging, and then we’re just gonna go ahead and run that function and see the sort of output that we get. And you’ll see the first thing that happens when you run locally is that Azure Functions will pop up your console, so that you can see the logging activity that’s happening inside of your function.</p>
<p>Here, you can see it picking up some configuration information, including the Cron schedule here that indicates that this function will be running every five minutes at five, 10, 15, 20, 25 minutes past the hour, etc. And so, as soon as we hit one of those specific timer intervals, our Timer trigger will fire, and there it is. As we see, we have successfully executed Function1. Because the timer fired, we have logged our information to the console, and we have confirmed that our function has executed successfully from end-to-end. So, congratulations, you have created your first Azure function.</p>
<p>Now, if you’d like to see the function.json file that goes along with that Azure function and defines all of the configuration inside the function, you can drill down by doing Show All Files inside of your Visual Studio. Then, you’re gonna want to go into the bin directory, into your compilation, down inside of Function1, and there you will see your function.json file with the specified trigger binding automatically injected into it. So, your function.json file will hold all of your bindings, all the bindings that are created throughout the rest of the project, triggers, or input or <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/introduction-to-output-bindings/">output bindings</a>, whatever they are, but they will all go into your function.json file, which was automatically created when you built the project and will automatically be deployed with your project when you deploy it into the cloud.</p>
<h1 id="Introduction-to-Output-Bindings"><a href="#Introduction-to-Output-Bindings" class="headerlink" title="Introduction to Output Bindings"></a>Introduction to Output Bindings</h1><p>Creating and then deploying your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Function</a> to the cloud in your usual way in Visual Studio automatically <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/demo-creating-a-new-function/">creates</a> and deploys that function.json file we talked about earlier that contains the configuration&#x2F;metadata information for your Azure Function that it requires based on the code you just wrote. The cloud runtime environment reads the file when it spins up your Azure Function and then runs the logic in the function itself. All in response to the activated trigger.</p>
<p>Triggers are one kind of binding. But Azure Functions also make use of input and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/demo-output-bindings/">output bindings</a> to allow you to easily connect to your related cloud resources such as data and other resources. Azure Functions 2.0 and later supports a variety of bindings that you can use for inbound or outbound connection to those resources.</p>
<p>Note quickly that all bindings except HTTP and Timer must be registered. .NET functions can access and register bindings just by using NuGet packages. Other functions can use extension bundles to access bindings through configuration settings.</p>
<p>Notice in the function.json file that input and output bindings can use directions both in and out, depending on the type of the binding. And some bindings even support a special direction InOut that is available using the Advanced editor in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> portal, but we won’t be talking about InOut bindings in this opening course on Azure Functions.</p>
<p>So what kinds of resource bindings are supported in Azure Functions? Well, for input you’ve got Blob Storage, Cosmos DB, IoT Hub, Microsoft Graph Excel tables, Microsoft Graph OneDrive files, Microsoft Graph events, Microsoft Graph Auth tokens, mobile apps, SignalR, Table storage and even Twilio. And for output bindings, it’s all of the above except for the Graph Auth tokens, IoT Hub and SignalR. Plus, Event Grid, Event Hubs, HTTP and webhooks, Microsoft Graph Outlook email, Notification Hubs, Queue Storage, SendGrid, and Service Bus. </p>
<p>In other words, your ability to create interactions in your Azure Functions between the various parts of your cloud architecture is nearly limitless through the use of input and output bindings.</p>
<p>So let’s continue our code example by adding an Output binding to our Azure Function. Suppose that we have a scenario in which whenever our current Timer triggered Azure Function goes off, we want to send a message to our Queue. Let’s just hop back into Visual Studio to implement that code.</p>
<h1 id="DEMO-Output-Bindings"><a href="#DEMO-Output-Bindings" class="headerlink" title="DEMO: Output Bindings"></a>DEMO: Output Bindings</h1><p>Implementation of your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/introduction-to-output-bindings/">output binding</a> in your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Function</a> inside of Visual Studio is very straightforward. All you’re actually doing is adding an output parameter argument to your function signature and then applying an attribute to that argument to actually indicate the binding that you’re looking to create.</p>
<p>So you can begin to see a pattern developing here. For example when we used the TimerTrigger attribute, we simply applied that attribute to our initial argument of the type that we want it to be using. And we’re doing exactly the same thing in our second argument where we apply this binding attribute for the queue to the out message that we’re going to be creating using our queue.</p>
<p>If the Visual Studio that you’re using doesn’t recognize this particular item, make sure that you have a couple of NuGet packages installed which would be the Microsoft.Azure.Storage.Queue and then the Microsoft.Azure.WebJobs.Extensions.Storage will give you everything that you’re gonna need to be able to interact with your queue both presently and in the future if you’re ever gonna be creating functions to peek into your queue or anything like that.</p>
<p>So those two namespaces will give you everything that you need. And then the only caveat here before we run our function would be to make sure that we actually have a queue that matches this name that exists because what our code is saying here is that when our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/demo-output-bindings-with-triggers/">trigger</a> function runs, we will automatically be sending data.</p>
<p>Because it’s an output binding, we’re going to be sending data to the object specified here in this argument. So to make sure that our queue exists, we just pop open our local cloud explorer, and it will include a menu item for local, you may have other connections established, but initially all you’re interested in for this project would be your local connection, using that as your WebJobs storage connection string. And you can see as you drill down that in your storage account you’ll have blob containers and queues and tables and so on and so forth. And so in our case we’re just checking our queues that we, yes, we do have an item in here called thequeue, and if we right-click on that queue, we can pop it open and we can actually see that there is nothing in it. Queue contains no messages.</p>
<p>So in theory when we run our function, we should be able to go back into that queue and see that this QMessage, Timer triggered, has been created. So you’ll note that there is another name for the string value that we’re passing to QMessage. I’ll get into the details of this in just a second.</p>
<p>We’re gonna use this when we do a combination of an input binding to combine with our output binding. But again right now our output binding simply acts as a receiver for data that we’re gonna be sending, in this case it’s our queue object and the particular message that we’re going to be sending is Timer triggered. This should all happen automatically when our timer function runs.</p>
<p>So we’ll just go ahead and run the function and see what we get. And so Visual Studio pops open our console as is typical for a local development Azure Function. It reports on its status in the usual way. And then we can see that our function has executed. So our Timer trigger function has executed and successfully.</p>
<p>So now in order to verify that everything worked properly with our queue, we just close that up, stop the host, head over to our queue, open it up, and we see that we have the first message successfully in there, Timer triggered. If you’re curious about what that looks like inside your function.JSON file, here it is, here is your trigger binding with your queue output. </p>
<p>Remember that Visual Studio takes care of all of this by simply creating the arguments properly with the attributes inside of your Azure Function. So typically you don’t even have to see inside of your function JSON file, you don’t have to make any changes to it. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> and Visual Studio will automatically take care of all of those bindings for you. So you have successfully implemented your first output binding in an Azure Function.</p>
<h1 id="DEMO-Output-Bindings-with-Triggers"><a href="#DEMO-Output-Bindings-with-Triggers" class="headerlink" title="DEMO: Output Bindings with Triggers"></a>DEMO: Output Bindings with Triggers</h1><p>So now that we’ve done that, successfully added an <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/demo-output-bindings/">output binding</a> to our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Function</a>. Recall that, queue can actually be a trigger as well. So we have now created a timer trigger to inject something into our queue, we could create another Azure Function that would respond to that queue entry, as a queue trigger.</p>
<p>So let’s go ahead and add another Azure Function. We’ll just go ahead and call that function two for the moment. Then when we do, we’ll get the choice of trigger types that we wanna create. We select queue trigger. We go ahead and we use the same <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> web jobs storage that we have been using for our default development environment. We can even rename the queue according to the particular queue that we have been working with. And then we select OK.</p>
<p>Visual Studio will whip up our new as your function with a queue trigger. And this particular function, again indicated with a queue trigger binding is named properly and using our connection and all this function does is again use the dependency injection parameter for logging. Logging information that says that our queue trigger function has actually run and then it should indicate the content of the queue item when we run the function.</p>
<p>So we just go ahead and run all of that and it should all weave together automatically. Once again, Visual Studio pops up our console so that we can see what’s going on inside of our Azure Function. And it reports back to us on its progress in the usual way. And it runs our function and we can see that everything that we wanted to happen has happened. Our timer trigger function has run and our queue trigger function has also run.</p>
<p>We can see that both functions succeeded. You can see the detail that our function with the queue trigger ran because a new queue message was detected. So our timer has run, our output binding has functioned in providing a message to the queue. And our queue trigger function has run in response to that.</p>
<p>So we have now chained together different Azure Functions with different triggers. That is one of the most valuable things that you can do with Azure Functions. You have a wide variety of these kinds of conditions that you can create. To use logical sequences in your programming a very valuable use of Azure Functions. And so you have successfully created your first function, your output binding, and your second function to respond as a trigger to that output binding.</p>
<h1 id="Introduction-to-Input-Bindings"><a href="#Introduction-to-Input-Bindings" class="headerlink" title="Introduction to Input Bindings"></a>Introduction to Input Bindings</h1><p>For input bindings, you’ll see in a moment that when using .NET, you just define the data types using function parameters in the typical way. For example, as your argument for an inbound stream, such as from a BLOB, you would use type stream from the standard System.IO namespace, while for the inbound text coming from a queue <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/introduction-to-triggers/">trigger</a> you would use a string argument. You can also use any custom type of your choice as an argument to de-serialize input to that object type.</p>
<p>For dynamically typed languages, such as JavaScript, you would need to set the dataType property in the function.json file as a part of that binding definition. For example, to read stream input from a BLOB, set the dataType to stream, like you see here.</p>
<p>Other options for dataType are byte array, binary, and string. So this time let’s go back into our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Function</a> inside Visual Studio so we can see an example of how we employ the use of an <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/demo-input-bindings/">input binding</a> as an argument.</p>
<h1 id="DEMO-Input-Bindings"><a href="#DEMO-Input-Bindings" class="headerlink" title="DEMO: Input Bindings"></a>DEMO: Input Bindings</h1><p>To implement an <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/introduction-to-input-bindings/">input binding</a> in your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Functions</a>, let’s go ahead and return to our queue trigger function and provide an attribute to that function so that we can create our binding. </p>
<p>Now remember that a queue trigger will be activated by any entry placed into the specific queue that is referenced. So although in our previous example, we were using our timer to create a queue entry, any connection to your queue from anywhere in your enterprise software that creates an entry in your queue will activate this trigger.</p>
<p>So let’s suppose that somewhere in your enterprise, you have a security application that takes a photograph every time a secure area or restricted area is violated, and that at the moment that that violation occurs an image is taken and sent to your blob storage. And that at the same moment, a queue entry is created saying, “Hey, there’s a new security image that somebody needs to review or that we need to process in order to find out what’s going on.” So what we need is an input binding that is expecting that data to be there, and then a queue trigger activated off of that queue entry, that will then go look for that image inside of our storage.</p>
<p>Here’s a quick look at what that activity does inside of your function, .json file, here you can see the direction of the input binding and the same specifications that we applied inside of our function parameter. So we’ve provided a blob attribute which references this container and this specific image.</p>
<p>So assuming that this image is here, we will not get a null value return. Let’s just go check our Cloud Explorer real quick and make sure yes, we do have an image in our Cloud Explorer that I put there earlier, QR2.jpg, which will match this name.</p>
<p>So when this trigger is fired, we should be able to have a valid log entry created that says, “New image was found,” and then just the size of the picture in this case, will help us separate which picture was found. Keep in mind that just for the moment that I have hard-coded that value, QR2.jpg so that we can verify our connection to blob storage and that when an image exists, we can actually effectively process it. We’ll soon be creating a dynamic value in there as a variable but in this case, we expect it QR2.</p>
<p>So let’s go ahead and run that function. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> will pop up your console as usual to let you see what’s going on inside of your Azure function. We can see that it has found two functions, both one and two. And we can also see that because we have a timer triggered, queue trigger firing, that we know was created by our timer, it has gone in and it has found the image that we were looking for QR2, 4388 bytes, we can go in, we can go back to our blob container, and we can verify that yes, the size of that image was 4.3 K.</p>
<p>So we know that in this particular case, the function has run properly, our connection to our storage is working. And now we can get on to an actual real-world example, in which we would not be providing a hard-coded value here. The key to this is that this value can be dynamic, based on the text content of the queue item that came in.</p>
<p>So we would want to have the queue item that had just come in from your security application, indicating some specific information in the queue, an image name that would help us go into our blob container and fetch that image.</p>
<p>So how do we do that? And the answer is by using special syntax, and a special argument called queue trigger, that is going to provide us with the text information that came into the my queue item argument that we have the queue trigger attribute applied to.</p>
<p>So now what we are expecting is that our blob will have an image of the same name as whatever the item was, that was created inside of our queue. So let’s go back into our Cloud Explorer, and we’ll pop open our blobs container again. And we’ll see that we have an image in here, an image in here that has been created in this case manually, but could have been sent in by our security application. That is called SecurityViolation.jpg.</p>
<p>So assuming that we create an item in our queue of the same name, we should now be able to see that a security image was found and with the actual length of that particular picture. So our queue item has to contain the word security when we create it. And of course, we need to actually have our function running to make sure that our Azure function will be responsive when we create our item in the queue.</p>
<p>So now that we know that we have that up and running, we just head back over to our queue and we create a message with very specific text called security violation, because that is the name of the image that we are expecting to be able to match. So we go ahead and we create that text entry in our queue. And as soon as we do, we can see that our queue trigger is immediately processed, and that it has indicated to us that it is in specific a security violation that it executed.</p>
<p>So now we have been able to see that our queue trigger Azure Function can be responsive to multiple ways of inputting information to our queue, that we can narrow down inside of that function, the specific item information that we’ve received from the queue, and that we can use that information to apply specific logic for that situation. And so you have effectively combined your input binding of expected data with your queue trigger function.</p>
<h1 id="Durable-Functions-Types-and-Uses"><a href="#Durable-Functions-Types-and-Uses" class="headerlink" title="Durable Functions: Types and Uses"></a>Durable Functions: Types and Uses</h1><p>Okay that covers our introduction to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Functions</a>. We got one running, we used bindings to interact with remote resources, we called functions from it, triggered functions from it and logged our activities. I spoke briefly earlier about the ILogger Dependency Injection that we were using for that.</p>
<p>Azure Functions 2.0 and later is much, much better at handling Dependency Injection than was version 1.0. And since it’s pretty much required in a stateless environment that’s a very good thing.</p>
<p>As I mentioned, if you’re not familiar with Dependency Injection, that’s okay. In this case the point of it is that Azure Functions as a runtime has some built-in tools such as Logging and Configuration that you can use for all of your Azure Function.</p>
<p>And you get access to those tools when you pass the right kind of object types in the Constructor of your Azure Function. Since you haven’t created those objects yourself you know that <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> must be doing it for you in the background. Because those background objects are actually controlling much of what happens in your function, Dependency Injection of objects is sometimes called Inversion of Control.</p>
<p>The need for Dependency Injection brings us back to a point I mentioned earlier and again just now, the whole on-demand serverless model is by nature stateless. It’s hard to hold state in something that doesn’t exist most of the time such as is the case with your Azure Function.</p>
<p>But what happens when you do have several Azure Functions and you would like them to be aware of what the others have done before they execute in a smarter way than we just employed using our bindings and triggers? It’s a common enough scenario and it’s why Durable Functions were created.</p>
<p>Durable Functions is an extension of Azure Function that lets you orchestrate, or tie together in one way or another, stateful functions in a serverless environment. The <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/use-of-orchestrator-and-activity-functions/">orchestration</a> is typically a sequence of logical processing. A Durable Function application is a solution you create that is composed of different Azure Functions, each one playing a different role within that orchestration.</p>
<h1 id="Use-of-Orchestrator-and-Activity-Functions"><a href="#Use-of-Orchestrator-and-Activity-Functions" class="headerlink" title="Use of Orchestrator and Activity Functions"></a>Use of Orchestrator and Activity Functions</h1><p>There are currently four durable function types in <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Functions</a>, Activity, Orchestrator, Entity, and Client. But we’ll start by looking at just the first two, Activity and Orchestrator. </p>
<p>Activity Functions are pretty much basic <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> Functions written more or less as usual with the one important difference being the requirement of an ActivityTrigger attribute that indicates that this function can be triggered from your Orchestrator function.</p>
<p>Your Orchestrator orchestrates the various Activity Functions by executing them as you specify. The Orchestrator function type requires an OrchestrationTrigger as we’ll soon see.</p>
<p>When you start your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/durable-functions-types-and-uses/">Durable function</a>, it creates the necessary control queues for each of the Activity Functions that it finds, and one worker item queue. Take note that although your Orchestrator looks like it is directly calling the function, under the covers it is instead actually sending a message to that work-item queue.</p>
<p>Your Activity Functions will receive a message from the queue through implementation of your ActivityTrigger and then execute any of its internal logic. When the Activity Function completes its execution, the function sends a response message to the control queue. The Orchestrator function then receives that response message via its OrchestrationTrigger, and the full behavior of the Durable Function is complete.</p>
<p>So an example might be you have a total of order items that you need to pass to another function to get taxes, or you have area parameters for something that then you need to calculate the total volume. Basically, any situation in which you have a value that you would like to pass on to another separate function to return a dependent value would be a use case for Durable Functions orchestrating your Azure Functions.</p>
<p>So let’s take a look at how this works in code by jumping back into our Visual Studio project.</p>
<h1 id="DEMO-Durable-Functions"><a href="#DEMO-Durable-Functions" class="headerlink" title="DEMO: Durable Functions"></a>DEMO: Durable Functions</h1><p>The <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/durable-functions-types-and-uses/">Durable Functions</a> equivalent of hello world, would be to create a set of activity functions that could be executed from your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/use-of-orchestrator-and-activity-functions/">orchestrator function</a>. So that’s what we’re going to do in this exercise, we’ll even create our own as <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Function</a> outside of the template, and we’ll have a value returned from that function that we can then pass on. In this case, we’ll be passing it back to the same function to show a change in value, but we could take that value and pass it along anywhere, thus maintaining state which is usually very challenging in the serverless world of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/use-of-orchestrator-and-activity-functions/https://cloudacademy.com/library/azure/">Azure</a> Functions.</p>
<p>To add a Durable Function to your Azure Functions project, just right click on the project name and do Add New Azure Function. We’ll actually go ahead and name this one FunctionDurable since we know that’s what we’re gonna do with it, and then the list of Azure Function Types pops up, and you will see that Durable Functions Orchestration is in the list. So we just click Okay, and Visual Studio will do its thing and create for us our basic Durable Functions template, which is going to include three different types of functions in it. It will include first our orchestrator function that will be responsible for calling all of the other activity functions. And then there will also be an HTTP trigger function, that is going to be how we are going to activate our orchestrator function in the first place, meaning that yes, at last, we will finally be executing one of our Azure Functions using the much mentioned HTTP trigger as if our function was an API endpoint.</p>
<p>So first, just to make sure that we can see what’s going on here, we’re expecting some text to be written to the screen when we run our orchestrator calling our activity functions. So let’s just run the Azure Function and make sure that everything is looking good inside of our Durable Function. As usual, Visual Studio pops up your console so that you can see all the logging. We’re going to need some important bits of this, you can see right there durable task has been created. And you can see in the green there just flashing by us was a critical bit of information that we need. This is our actual HTTP endpoint that we would need to use from a browser to call our Azure Function.</p>
<p>So we’ll go ahead and do a quick copy on that, we’ll pop open a new window, we’ll paste that in, and we’ll run our Durable Function. You can see we got something back from running it so everything’s looking good. And when we come over to our console, we can see it’s full of a lot of stuff, including here’s a hello to London, here’s a hello to Seattle. And so we can see that each of the activity functions within our Durable Function has run properly.</p>
<p>So now let’s take advantage of some of the power of that so that we can actually pass some values from one Durable Function to another and see that important sequence happening. So let’s just create a little bit of space here. And then I have a pre-created function that I can drop in that does nothing but add five to the value that we pass in. So here’s x equals five, here’s the logging information telling us that we’re gonna be adding that value to whatever the value that is passed in. And then we’re gonna be going ahead and returning that value.</p>
<p>So how would we get access to some of that stuff up here inside of our orchestrator function? Let’s go ahead and take out a few thing. And this is the simple process of creating an integer, having the value of that integer then reassigned to the return value from our add five function, and then doing the same thing again. So between Seattle and London, we should have one output that says we’re adding five to zero, and then we should have another output that says we’re adding five to five. And we can watch that value change dynamically as our Durable Function runs.</p>
<p>So let’s go ahead and run that function and see what we get. Azure Functions, once again, displays our log, verifying we have a durable task running. And there’s the endpoint that we’re going to need to use, which of course, fortunately, turns out to be the same endpoint that we just used.</p>
<p>So now that we know that everything is up and running, we can actually just come back to our open browser, make another request to the same endpoint. When it’s completely finished, we can go in, and we can take a look. And we can see that our function durable hello has been running properly, and so has our add function. here we can see down in the list, we’re adding five plus five. Here further above, we’re adding five plus zero.</p>
<p>So, as we are going through our Azure Function, and here’s the beginning saying hello to Seattle, adding five plus zero, adding five plus five, and then at the end here, saying hello to London. And so, in this process, we have really been able to see very powerfully that Azure Functions can tie together in a number of ways, orchestrating your Azure Functions to create a sequence like this is one of the most powerful ways that you can manage your state and the logic within your larger application picture.</p>
<p>So, now we have successfully created an Azure Durable Function that has called individual activity functions. We have activated that function using an HTTP trigger endpoint. And that concludes the first part of our introduction to Durable Functions.</p>
<h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><p>That concludes our introductory look at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Functions</a>. We’ve seen that Azure Functions are actually just little bits of your application logic that you can have reside in the cloud as a part of your general software architecture, and they can perform on demand pretty much anything that you want them to, using a variety of triggers such as timers or data events or HTTP triggers, as if they were regular API endpoints.</p>
<p>You have the ability to string together different bindings of a wide range of data sources and resources such as queue, BLOB, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> tables, etc, etc, that you can weave all of these things together into a sequence of bindings and triggers that allow you to really dictate a logical flow of data or processing through one piece of your application logic. That’s just one way to orchestrate Azure Functions together.</p>
<p>The formal Azure Functions way of orchestrating things together is by using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/demo-durable-functions/">Durable Functions</a>. We’ve seen that Durable Functions can, through the use of orchestration, call other Azure Functions that can pass data to and from each other using the orchestration function as that sort of nexus that lets you be in complete control of logical and data flow.</p>
<p>So, hopefully, you’ve seen some things within Azure Functions that get you excited to use them. Hopefully, you can apply them to the benefit of your enterprise coding structure soon. Thank you again for taking my course. I hope you enjoyed it, I hope you got a lot out of it. Happy coding!</p>
<h2 id="6DEMO-Output-Bindings"><a href="#6DEMO-Output-Bindings" class="headerlink" title="6DEMO: Output Bindings"></a>6<strong>DEMO: Output Bindings</strong></h2><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/visualstudio/azure/vs-azure-tools-resources-managing-with-cloud-explorer?view=vs-2022">Cloud Explorer Alternatives</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Lab-Creating-a-Topic-and-Subscription-in-Azure-Service-Bus-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Lab-Creating-a-Topic-and-Subscription-in-Azure-Service-Bus-14/" class="post-title-link" itemprop="url">AZ-204-Lab-Creating-a-Topic-and-Subscription-in-Azure-Service-Bus-14</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:10:37 / Modified: 11:10:38" itemprop="dateCreated datePublished" datetime="2022-11-14T11:10:37-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Lab-Creating-a-Topic-and-Subscription-in-Azure-Service-Bus-14/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Lab-Creating-a-Topic-and-Subscription-in-Azure-Service-Bus-14/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/65/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/65/">65</a><span class="page-number current">66</span><a class="page-number" href="/page/67/">67</a><span class="space">&hellip;</span><a class="page-number" href="/page/199/">199</a><a class="extend next" rel="next" href="/page/67/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">1986</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
