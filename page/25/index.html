<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/25/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/25/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-11/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Introduction-to-Kubernetes-11</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:41" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:41-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:23:04" itemprop="dateModified" datetime="2022-11-20T22:23:04-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to the introduction to Kubernetes Course. Kubernetes is a production grade container orchestration system that helps you maximize the benefits of using containers. Kubernetes provides you with the toolbox to automate the deployment, scaling, and operation of containerized applications in production.</p>
<p>In this course, we’ll teach you all about Kubernetes, including what it is and how to use it. Before we get into it, allow me to introduce myself. I’m JT Lewey and I’ll be your trainer for this course. I’m a content researcher and developer here at Cloud Academy. And I hold both the certified Kubernetes application developer certification, as well as the certified Kubernetes administrator certification. So feel free to reach out to me about either of those topics or other general DevOps questions you have.</p>
<p>Let’s talk about who should attend this course. This course is suitable for those who are looking to deploy containerized applications. This is especially useful if you already have an existing system and are evaluating deployment options. Container orchestration skills are relevant for the following positions, DevOps engineers, cloud engineers, site reliability engineers, and as well as just anybody who is a container enthusiast and looking to beef up their container orchestration skills.</p>
<p>In this course, we’re gonna be covering three main topics. And our first is gonna be an overview of Kubernetes, specifically addressing, what is it? Why is it so successful? And how can you start? From there, we’re gonna be deploying containerized applications into Kubernetes. And this involves a hands on approach. Specifically with microservices. I’ve created a <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/intro-to-k8s">GitHub repository</a> that has all the available files to you should you wish to follow along. I also suggest that you spin up our intro to Kubernetes lab where it has all the available GitHub repo files as well as a full Kubernetes environment.</p>
<p>The last topic we’re gonna be discussing, Personal Topics, I recommend you be introduced to should you wish to know more about Kubernetes. Let’s identify our concrete learning objectives. You will know these after you complete this course. Specifically, you’re gonna be able to describe Kubernetes and what it is used for. You’re going to be able to deploy single and multi container applications onto Kubernetes. You will be able to use Kubernetes services to structure any number of applications and you’ll be able to manage those applications through deployments and rollouts. You’ll also be able to ensure container pre-conditions are met and that these containers are kept healthy. You’ll be able to manage configuration maps, secrets, and how to control persistent data within Kubernetes. And lastly, you’re gonna be able to discuss the popular tools and how they can benefit you in your Kubernetes journey.</p>
<p>There are some prerequisites to this course, and to get the most out of it, you should have a solid understanding of Docker. But don’t worry. We have courses available if you’re interested in learning Docker and would like to take those before this. Next, you should have a solid understanding of YAML. But don’t worry, as it’s fairly easy to pick it up as we go. And lastly, we’re gonna be establishing a Kubernetes cluster. So any Kubernetes experience will obviously help you in learning more about the basics and more about how to configure Kubernetes.</p>
<p>My name’s Jonathan Lewey from Cloud Academy. And I’m so excited for you to take this introduction course. If you have any questions or concerns, please feel free to reach out to me on LinkedIn or <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. So enough’s enough. Let’s get into it.</p>
<h1 id="Kubernetes-Overview"><a href="#Kubernetes-Overview" class="headerlink" title="Kubernetes Overview"></a>Kubernetes Overview</h1><p>This lesson will provide a high level overview of Kubernetes. We will cover what you can do with Kubernetes including some of the core features that have driven Kubernetes success. We will also discuss the competitive landscape around Kubernetes. Kubernetes, often abbreviate as K8s, is an open-source container-orchestration tool designed to automate, deploying, scaling, and the operation of containerized applications.</p>
<p>Kubernetes was born out of Google’s experience running workloads in production on their internal Borg cluster manager for well over a decade, it is designed to grow from tens, thousands, or even millions of containers. Organizations adopting Kubernetes increased their velocity by having the ability to release faster and recover faster with Kubernetes self healing mechanisms. Kubernetes is a distributed system. Multiple machines are configured to form a cluster. Machines may be a mix of physical and virtual and they may exist on-prem or in cloud infrastructure each with their own unique hardware configurations.</p>
<p>Kubernetes places containers on machines using scheduling algorithms that consider available compute resources, requested resources priority, and a variety of other customizable constraints. Kubernetes is also smart enough to move containers to different machines as this machines are added or removed. Kubernetes is also container runtime agnostic which means you can actually use Kubernetes with different container runtimes.</p>
<p>Kubernetes most commonly uses Docker containers but can also be used with Rocket containers, for example. This kind of adaptability is a result of Kubernetes modular design. It also has a lead to Kubernetes widespread adoption and made Kubernetes one of the most active open source projects around Kubernetes also provides excellent end user abstractions by using declarative configuration for everything. Engineers can quickly deploy containers, wire up networking, scale and expose the applications to the real world. We’ll cover all of these features throughout the lesson.</p>
<p>Operation staff are not left in the dark either. Kubernetes can automatically move containers from failed machines to running machines. There are also built-in features for doing maintenance on a particular machine. Multiple clusters can also join up with each other to form a Federation. This feature is primarily for redundancy, such that, if one cluster dies, containers will automatically move to another cluster.</p>
<p>The following features also contribute to making Kubernetes a top choice for orchestrating containerized applications: the automation of deployment rollout and rollback, seamless horizontal scaling, secret management, service discovery and load balancing, support for both Linux and Windows containers, simple log collection, stateful application support, persistent volume management, CPU and memory quotas batch job processing, and role-based access control. With the popularity of containers, there’s been a surge in tools to support enterprises adopting containers in production. Kubernetes is just one example.</p>
<p>So let’s compare Kubernetes with some other tools because now that we know what Kubernetes can do it’s sometimes useful when we can compare one technology to another. We’ll compare DCOS, Amazon ECS, and Docker Swarm Mode, each has their own niche and unique strength. This section will help you understand Kubernetes approach and decide if it fits your particular use cases.</p>
<p>DCOS or Distributed Cloud Operating System is similar to Kubernetes in many ways DCOS pools compute resources into a uniform task pool, but the big difference here is that DCOS targets many different types of workloads including, but not limited to, containerized applications. This makes DCOS attractive to organizations which are not using containers for all of their applications. DCOS also includes a Package Manager to easily deploy it to his systems like, Kafka or Spark. You can even run Kubernetes on DCOS given its flexibility for different types of workloads.</p>
<p>Amazon ECS, or the Elastic Container Service is AWS’ ability to orchestrate containers. ECS allows you to create pools of compute resources and uses API calls to orchestrate containers across them. Compute resources are EC2 instances that you can manage yourself or let AWS manage them with AWS Fargate. It’s only available inside of AWS and generally, less feature compared to other open source tools. So it may be useful for those of you who are deep into the AWS ecosystem.</p>
<p>Lastly, Docker Swarm Mode is the official Docker solution for orchestrating containers across a cluster of machines. Docker Swarm Mode builds a cluster from multiple Docker hosts and distributes containers across them. It shows a similar feature set with Kubernetes or DCOS. Docker Swarm Mode works natively with the docker command. This means that associated tools like Docker Compose can target Swarm Mode clusters without any changes.</p>
<p>Docker Enterprise Edition leverages Swarm Mode to manage an enterprise-grade cluster. And Docker also provides full support for Kubernetes if you want to start out with Swarm and later swap over to Kubernetes. So if you’re not already fixed on you using Kubernetes I would recommend that you conduct your own research to understand each tool and its trade-offs. Cloud Academy has content for each option to help you make the right decision.</p>
<p>In the next lesson, we’ll go through some of our options for deploying to Kubernetes. So I’ll see you there.</p>
<h1 id="Deploying-Kubernetes"><a href="#Deploying-Kubernetes" class="headerlink" title="Deploying Kubernetes"></a>Deploying Kubernetes</h1><p>Once you’ve decided on Kubernetes, you have a variety of methods for deploying Kubernetes. This course focuses on the core concepts. But because it is only natural to ask how to get started using Kubernetes, this short lesson discusses some of your options for deploying Kubernetes.</p>
<p>Deploying Kubernetes single-node cluster. For development and test scenarios, you can run Kubernetes on a single-machine. Docker for Mac and Docker for Windows, both include support for running Kubernetes on the local machine in a single-node configuration. Just make sure Kubernetes is enabled in the settings. This is the easiest way to get started if you already have Docker installed.</p>
<p>Another option is to use minikube which supports Linux in addition to Macs and Windows. Lastly, Linux systems can use kubeadm to set up a single-node cluster. Kubeadm is used as a building block for building Kubernetes clusters, but it can effectively create single-node clusters. But be aware that kubeadm will install Kubernetes on the system itself rather than a virtual machine, like the prior methods.</p>
<p>Single-node clusters are also useful within continuous integration pipelines. In this use case, you want to create ephemeral clusters that start quickly and are in a pristine state for testing applications in Kubernetes each time you check a new code. Kubernetes in Docker, abbreviated K-in-D or kind is made specifically for this use case.</p>
<p>Deploying Kubernetes multi-node cluster. For your production workloads, you want clusters with multiple nodes to take advantage of horizontal scaling and to tolerate node failures. To decide what solution works best for you, you need to ask several key questions including, “How much control do you want over the cluster versus the amount of effort you are willing to invest in maintaining it?”</p>
<p>Fully-managed solutions free you from routine maintenance but often lag the latest Kubernetes releases by a couple of version numbers for consistency. New versions of Kubernetes are released every three months. Examples of fully-managed Kubernetes as a service solutions include Amazon Elastic Kubernetes Service or EKS, Azure Kubernetes Service or AKS, and Google Kubernetes Engine or GKE.</p>
<p>To have full control over your cluster, you should check out kubespray, kops, and kubeadm. The next question is, “Do you already have investment into and expertise with a particular cloud provider?” Cloud provider’s managed Kubernetes services integrate tightly with other services in their cloud. For example, how identity and access management is performed. There will be a lot less friction to staying close to what you already know.</p>
<p>After that, we have, “Do you need enterprise support?” Several vendors offer enterprise support and additional features on top of Kubernetes. These can include OpenShift by RedHat, Pivotal Container Service, or Rancher.</p>
<p>Another question to consider is, “Are you concerned about vendor lock-in?” If you are, you should focus on open source solutions, like kubespray and Rancher that can deploy Kubernetes clusters to a wide variety of platforms.</p>
<p>Some other questions that are not important are, “Do you want the cluster on-prem, in the cloud, or both?” Because Kubernetes provides users with an abstraction of cluster of resources to the underlining nodes that can be running in different platforms. Kubernetes itself is at the core of open source hybrid clouds. Even cloud vendor Kubernetes solutions allow using on-prem compute. For example, GKE on-prem lets you run GKE on-premise, EKS allows you to add an on-premise nodes to the cluster, and Azure Stack allows you to run AKS on-prem.</p>
<p>Another question to consider is, “Do you want to run Linux containers, Windows containers, or a mix? To support Linux containers, you need to ensure you have Linux nodes in your cluster. To support Windows containers, you need to ensure that you have Windows nodes in your cluster. Both Linux and Windows nodes can exist in the same cluster to support both types of containers.</p>
<p>All that being said, in the context of this course, Cloud Academy has you covered for the following along with a course using a real multi-node cluster. The introduction to Kubernetes playground lab provides the same cluster that will be used during this course. So if you want to follow along without setting up your own cluster, go ahead and start that lab now and feel free to use any other cluster if you’d like to.</p>
<p>In the next lesson, we’re going to be covering the Basics of Kubernetes Architecture. Continue on when you are ready.</p>
<h1 id="Kubernetes-Architecture"><a href="#Kubernetes-Architecture" class="headerlink" title="Kubernetes Architecture"></a>Kubernetes Architecture</h1><p>This lesson will cover Kubernetes Architecture. What we cover here will be enough to understand and reason about topics we’ll learn later in this course. It is intended to build a strong foundation rather than to be an exhaustive review. Kubernetes itself is a distributed system. It introduces its own dialect to the orchestration space. Internalizing the vernacular is an important part of success with Kubernetes. And we will define several terms as they arise but know that there is also a Kubernetes glossary available in the introduction to Kubernetes learning path such that you have a single point of reference for the terms you need to know and more comprehensive glossary maintained by Kubernetes is also linked from there.</p>
<p>You must also understand the architecture to have a basic understanding of how features work under the hood. The Kubernetes cluster is the highest level of abstraction to start with. Kubernetes clusters are composed of nodes and the term cluster refers to all of the machines collectively and can be thought of as the entire running system.</p>
<p>The machines in the cluster are referred to as nodes. A node may be a VM or a physical machine. Nodes are categorized as worker nodes or master nodes. Each worker node include software to run containers managed by Kubernetes control plane and the control plane runs on master nodes. The control plane is a set of APIs and software that Kubernetes users interact with. These APIs and software are collectively referred to as master components.</p>
<p>The control plane schedules containers onto nodes. So the term scheduling does not actually refer to time in this context. Think of it from a Kernel perspective the Kernel schedules processes onto CPU’s according to multiple factors. Certain processes need more or less compute or may have different quality of service rules. Ultimately the scheduler does its best to ensure that every container runs. Scheduling in this case refers to the decision process of placing containers onto nodes in accordance with their declared compute requirements.</p>
<p>In Kubernetes containers are grouped into Pods. Pods may include one or more containers. All containers in a Pod run on the same node. And the Pod is actually the smallest building block in Kubernetes. More complex and useful abstractions sit on top of Pods. Services, define networking rules for exposing Pods to other Pods or exposing Pods to the internet. And Kubernetes also uses deployment to manage the deployment configuration and changes to running Pods as well as horizontal scaling. These are fundamental terms you need to understand before we can move forward.</p>
<p>We’ll elaborate on these terms and introduce more terms as we progress throughout the course but I cannot overstate the importance of them. I suggest you replay this section as many times as you need until all of this information sinks in. Lets recap about what we’ve learned so far. Kubernetes is an orchestration tool. A group of nodes form a Kubernetes cluster. Kubernetes runs containers in groups called Pods. Kubernetes services expose Pods to the cluster as well as to the public internet. And Kubernetes deployments control rollout and rollback of Pods.</p>
<p>In the next lesson, we’re going to be seeing how to interact with Kubernetes clusters.</p>
<h1 id="Interacting-with-Kubernetes"><a href="#Interacting-with-Kubernetes" class="headerlink" title="Interacting with Kubernetes"></a>Interacting with Kubernetes</h1><p>As we have seen, the master components provide the Kubernetes control plane. The way that you retrieve and modify state information in the cluster, is by sending a request to the Kubernetes API server, which is the master component that acts as a front end for the control plane. This leads us to the first method of interacting with Kubernetes, directly communicating via rest API calls. It is possible but not common to need, to work directly with the API server. You might need to if you’re using a programming language that does not have a Kubernetes client library.</p>
<p>Client libraries are our second method of interacting with Kubernetes. Client libraries can handle the tedium of authenticating and managing individual REST API requests and responses. Kubernetes maintains official client libraries for Go, Python, Java, .NET, and JavaScript. There are also many community-maintained libraries if there isn’t official support for your language of choice. The client libraries are a great choice for the OPAs writing code to interact with Kubernetes.</p>
<p>The next method of interacting with Kubernetes is the most common, and what we will focus on in this course, it is the Kubernetes command line tool called cube control, or Kubectl. With cube control, you can issue commands that are at a high level of abstraction with each command, translating into the appropriate API server request. With cube control, you can also access clusters locally, as well as remote. Your success with Kubernetes directly correlates with your Kubectl skill. You can accomplish all your day-to-day work using Kubectl.</p>
<p>So it is vital to learn this command because it manages all different types of Kubernetes resources, and provides debugging and introspection features. Luckily, Kubectl follows an easy to understand design pattern. When you learn to manage one resource, you learn to manage them all. Let’s introduce some common sub commands to see what they look like and what Kubectl can do.</p>
<p>Starting with Kubectl create, Kubectl create creates a new Kubernetes resource. You can create several resources using the built-in sub commands of create, or you can use resources specified in a file. The files are most commonly in gamble format and are referred to, as manifests.</p>
<p>Kubectl delete, Kubectl does the opposite of create, in that it deletes a particular resource. You can do the same with a file, with resources declared inside of it. Kubectl get, returns a list of all the resources for a specified type. For example, Kubectl get, pods lists all the pods and the current namespace. Kubectl describe is going to print detailed information about a particular resource or a list of resources. As an example, Kubectl describe pod, server gives detailed information about the pod named server. Kubectl logs, print container logs for a particular pod or a specific container inside of a multi container pod.</p>
<p>We’ll go deeper into these commands and more as the course progresses. This is just enough to kickstart you for our next lesson. And our final method of interacting with Kubernetes, is through the web dashboard. The dashboard provides a nice list of dashboards, as well as, easy to navigate views of cluster resources. The web dashboard is covered in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/deploy-a-stateful-application-in-a-kubernetes-cluster/">Deploy Stateful Application in a Kubernetes Cluster</a> lab here on cloud Academy. But if you want, you can check it out after you complete this course.</p>
<p>The web dashboard is optional, so not all clusters will have it. Kubectl truly is the way to go for maximum productivity and it really doesn’t take long to get the hang of it. We are now ready to start getting our hands dirty with Kubernetes. We can’t cover everything in this introductory course, but I want to cover many of the main parts of Kubernetes. In the next lesson we’re going to be deploying our first application to Kubernetes. So see you then.</p>
<h1 id="Pods"><a href="#Pods" class="headerlink" title="Pods"></a>Pods</h1><p>This lesson will introduce you to working with Kubernetes cluster and we’re going to be specifically focusing on pods. By doing so, you will see first-hand patterns used by kubectl and some examples of manifest files. But first, let’s review the theory. Pods are the basic building block in Kubernetes. Pods contain one or more containers and we’re going to be sticking with one container per pod in this lesson but we’ll be talking about multi-container pods later. </p>
<p>All pods share a container network that allows any pod to communicate with any other pod, regardless of the nodes that the pods are running on. Each pod gets a single IP address in the container network so Kubernetes will do all the heavy lifting and make that happen. You get to work with the simple abstraction. All pods can communicate with each other and that each pod has one IP address. </p>
<p>Because pods include containers, the declaration of a pod includes all the properties that you would expect for example with Docker rhyme. These include the container image, any ports you want to publish to allow access to the container, choosing a recent policy to determine if a pod should automatically restart, when its container fails, and limits on the CPU and memory resources but there are also a variety of other properties that are specific to pods in Kubernetes. We’re going to be seeing many examples of those in the coming lessons. </p>
<p>All of the desired properties are written in a manifest file. Manifest files are used to describe all kinds of resources in Kubernetes, not only pods. Based on the kind of resource that the manifest file describes, you will configure different properties of that file. The configuration specific to each kind of resource is referred to as its specification or spec. The manifests are sent to the Kubernetes API server where the necessary actions are taken to realize what is described in the manifest. You will use kubectl to send a manifest to the API server and one way of doing this is with the kubectl create command. </p>
<p>For pod manifests, the cluster will take the following actions. Selecting a node with available resources for all of the pods’ containers, scheduling the pod to that node. The node will then download the pod’s container images and then subsequently run the containers. There are more steps involved but that is more than enough to get the idea. We mentioned before that kubectl also provides sub-commands to directly create resources without manifests. </p>
<p>It’s usually a good idea to stick with manifests for several reasons. You can check in your manifests into a source control system to check their history and rollback when needed. It makes it easy to share your work such that it can be created in other clusters and it’s also easier to work with compared to stringing together sequences of commands with many options to achieve the same result. So we’re going to be sticking with manifests for this course. </p>
<p>Now that we’re ready to see all this in action using kubectl and a Kubernetes cluster, our goal will be to deploy an Nginx web server using a Kubernetes pod. If you are using the Introduction to Kubernetes Playground, follow the instructions to the EC2 instance to connect to the Bastion or feel free to connect using a local terminal like I am. If you use a different solution for a Kubernetes cluster, simply follow their provided instructions to make sure kubectl can talk to the cluster. </p>
<p>So I’m here at my terminal, connected to the Bastion host which has kubectl configured to talk to the lab cluster. To confirm that kubectl has configured to talk to the cluster, we first can enter our first few kubectl commands. Kubectl get pods. The output tells us that no pod resources were found in the default name space of the cluster. If it wasn’t able to connect to the API server, you would have seen an error message instead so everything looks good. </p>
<p>Let’s start with a minimal example of a pod manifest to get a taste for manifests. We’ll gradually build them up as we go. I’ve prepared the 1.1 basic pod .yaml file for this. All the course files are preloaded into the source directory on the lab instance and also available on the course get hub repo. This manifest declares a pod with one container that uses the Nginx latest image. All manifests have the same top level keys, API version, kind, and metadata followed by the spec. </p>
<p>Kubernetes supports multiple API versions and version one is the core API version containing many of the most common resources such as pods and nodes. Kind indicates what the resource is. Metadata then includes information relevant to the resource that can help identify resources. The minimum amount of metadata is a name which is set to my pod. </p>
<p>Names must be unique within a Kubernetes name space and spec is specification with a clear kind and must match what is expected by the defined API version. For example, the spec can change between the beta and the generally available API version of a resource. The spec is essentially where all of the meat goes. You can refer to the official API docs for complete info on all versions and supported fields. I’ll explain the ones that we need for this course but know that there are far more left to discover. </p>
<p>The pod spec defines the containers in the pod. The minimum required field is a single container which must declare its image and name. This pod only has a single container but the yaml is a list allowing you to specify more than one. Back at the command line, we can create the pod by changing into the source directory with CD source. We then issue kubectl create -f 1.1-basic_pod.yaml. The f option tells us to create, that the create command, is going to be creating a manifest from a file. For any kubectl command, you can always depend –help to display the help page to get more information. </p>
<p>Now if we run kubectl get pods, we can see my pod is running. My pod is technically an object of a pod kind of resource but it is common to simply use resource to also describe the object as well as the kind. Kubectl shows the name, the number of running containers, the pod state, restarts, and the age of the pod in the cluster. You should memorize the get commands since you’ll use it all the time and I really mean all of the time. </p>
<p>Let’s see some more detailed information about this particular pod. Using the describe command to get complete information. Kubectl describe pod and we’re going to pipe it to more. Describe takes a resource kind just like get and to narrow in on specific resources of that kind, we add the name which you can also do with get. We’re going to be piping the output to more so we can press space bar to page throughout this output. </p>
<p>As you can see, there’s a lot more information than what get provides. The name, name space, and the node running the pod are given at the top along with other metadata. Also note that a pod is assigned an IP. No matter how many containers we include, there would be only one IP. In the containers section, we can see that the image and whether or not the container is ready. You can also the port and the container port are both set to none. </p>
<p>Ports are part of the container spec but Kubernetes assigns default values for us. Just like Docker, you need to tell Kubernetes which port to publish if you want it to be accessible. We’ll have to go back and declare our port after this. Otherwise, nothing is going to reach the web server and at the bottom, we have our events section. It lists the most recent events related to the resource. You can see that the steps Kubernetes took to start the pod from scheduling on the container image to starting the container. The events section is shared by most kinds of resources when you use describe is very helpful for debugging. </p>
<p>Let’s tell Kubernetes which port to publish to allow access to the web server. I’ve prepared the 1.2 port file specifically for that. Compared to the 1.1 file, we can see the ports mapping is added and the container port field is set to 80 for HTTP. Kubernetes is also going to be using TCP as the protocol by default and we’ll assign it an available host port automatically so that we don’t need to declare anything more. </p>
<p>Kubernetes can apply certain changes to different kinds of resources on the fly. Unfortunately, Kubernetes cannot update ports on a running pod so we need to delete the pod and recreate it. We’re going to be running our kubectl delete pod my pod to delete this pod. You can also specify with the -f with referencing to the 1.1 file and Kubernetes will delete all of the resources declared in that file. </p>
<p>Now, we can issue the command kubectl create -f 1.2.yaml. And describe the pod again. You don’t need to describe the pod every single time. I just prefer to do this to see the result of my work and make sure that everything went as I expected. Now we can see that port 80 is given as the port so you may think to try to send a request to port 80 on that noted IP but it still won’t work. Why do you think that is? Well the pod’s IP is on the container network and this lab instance is not part of the container network so it won’t work. But if we sent the request from a container in a Kubernetes pod, the request would succeed since pods can communicate with all other pods by default. We’ll see how we can access the web server from the lab instance in the next lesson. </p>
<p>Before we move on, I want to cover a couple more points and the first is shared between all resources and the second is specific to pods. In the describe section, you might have seen the labels field was set to none. Labels are key value pairs that identify resource attributes. For example, the application tier, whether it’s front end or back end or maybe a region such as US East or US West. </p>
<p>In addition to providing meaningful and identifying information, labels are used to make selections in Kubernetes. For example, you could tell kubectl to get only resources in the US West region. So our 1.3 manifest has a label added to identify the type of app that the pod is a part of. We’re using an Nginx web server and the label value is web server. You could have multiple labels but one is enough in this example. </p>
<p>Our last point that I want to make pertinent is that Kubernetes can schedule pods based on their resource requests. The pods that we’ve seen so far don’t have any resource requests set which makes it easier to schedule them because the scheduler doesn’t need to find nodes that have these requests for amounts of resources. It’ll just throw them onto any node that isn’t under pressure or starved of resources. However, these pods will be the first to be evicted if a node becomes under pressure, it needs to free up resources. That’s called best effort quality of service which was displayed in the describe output. Best effort pods can also create resource contention with other pods on the same node and usually it’s a good idea to set resource requests. </p>
<p>In the 1.4 yaml, I’ve set a resource request and limit for the pod’s container. Request sets the minimum required resources to schedule the pod onto a node and the limit is the maximum amount of resources you want the node to ever give the pod. You can set resource requests and limits for each container. There’s also support for requesting amounts with local disk by using the ephemeral storage. </p>
<p>When we create this pod, kubectl delete pod by pod and then subsequently kubectl -f 1.4, the pod will be guaranteed the resources you requested or it won’t be scheduled until those resources are available. Kubectl describe my pod will now list our pod and we’ll see this guaranteed quality of service. You need to do some benchmarking to configure a reasonable request and limit but the effort is well worth it to ensure your pods have the resources they need and best utilization of the resources in the cluster. This is one of the reasons why we are using containers in the first place. </p>
<p>For the rest of this course, we will use best effort pods since we won’t have any specific resource requirements in mind. This isn’t something you should do in protection environments, however. We’ve covered a lot in this lesson so let’s review what we covered. Pods are the basic building block in Kubernetes and contain one or more containers. You declare pods and other resources in manifest files. All manifests share an API version, kind, and metadata related to that resource. Metadata must include a name but labels are usually a good idea to also help you further filter down your resources. </p>
<p>Manifests also include a spec to configure the unique parts of each resource kind. Pod specs include the list of containers, which must specify our container name and image, but is often useful to set the resource requests and limits. We’re going to see more fields of pod specs in later lessons. </p>
<p>In our next lesson, we’re going to be making the web server running in the pods accessible from our lab VN with services. I’ll see you there.</p>
<h1 id="Services"><a href="#Services" class="headerlink" title="Services"></a>Services</h1><p>So in our previous lesson, we created a webserver Pod but at the moment, it’s inaccessible apart from other Pods in the container network, which isn’t really useful. Even for pausing the container network, it isn’t very convenient to access the webserver as it is because Pods if you’d be able to find the IP address of the webserver Pod and keep track of any changes to it. So remember that Kubernetes will reschedule Pods on to other nodes, for example if the node fails.</p>
<p>But what happens if you have a Pod that fails? Once the Pod is rescheduled it will be assigned an IP address from the available pool of addresses and not necessarily the same IP address it had before. So to overcome all of these networking issues, Kubernetes employs services. thinking back to the Kubernetes’ definition for a service, a service defines networking rules for accessing Pods in the cluster and from the internet you can declare a service to access a group of Pods using labels. And in our example, we can use our app label just like the webserver Pod for the services target. Clients could then access the service at a fixed address. And the services networking rules will direct client request to a Pod in the selected group of Pods.</p>
<p>In our example, there is only one Pod but in general there can be many. The service will also distribute these requests that come into it across the Pods to balance the load. Let’s visualize how we’ll use the service to solve our problem of accessing the webserver running in the Pod.</p>
<p>First, we’re gonna create a service that selects Pods with the app&#x3D;webserver label. That will cause the service to act as a kind of internal load balancer across those Pods. The service will also be given a static IP address and Port by Kubernetes that will allow us to access the service from outside of the container network, and even outside of the cluster.</p>
<p>Let’s see how we do it. Our first three fields are set to the same as before. The kind is now Service, metadata uses the same label as the Pod since it is related to the same application. This isn’t required but it is a good practice to stay organized. Now for the spec, the selector is our important field. The selector defines the labels to match the Pods against. At this example of targets Pods with the app&#x3D;webserver, which will select the Pod that we’ve already created. Services must also define port mappings. So, this service targets Port 80. This is the value of the Pods’ container port.</p>
<p>Lastly, is the optional type. This value defines actually how to expose the Service and we’re gonna set it to NodePort. NodePort allocates a port over this service on each node in the cluster. By doing this, you can send a request to any node in the cluster on the designated port and be able to reach that Service. The designated port will be chosen from the set of available ports on the nodes, unless you specify a NodePort as part of the specs ports.</p>
<p>Usually it is better like Kubernetes shows the NodePort from the available ports to avoid the chance that your specified port is already taken. That would cause the service to fail to create In future lessons, we’re gonna be covering alternate values to service types, so don’t worry.</p>
<p>Now, let’s create the service with our familiar kubectl create -f 2.1. And we’re gonna list the services with kubectl get services. Notice that both commands are really familiar. Kubectl follows a simple design pattern which makes it easy to manage and explore different resources. Kubectl also displays the name, Cluster-IP, External-IP, Ports, and Age of each service.</p>
<p>Let’s bring it down, starting with Cluster-IP. This is our private IP for each service. Our External-IP is not available for NodePort services but if it were, then this would be the public IP for a service. Note that the Ports column, Kubernetes will automatically allocate a Port in the Port range allocated for NodePorts which is commonly port numbers between 30,000 and 32,767.</p>
<p>Let’s describe the service to see what other information is available with kubectl describe service webserver. Just like before, you’ll see a bunch of useful debugging information. The Port was shown in the get services output and also in this output. But we also can see the Endpoint, which is the address of each Pod in the selected group, along with a container port. If there were multiple Pods selected by the label, then you would see each of them listed here.</p>
<p>Kubernetes automatically adds and removes these endpoints as matching Pods are created and deleted, and so you don’t need to do anything to manage those endpoints. Now that we know that the NodePort is on we need a nodes IP, it can be any nodes IP. And one way to list them is to grep for this address in the described node output and add the -A option to include lines after the match.</p>
<p>So let’s do that, kubectl describe nodes and we’re to pipe it to grep -i address -A 1. Nodes are resources in the cluster, just like Pods and services. So, you can use the get and describe commands on them. You can check out all the information in the describe output on your own, or for right now, we just need those IPs. The IP addresses are the internal or private IPs of our nodes inside of our cluster.</p>
<p>Our lab VM is in the same virtual network, so it can reach the nodes using these addresses. And I’ve allowed incoming traffic on the NodePort range from the lab instance in the firewall rules to allow the request. Choose any of the addresses and use the curl command to send an http request to the IP with a NodePort upended. That is the raw html output being served up by Nginx. You can try any of the node IPs and it will give your exact same result, all thanks to a Kubernetes service.</p>
<p>In this lesson, we saw that services allow us to expose Pods using a static address, even though the addresses of the underlying Pods may be changing. We also specifically used a NodePort service to gain access to the service from outside of the cluster on a static Port that is reserved on each node in the cluster. This allowed us to access the service by sending a request to any of the nodes, just not the node that is running the Pod. There is more to say about Pods and services. We will use more complex application in the future to illustrate some of the remaining topics in the next couple of lessons. Think microservices will start by covering multi-container Pods, to continue on when you’re ready.</p>
<h1 id="Multi-Container-Pods"><a href="#Multi-Container-Pods" class="headerlink" title="Multi-Container Pods"></a>Multi-Container Pods</h1><p>This lesson continues to expand upon what we’ve already learned about Pods. Specifically, we’re going to be exploring the details of working with Multi-Container Pods. This is where we really start to hit the good stuff. As we learn more about Multi-Container Pods, we’re also gonna be learning about Namespaces and Pod Logs.</p>
<p>We’re using a sample application for this lesson. It’s a simple application that increments and prints a counter. It’s split into 4 containers across 3 tiers. The application tier includes the server container that is a simple Node.js application. It accepts a post request to increment a counter and a get request to retrieve the current value of the counter. The counter is stored in the Redis container which comprises the data tier. The support tier includes a poller and a counter.</p>
<p>The poller container continually makes a get request back to the server and prints the value. The counter continually makes a post request to the server with random values. All the containers use environment variables for configuration and these Docker images are public, so we can reuse them for this exercise.</p>
<p>Let’s walk through modeling the application as Multi-Container Pods. We’ll start by creating a Namespace for this lesson. Remember that a Namespace separates different Kubernetes resources. Namespaces may be used to isolate users, environments, or applications. You can also use Kubernetes’ role-based authentication to manage users as access to resources in a given Namespace. Using Namespaces is a best practice.</p>
<p>So, let’s start using them now and we’ll continue to use them throughout the remainder of this course. The created, just like any other Kubernetes resource. Here is our Namespace manifest. Namespaces don’t require a spec. The main part is the name which is set to microservices and is a good idea to label it as well. Everything in this Namespace will relate to the counter microservices app. So let’s create the Namespace in kubectl. With kubectl create -f 3.1.</p>
<p>Use your kubectl commands, either use a –namespace or -n option to specify the Namespace, otherwise the default Namespace will be used. You could also use the kubectl create namespace command but for this course, we’re gonna be sticking to manifest. Now out of the Pod, I’ve named the Pod app. Off the top, I want to mention you can specify namespace in this metadata for this Pod but that makes this manifest slightly less portable because the Namespace can’t be overwritten at the command line.</p>
<p>Moving down to the Redis container, we’ll use the latest official Redis image. The latest version is chosen to illustrate a specific point. When you use the latest tag in Kubernetes, and it will always pull the image whenever the Pod started. This can introduce bugs, if a Pod restarts and pulls the new latest version without you realizing it.</p>
<p>Prevent always pulling the image in using an existent version, if one exist. You can set the imagePullPolicy field to IfNotPresent. It’s useful to know this but in most situations you’re better off specifying a specific tag rather than the latest. When specific tags are used, the default imagePull behavior is, IfNotPresent. So, the standard Redis port of 6379 is published with this container.</p>
<p>Now, onto the server container. The server container is straightforward. The image is the public image from this sample application. The tag is used to indicate the microservice within the microservices repository And the server, runs on port 8080, such that it is exposed. The server also requires a REDIS_URL environment variable to connect to the data tier. We can set this in the environment variable sequence.</p>
<p>So how does the server know where to find Redis? Well, because containers in a Pod share the same network stack, a result of which that they all share the same IP address. So, they can reach other containers in the Pod on the local host at their declared container port. The correct host port in this example is localhost:6379 Our imagePullPolicy is admitted because Kubernetes uses IfNotPresent when the explicit tag is given. </p>
<p>We can use the same approach for the counter and poler containers. These containers require the API_URL environment variable to reach the server in the application tier. The correct host port combo for this example is localhost:8080. </p>
<p>Now, let’s create the Pod this time but by adding the -n option to set the Namespace for this Pod, such that it’s created in a microservices Namespace. Kubectl create -f 3.2 yaml -n microservice. Remember to include the same Namespace option with oq control commands that are relating to the Pod. Otherwise you will be targeting the default Namespace. </p>
<p>If we wanted to get the Pod, we would issue kubectl get -n microservices pod app. The -n namespace option can be included anywhere after kubectl. It doesn’t have to be after get, it could be before or after. When you have tab completion enabled. It makes sense to put it earlier, to get to completions for your target namespace. </p>
<p>Let’s observe the output, and we’ll see a &#x2F;4 under the status, since we have 4 containers in the Pod. The status also summarize what is going on but it is best to describe the Pod to see what is going on in more detail. Kubectl describe -n microservice pod app. You’ll see the event log has more going on now that there are multiple containers. </p>
<p>The same events are being triggered for each container from Pulling to Starting as was the case for a Single-Container Pod is something goes awry. You should check the event log to see what’s happening behind the scenes to debug any issue. In this case, everything looks good. </p>
<p>Once the containers are running, we can look at the container logs to see what they’re doing. Logs are simply anything that is written to standard out or standard error in the container. The containers need to write messages to standard out or standard error, otherwise nothing will appear in the logs. Kubernetes records the logs and they can be viewed via the logs command the kubectl log command retrieves logs for a specific container in a given Pod. It dumps all of the logs by default or you can use the tail option to limit the number of logs present. </p>
<p>Let’s see the most 10 recent logs for the counter container in the app Pod. Was kubectl logs -n microservice app counter –tail 10. Here we can see the counter is incrementing by the count by random numbers between 1 and 10. Let’s check the value of the count by inspecting the logs for the poller container. This time we’ll use the -n -f to stream the logs in real time, which is short for follow. Kubectl logs -n microservice app poller -f. We can see the count is increasing every second as the counter continues to increment it. That confirms it, our first multi-container application is up and running. Press Control + C to stop following the logs.</p>
<p>In this lesson, we created a Multi-Container Pod that implements a 3-tier application. We use the fact that containers in the same Pod can communicate with one another using local host. We also saw how to get logs from containers running in Kubernetes by using the kubectl logs command. Remember that logs worked by recording what the container writes to standard out and standard error. The logs also allowed us to confirm that the application is working as expected by continuously incrementing that count.</p>
<p>But there are some issues with the current implementation. Because Pods are our smallest union of work, Kubernetes can only scale out by increasing the number of Pods and not the containers inside of the Pod. If we want to scale out the application tier with the current design we have to also scale out all other containers proportionately. This means that there would be multiple Redis containers running, each would have their own copy of the counter. That’s certainly not what we’re gonna be going for.</p>
<p>It is a much better approach, if we were able to scale each of these services independently. Breaking the application out into multiple Pods and connecting them with services is our ideal implementation. We’ll walk through the design in next lesson but before moving on, it’s worth noting that sometimes you do want each container in a Pod to scale proportionately. It comes down to how tightly coupled the containers are, and if it makes sense to be thinking of them as a single unit.</p>
<p>With that point out of the way, I’ll see you in our next lesson where we will leverage services to break our tightly coupled Pod design into multiple independent Pods.</p>
<h1 id="Service-Discovery"><a href="#Service-Discovery" class="headerlink" title="Service Discovery"></a>Service Discovery</h1><p>We’ve seen Services in action, and in the context of allowing external access to pods running in the cluster, when we created them, with a node port. It’s time to see how services are useful within the cluster. We’ll split our example Microservices application into three pots, one for each tier. Remember that we use the fact that the containers in the same pod can communicate with each other using the local host. But that’s not going to work with our multi-pod design.</p>
<p>That’s where Services come in. Services provide a static end point to access pods in each tier. We could directly use the individual pod IP addresses on the container network, but that would cause the application to break when pods are restarted, because their IP address could change. An added benefit of Services is they also distribute load across the selected group of pods, allowing us to take advantage of the scaling application tier across multiple server pods. </p>
<p>So to realize these benefits, we need to create a data tier service in front of the Redis pod, and an application to your Service in front of the server pod. There are two Service discovery mechanisms built into Kubernetes. The first are environment variables, and the second is DNS. Kubernetes will automatically inject environment variables into containers that provide the address to access services. The environment variables follow a naming convention so that all you need to know is the name of the service to access it. Kubernetes also constructs DNS records based on the service name and containers are automatically configured to clear the clusters, DNS, to discover those services. You’ll see examples of both techniques in this lesson.</p>
<p>We’ll start with creating a new namespace to organize the resources for this lesson. It’s called Service Discovery, and we’ll do that with kube control create, dash F 4.1. Moving on to the data tier. We have a manifest that includes multiple resources. With YAML, we’re allowed to create multiple resources by separating them with three hyphens. It’s possible to cram all the pods and services into one file, but separating them by tier mimics the way we want to manage each tier independently.</p>
<p>We have a service, and now we have our Redis pod. Both are named data tier. The pod has a tier label, which is used by the service as its selector. In our example, we only have one Microservice in the data tier, but that won’t be the case in general. You can include as many labels as necessary in the selector to get just what you need. We can get by with just this one label, in this case. Services can also publish more than one port, which makes a naming the ports mandatory to identify them. We only have one, so the name is optional.</p>
<p>In YAML, everything after our pound or hashtag symbol is a comment. Comments are for readability, and don’t affect how Kubernetes interprets the manifest. Lastly, we set the type to cluster IP, which is the default so that the line could be omitted. Cluster IP creates a virtual IP inside the cluster for internal access only. So we can now use kube control, create, dash F 4.2 YAML and append the namespace with dash N service-discovery.</p>
<p>To create the resources, the command is the same, regardless of how many resources are specified in the file. The resources in the file are created in the order they are listed in the file. Let’s check that the pod is running with kube control, get pod dash N service-discovery. Then describe the service with kube control describe service dash N service-discovery data tier. To make sure that our service has a cluster IP, and that one endpoint corresponds to the data tier pod selected by the service.</p>
<p>Let’s move on to the app tier. Again, we have a service and a pod. The service selects the pods with a tier label, matching the server pod declaration. The pod spec is the same as the one before with one exception, the value of Redis URL environment variable is set using environment variable set by Kubernetes over to service discovery. The value used to be local host 6379, but now we need to access the data tier service. </p>
<p>There are separate environment variables made available to you. The service cluster IP address is available using the environment variable, following the pattern of a service name in all capital letters, with hyphens replaced by underscores followed by underscore service, underscore host in all caps. By knowing the service name you construct the environment variable name, to discover that service IP address. </p>
<p>In our example, with the environment variable and its data tier service host, the port environment variable is similar with host replaced by port. In our example, that is data tier service port, if the port includes a name, you can also append and underscore port name in all caps, hyphens replaced by underscores, which is data tier service port Redis, in our example. The data tier service only declares one port, so the appended name is optional. </p>
<p>As a best practice you can append the service name to tolerate adding ports to the service in the future. When using environment variables in the value field, you need to enclose the variable name in parentheses and precede it with a dollar sign. This allows composing container environment variables from the Kubernetes provided values. When using environment variables for service discovery, the service must be created before the pod in order to use environment variables for service discovery. That is, Kubernetes does not update the variables of running containers. They only get set at startup. </p>
<p>The service must also be in the same namespace for the environment variables to be available. So let’s create our application tier with kube control, create dash F 4.3 YAML, dash N service discovery. Now onto the support tier. We don’t need a service for this tier, just a pod will do, and it contains the counter and polar containers used before. This time we’re gonna be using DNS for service discovery of the app tier service. </p>
<p>Kubernetes will add a DNS A records for every service. The service DNS names follow the pattern of a service name, dot service namespace. In our example that is, app dash tier.service dash discovery. However, if the service is in the same namespace, then you can simply only use the service name. The polar omits the namespace in this manifest.</p>
<p>No need to convert hyphens to underscores, or use all caps when using DNS service discovery. The cluster DNS resolves the DNS name to the service IP address. You can get service port information using DNS SRV records, but that isn’t something that we can use in the manifest file. So I’ll have to either hard-code the port information or use the service port environment variable. The counter uses a hard-coded port, and the polar uses the port environment variable for illustration. </p>
<p>It is possible to use the DNS SRV port record to configure the pod on start-up using something called iNET containers, but we’re going to be covering that later on in this course. So let’s create the support tier. Starting with Kube control create dash F 4.4 YAML, and then the namespace service discovery. </p>
<p>Now let’s check all the pods, with kube control, get pods, namespace service discovery. There are three running pods creating four containers in total. So let’s check the polar logs to see what’s going on with our account. With kube control logs dash N service discovery support tier polar, and let’s follow them. Look at that. The application is just plugging away, and that is such a satisfying result.</p>
<p>Let’s recap this lesson before jumping into the next one. We’ve covered structuring a varying number of applications using Services as interfaces between tiers. We use the cluster IP type of service. We’re accessing the data and application tiers within the cluster. We also covered how Kubernetes Services works with environment variables and DNS. That allowed us to refactor our multi container pod application into instead a multi-tier application that we stood up in this lesson.</p>
<p>When using environment variables for service discovery, the service must be created for the pod, before the pod, in order to use the environment variables for that service discovery. Their service must also be in the exact same namespace. DNS records overcome the shortcomings of environment variables. DNS records are added and removed from the clusters DNS as services are created and destroyed. The DNS name for services include a namespace, allowing communication with services and other namespaces. And finally SRV DNS records are created for service port information.</p>
<p>So what do you think? Are you getting excited about the capabilities that Kubernetes can do? It just keeps getting better and better. And it’s gonna get better in the next lesson. To put it in context, consider how we would scale our current application. We could increase the number of server pods by changing the name to something like example app tier dash one, then creating example app tier dash two, and so on. And we could glue this all together with some scripting, a bit of extra work to make the scaling easy. But what then happens when we would want to reconfigure the server container? Well, let’s see.</p>
<p>We could create an example app tier version one dash one and then example app tier version two dash one, with some updated scripting. These things could probably handle that, but what happens when something goes wrong? Or what if there’s an error in the new version? We could probably handle that by pulling the API and checking the status again, with probably some more scripting, include some more code, but there should probably be a better way to do this, which is exactly what the next lesson is going to be covering. And that is deployment. So let’s learn about it in the next lesson.</p>
<h1 id="Deployments"><a href="#Deployments" class="headerlink" title="Deployments"></a>Deployments</h1><p>The previous lessons have created pods directly, but I’ve gotta be honest with you, we’ve kind of been cheating a bit so far. You’re not really supposed to create pods directly. Instead, a pod is really just a building block. They should be created via a higher level abstraction such as deployments. This way, Kubernetes can add on useful features and higher level concepts to make your life easier.</p>
<p>This lesson is gonna be covering the basics of the deployments with the following lessons covering auto-scaling and rolling updates. So let’s start by covering some theory, and then we’ll see deployments in action. A deployment represents multiple replicas of a pod. Pods in their deployment are identical, and within a deployment’s manifest, you embed a pod template that has the same fields as this pod spec that we have written before.</p>
<p>So you describe a state in the deployment, for example, five pod replicas of Redis version five, and Kubernetes takes the steps required to bring the actual state of the cluster to that desired state that you’ve specified. If for some reason one of the five replica pods is deleted, Kubernetes will automatically create a new one to replace it. You can also modify the desired state and Kubernetes will converge the actual state to that desired state. We’ll see a bit of that in this lesson with more on updates in a later lesson. The Kubernetes master components include a deployment controller that takes care of managing the deployment.</p>
<p>Now, let’s see how this works in practice. We’ll use our microservices three tier application to demonstrate deployments and we’ll replace the individual pods with a deployments that manage the pods for us. Let’s start by creating a new namespace called deployments for this lesson. So we’re gonna do that with kubectl create dash f 5.1 yaml, just like we’ve done previously. Now, a deployment is a template for creating pods. </p>
<p>A template is used to create replicas, and a replica is a copy of a pod. Applications scale by creating more replicas. This will be more clear when you just see the YAML files and as we demonstrate more features throughout this lesson. Now, I’m comparing the data tier manifest from the last lesson to our current manifest that uses deployments. I wanna highlight how there are significant similarities, but just a few changes, and the first change is the API version is now apps version one. Higher level abstractions for managing applications are in their own API group and not part of the core API. The kind is set to deployment, and our metadata from the last lesson is directly applied to said deployment.</p>
<p>Next comes a spec. The deployment spec contains deployment-specific settings and also a pod template, which has exactly the same pod spec as the last lesson in it. It in the deployment-specific section, the replica key sets how many pods to create for this particular deployment. Kubernetes will keep this number of pods running. We set the value to one because there cannot be multiple Redis containers. We’ll have one Redis pod.</p>
<p>Next, there’s the selector mapping. Just like we saw with services, deployments use label selectors to group pods that are in the deployment. The match labels mapping should overlap with the labels declared in the pod template below, and kubectrl will complain if they don’t overlap. The pod template metadata includes labels on the pods.</p>
<p>Note that the metadata doesn’t need a name in the template because Kubernetes generates unique names for each pod in the deployment. Similar changes are made to the app tier manifest and the support tier manifest, mainly adding a selector in a template for the deployment. We can complete the same process for the app and support tiers, also setting replicas to one for both cases.</p>
<p>One is actually the default, so it isn’t strictly required, but it does emphasize that a deployment manages a group of identical replicas. So let’s create the tiers now. Be sure to set the deployments namespace, and we’ll use multiple f options to create them all in one go with kubectrl create namespace deployments f 5.2, 5.3, 5.4 YAMLs.</p>
<p>Now let’s get our deployments with kubectrl get namespace deployments deployments. Kubectrl displays three deployments and the replica information. Note that they all show one replica right now. So remember that horrible scenario I described at the end of the last lesson? Well, we can see how deployments solve the problem by asking kades for the pods.</p>
<p>Note that each pod has a hash at the end of it. Deployments add this uniqueness to the names, automatically allowing us to identify pods of a particular deployment version. We can see how this works by running more than one replica in a deployment. We’ll use kubectrl scale command for modifying replica counts. We’ll scale the number of replicas in the support tier to five, which will cause the counter to increase five times more quickly. The scale command is equivalent to editing the replica value in the manifest file and then running kubectrl apply to apply the change. It’s just optimized for this one-off use case.</p>
<p>Now, if we run kubectrl get pods in the namespace of deployments, we can see the pods again to see what happened. Note that the support tier pods continue to show two of two ready containers. This is because replicas replicate pods, not individual containers inside of a pod. Deployments ensure that the specified number of replica pods are kept running. So we can test this by deleting some pods with kubectrl delete, the namespace deployments pods support tier, and then the hash. And now watch as Kubernetes brings them back to life.</p>
<p>All right, so Kubernetes can resurrect pods and make sure the application runs the intended number of pods. As a side note, I used the Linux watch command with the dash n1 option to update the output every one second. Kubectrl also supports watching by using the w option and any changes are appended to the bottom of the output compared to overriding the entire output with the Linux watch command. You might prefer one over the other depending on what you’re watching. But let’s go ahead and scale out the app tier to five replicas, as well, with kubectrl scale namespace deployments deployment app tier replicas five.</p>
<p>Now let’s get the list of pods with kubectrl namespace deployments get pods. As you can see, Kubernetes makes it really quite painless. They did all the heavy lifting for us, and now we can confirm that the app tier service is load balancing requests across the app tier pods by describing the service, kubectrl describe namespace deployments service app tier. And now observe that the service now has five endpoints matching the number of pods in said deployment. Thanks to label selectors, the deployment and the service are able to track the pods in the tier.</p>
<p>Let’s review what we’ve done in this lesson. We’ve used deployments to have Kubernetes manage the pods in each application tier. By using deployments, we get the benefits of having Kubernetes monitor the actual number of pods and converge to our specified desired state. We also saw how we can use kubectrl scale to modify the desired number of replicas in Kubernetes. This will do what it takes to realize the number of replicas we specify. We also saw how it seamlessly integrates with services that load balance across the deployments’ pods.</p>
<p>A word of caution with scaling deployments is that you should make sure that the pods you are working with support horizontal scaling. That usually means that the pods are stateless as opposed to stateful. The data for the app tier is stored in the data tier, and we could add as many app tier pods as we like because the state of the application is stored inside of the data tier.</p>
<p>With our current setup, we can’t scale the data tier out ‘cause that would create multiple copies of the application counter. However, if we never scale the data tier, we still get the benefit of having Kubernetes return the data tier to its desired state by using deployments. We also get more benefits when it comes to the performing of updates and rollbacks, which we’ll see in a couple of lessons. So it still makes sense to use a deployment for the data tier. We rarely should be directly creating pods.</p>
<p>Kubernetes has even more tricks up its sleeve when it comes to scaling. We arbitrarily scaled the deployment, but in practice, you would like to scale based on CPU load or some other metric to react to the current state of the system to make the best use of available resources. So let’s see how to do that in the next lesson.</p>
<h1 id="Autoscaling"><a href="#Autoscaling" class="headerlink" title="Autoscaling"></a>Autoscaling</h1><p>We’ve seen deployments work their magic in the last lesson. We also saw how to scale the deployment replicas but it would be nice to not have to manually scale the deployment. That’s where autoscaling comes in. Kubernetes supports CPU-based autoscaling and autoscaling based on a custom metric that you can define. We’re gonna be focusing on CPU for this course.</p>
<p>Autoscaling works by specifying a desired target CPU percentage and a minimum and a maximum number of allowed replicas. The CPU percentage is expressed as a percentage of the CPU resource request of that Pod. Recall that Pods can set resource requests for CPU to ensure that they’re scheduled on a node with at least that much CPU available. If no CPU request is set, autoscaling won’t take any action.</p>
<p>Kubernetes will increase or decrease the number of replicas according to the average CPU usage of all of the replicas The autoscaler will also increase the number of replicas when the actual CPU usage of the current Pods exceeds the target and vice versa for decreasing the number of Pods. It will never create more replicas in the maximum nor will they decrease the number of replicas below your configuring minimum. You can configure some of the parameters of the autoscaler, but the default will work fine for us.</p>
<p>With the defaults, the autoscaler will compare the actual CPU usage to the target CPU usage. And either increase the replicas if the actual CPU is sufficiently higher than the target, or it will decrease the replicas if the actual CPU is sufficiently below the target. Otherwise it will keep the status quo. Autoscaling depends on metrics being collected in the cluster.</p>
<p>Kubernetes integrates with several solutions for collecting metrics. We’re going to be using the Metrics Server which is a solution that is maintained by Kubernetes itself. There are several manifest files on the Kubernetes Metrics Server GitHub repo that declare all of the resources. We will need to get Metrics Server up and running before we can use autoscaling.</p>
<p>Once Metrics Server is running, autoscalers will retrieve those metrics and then make calls with the Kubernetes metrics API. The lab instance includes a Metrics Server manifest in the Metrics Server sub-directory. It’s outside the scope of this course to discuss all the resources that comprise of the Metrics Server. So all we need to do is create them and we can count on metrics being collected in the cluster.</p>
<p>Here we can use the kubectl apply command and then specify the Metrics Server folder to create all of the resources within the Metrics Server folder. kubectl control will then create all of the manifests it finds in that directory. You can see quite a few of these resources are created. One of them is the deployment, in the Metrics Server, runs actually as a pod in the cluster, and that pod is managed by that deployment. It takes a minute or two for the first metrics to start trickling in.</p>
<p>Let’s confirm that the Metrics Server is running by watching the pod. With kubectl top pods namespace deployments. This will list the CPU and memory uses of each pod in the namespace. You can use the top command to benchmark a pod’s resource utilization, and then subsequently debug resource utilization issues. Our pods are all using a small fraction of one CPU. The m stands for milli. 1000 milli CPUs equals one CPU.</p>
<p>Now that we have metrics, the other thing the autoscaler depends on is having a CPU request in the deployments odd spec. Let’s see how that looks in the app-tier deployment. I’ve highlighted the change from the previous lesson. Each pod will now request 20 milli CPU. Kubernetes will only scale the pods and each node with at least 0.02 CPU’s remaining. I also set the replicas to five to keep five replicas running.</p>
<p>Now, if we try to create the resources, kubectl will tell us that they actually already exist. Create will check if a resource of a given type and name already exists and it will fail if it does. We could delete the deployment and then recreate it but it would be nice to avoid the downtime that is involved. Instead, Kubernetes provides a command that can apply changes to existing resources. That’s what kubectl applies. So let’s apply that to 6.1 now.</p>
<p>Apply will update our deployment and do include the CPU request. It will warn us about mixing create and apply, but we can go ahead and ignore that. I’d encourage you to take the certified Kubernetes administrator course here on CloudAcademy if you’d like to learn more about the differences between create and apply.</p>
<p>So we’ve set the request low enough that the five replicas can remain scheduled in the cluster as we can see if we get the deployments output. Five actual pods are ready matching the five pods we desired. This completes like the prerequisites for autoscaling. The autoscaler, which has the full name of HorizontalPodAutoscaler because it scales horizontally or out, it’s just another resource in Kubernetes we can use a manifest to declare.</p>
<p>The HorizontalPodAutoscaler kind is part of the autoscaling version one API. It’s spec includes a min and max to set and lower the upper bounds on running replicas. The targetCPUUtilizationPercentage field sets the target average CPU percentage across the replicas. With the target set to 70%, Kubernetes will decrease the number of replicas if the average CPU utilization is 63% or below and increase replicas if it is 77% or higher.</p>
<p>Lastly, the spec also includes a scale target reference, that identifies what is actually scaling. In this case, we are targeting the app-tier deployment. We’ve added the equivalent kubectl autoscale command to achieve the same result, but we’ll stick with the manifests for everything. So let’s create the autoscaler with kubectl create file 6.2. Now we can watch the deployment until the autoscaler kicks in with the watch command. Well, would you look at that, the kernel is already updated, Kubernetes does not disappoint.</p>
<p>We can also describe the HorizontalPodAutoscaler to see what events took place. Now, it would be painful to type out pod autoscaler many times, but fortunately kubectl accept shorthand notations for resource types. So we’ll just run kubectl api dash resources for a full list of those shorthand notations. The output is sorted by the API group that appears in the third column. The lone autoscaling resource is the horizontalpodautoscalers and we can use hpa as the short name.</p>
<p>So let’s describe it with kubectl describe deployments hpa. We can see the successful rescale events and the current metrics are all below the target. We can also get the HorizontalPodAutoscaler for a quick summary of the current state with kubectl get namespace deployments hpa. The first number in the target expresses the current average CPU utilization as a percentage of the CPU request. </p>
<p>We can see that we are well below the target but we are at the minimum replicas so it won’t scale any further down. Let’s say we wanted to modify the minimum to two replicas. We could modify the manifest, save it, and then use the apply command or we could use the kubectl edit command which combines those three actions into one.</p>
<p>So let’s edit the odd autoscaler. The server side version of the manifest is presented in the vai console editor. If you haven’t used vai before, don’t worry, I’ll tell you everything we need to do. In general it’s a good idea to stick with modifying our local manifest so the changes can easily be checked into a VCs, but I want you to know that the edit command is available. You’ll notice that the server’s manifest contains additional fields that we didn’t configure. The server includes several fields automatically to help it manage resources. Type dash space one to jump the cursor down to the first occurrence of space one, which is our minReplicas field value.</p>
<p>Now press A to start editing the file. Then press your right arrow key to move the cursor after the one then press backspace two to change the minReplicas to two. Then press escape to stop editing followed by colon write quit or wq, to write to the file and quit to the editor. And Kubernetes will go out and automatically apply those changes to the HorizontalPodAutoscaler. Now you can watch the deployment with the Linux watch command. It’ll typically happen within 15 seconds which is the default period for the HorizontalPodAutoscaler to check if it should scale.</p>
<p>This wraps up our tour for autoscaling Kubernetes. To recap, Kubernetes depends on metrics with being collected in a cluster before you can use autoscaling. We accomplish that by adding the Metrics Server to the cluster. You must also declare CPU request in your deployments pod template so that autoscaling can compute each pod’s percentage CPU utilization. With those prerequisites taken care of, you can use the HorizontalPodAutoscaler. You configure it with a target CPU percentage and then min and max replicas.</p>
<p>Kubernetes will do all the heavy lifting for us, dynamically scaling our deployment based on the current state of the load. While we were doing this, we were able to also pick up the kubectl apply command, to update a resources rather than deleting and recreating it. And the edit command, which is shorthand for editing a live resource and then having it automatically applied. In the next lesson, we’re gonna wrap up our coverage over deployments by discussing how to deployments help you when deploying code or configuration changes. I’ll see you there.</p>
<h1 id="Rolling-Updates-and-Rollbacks"><a href="#Rolling-Updates-and-Rollbacks" class="headerlink" title="Rolling Updates and Rollbacks"></a>Rolling Updates and Rollbacks</h1><p>The last topic we will discuss on deployments is how updates work. Kubernetes uses rollouts to update deployments. And a Kubernetes rollout is a process of updating or replacing replicas with new replicas matching a new deployment template. Changes may be configurations such as environment variables or labels, or also code changes which result in the updating of an image key of the deployment template. In a nutshell, any change to the deployment’s template will trigger a rollout.</p>
<p>Deployments have different rollout strategies, and Kubernetes uses rolling updates by default. Replicas are updated in groups, instead of all at once until the rollout is complete. This allows service to continue uninterrupted while the update is being rolled out. However, you need to consider that during the rollout there will be pods using both the old and new configuration of the application. In such, it should gracefully handle that.</p>
<p>As an alternative deployments can also be configured to use the recreate strategy which kills all of the old template pods before creating the new ones. That, of a course, incurs downtime. So we’re going to be focusing on the rolling updates in this course. We actually have already rolled out an update in the last lesson when we added the CPE request to the app tier deployments pod template.</p>
<p>Scaling is an orthogonal concept to rolling updates. So all of our scaling events do not create roll-outs. Kubectl includes commands to conveniently check, pause, resume, and rollback rollouts. So let’s check out those now. We’ll use our deployments namespace again and focus on the app tier deployment.</p>
<p>First, we will delete the existing auto scaling configuration. Auto-scaling and rollouts are compatible, but for us to easily observe rollouts as they progress we’ll need many replicas in action. Deleting the autoscaler is going to help us with that.</p>
<p>Next let’s edit the app tier deployment with the following command. We’re gonna be jumping down to replicas and start editing them just change them after two and instead enter 10. It’ll be easier to see the raw in action with a large number of replicas. Also remove the resource request by pressing escape to stop editing. Then jumping down to resources and D three D to delete the three lines comprising the resource request. This will avoid any potential problems with scheduling the replicas if all 10 of the CPU requests can be satisfied. We’ll go ahead and write quit now. And we’re going to be watching this with the Linux watch command.</p>
<p>Now it’s time to trigger a rollout. Open the app to your deployment with Q+control+edit. From here, we can see the server added the default values for the deployment strategy. Specifically, the type is rolling update in the corresponding match surge specifies how many replicas over the desired total are allowed during a rollout. A higher surge allows new pods to be created without waiting for old ones to be deleted.</p>
<p>In the maxunavailable controls how many old pods can be to be deleted without waiting for new pods to be ready. We’ll keep the defaults of 25%. You may want to configure them if you want to trade off the impact on availability or resource utilization with the speed of the rollout. For example, you can have all of the new pods start immediately, but in the worst case you can have all of the new pods and all the old pods consuming resources at the same time effectively doubling the resource utilization for a short period.</p>
<p>With those fields out of the way, let’s trigger a rollout.This command will replace server with name cloudacademy for all of our previous pods. This is just a nonfunctional change for us but it will demonstrate the rollout functionality. So we’re go ahead and apply this with right click. Then we can immediately watch the rollout status with Q+control if we’re fast enough Q controlled rollout status streams progress updates in real time. You’ll see the new replicas coming in and old replicas going out.</p>
<p>To repeat this exercise until you see the entire flow and experiment with the number of replicas maxsurge and maxunavailable as you please. Rollouts may also be paused and resume. I’m gonna be splitting my window into two to better illustrate what is going on by entering tmax. This is a terminal multiplexer and I’m going to press control+B followed by the percent symbol to split the terminal vertically.</p>
<p>To switch between the two terminals, you can enter control+B followed by the left or right arrow. In the right terminal, I’ll prepare the same rollout status command we used before so that I can watch the status change as soon as we apply an update. And then we’ll jump over to the left terminal and edit the app tier deployment again. Let’s change the container name again by entering the following.</p>
<p>Next, we will quickly write the file to apply the changes then watch the status rollouts in the right terminal and pause the rollout mid-flight in the left terminal. Now the rollout is paused, but pausing won’t pause replicas that were created before the pausing. They will continue to progress to ready. However, there will be no new replicas created after the rollout is paused. We can use the rollout resume command for exactly that purpose. The rollout picks up right where it left off and goes about its business.</p>
<p>So I’m going to stop the terminal multiplexer now by doing control+B, and Y. So now consider you found a bug in the new revision and you needed to roll back. So kubectl has a handy command exactly for that. With kubectl, rollout, undo. This will roll back to the previous revision. You can also roll back to a specific version. You can also use kubectl rollout history to get a list of all versions and then grab the specific version and pass it into that.</p>
<p>That’s all for this demonstration of rolling updates and rollbacks. But before we move on let’s scale back the app tier to one replica to give us some more CPU resources. Deployments, and rollouts are very powerful constructs. Their features cover a large swath of use cases.</p>
<p>So let’s reiterate what we’ve covered in this lesson. We learned that rollouts are triggered by updates to a deployments template. Kubernetes uses a rolling update strategy by default. We also learned that we can pause, resume, and undo rollouts of deployments. There’s still so much more that we could do with deployments and rollouts depend on container status.</p>
<p>Kubernetes assumes that created containers are immediately ready and the rollout should continue. But this does not work in all cases. We may need to wait for the web server to accept connections. So here’s another scenario. Considering an application using a relational database, the containers may start but it will fail until a database and tables are created.</p>
<p>These scenarios must be considered to build reliable applications. This is where probes in init containers come into the picture. So we’ll take a look at integrating probes and init containers in our next two lessons.</p>
<h1 id="Probes"><a href="#Probes" class="headerlink" title="Probes"></a>Probes</h1><p>The previous lesson covered deployment rollouts. Kubernetes assumes that a Pod was ready as the container was started, but that’s not always true. For example, if the container needs time to warm up Kubernetes should wait before sending any traffic to the new Pod. It’s also possible that a Pod is fully operational but after some time it becomes non-responsive. For example, if it enters a deadlock state, Kubernetes shouldn’t send any more requests to that Pod and will be better off to restart a new Pod.</p>
<p>Kubernetes provides probes to remedy both of these scenarios and probes are sometimes referred to as health checks. The first type of probe are readiness checks. They are used to probe when a Pod is ready to serve traffic. As I mentioned before, often a Pod is not ready after its containers have just started. They may need time to warm caches or load configurations.</p>
<p>Readiness probes can monitor the containers until they are ready to serve traffic. But readiness probes are also useful long after startup. For example, if the Pod depends on an external service and as service goes down, it’s not worth sending traffic to that Pod since it can’t complete it until the external service is back.</p>
<p>Readiness probes control the ready condition of a Pod. If a readiness probe succeeds, the ready condition is true, else, it is false. Services use the ready condition to determine if the Pod should be sent traffic. In this way, probes integrate with services to ensure that traffic doesn’t flow to Pods that aren’t ready. This is a familiar concept if you’ve used a cloud load balancer. Back in instances that fail health checks are not served traffic, just as services won’t serve traffic to Pods that aren’t ready.</p>
<p>Services are our load balancers in Kubernetes. The second type of probe is called a liveness probe. They are used to detect when a Pod has entered a broken state and can no longer serve traffic. In this case, Kubernetes will restart the Pod for you. That is the key difference between these two types of probes. Readiness probes determine when a service can send traffic to a Pod because it is temporarily not ready and a liveness probe decides when a Pod should be restarted because it won’t come back to life. You declare both probes in the same way. You just have to decide which course of action is appropriate if a probe fails. Stop serving traffic or restart.</p>
<p>Probes can be declared on containers in a Pod. All of the Pod’s container probes must pass for the Pod to pass. You can define any of the following as the action probe to check the container. A simple command that runs inside of a container, an HTTP GET request or the opening of a TCP socket. The command probes succeeds if the exit code of the command is zero, else, it will fail. A GET request succeeds if the response code is between 200 and 399. A TCP socket probes succeeds if a connection can be established. By default, the probes check the Pods every 10 seconds.</p>
<p>Our objective in the hands-on part of this lesson is to test our containers using probes. Specifically, we will add readiness and liveness probes to our application. We will use the application of manifest from the deployments lesson as the base of our work in this lesson. But before we get started creating probes, let’s first crystallize the concepts by relating these probes to our application. The data tier contains one redis container. This container is alive if it accepts TCP connections. The redis container is ready, if it responds to redis commands such as get or ping.</p>
<p>There is a small but important difference between the two. A server may be alive but not necessarily ready to handle incoming requests. The API server is alive if it accepts HTTP request but the API server is only ready if it is online and has a connection to redis to request an increment, the counter. The sample application has a path for each of these probes. The counter and polar containers are live and ready if they can make an HTTP request back to the API server.</p>
<p>So let’s apply this knowledge to the deployment templates. We will go in the same order we just discussed but skip the support tier because the server demonstrates the same functionality. Let’s start by creating the probes in the namespace to isolate the resources in this lesson. Now take a look at this comparison that shows the addition of a name for the port and that the probes are the only changes to the data to your deployment. The liveliness probe uses the TCP socket type of the probe in this example, and by using a named port, we can simply write the name rather than the port number. This will protect us in the future if the port number ever changes and someone forgets to update the probe port number. Also, by setting the initial delay seconds, we give the redis server an adequate time to start.</p>
<p>We can also configure failure threshold, delays and timeouts for all probes. The default value will work for this example but you can reference Kubernetes documentation for more information on different values. Next, the readiness probe uses the exact type of probe to specify command. What this does, is runs a command inside the container similar to docker exec if you’ve used that before. The redis-cli ping command test if the server is up and is ready to actually process redis specific commands. Commands are specified as a list of strings.</p>
<p>Given the consequences of failing a liveness probe is going to be restarting a Pod. It’s generally advisable to have the liveness probe at a high delay than the readiness probe. I’ll also point out that by default three sequential probes need to fail before a probe is marked as failed, so that we have some buffer. Kubernetes won’t immediately restart the Pod the first time the probe fails, but we can configure it that way if we need to.</p>
<p>The particular delay depends on our application and how long it reasonably requires to start up. Five seconds should be more than enough to start checking readiness. And by default, we only need to pass a single probe before any traffic is sent to the Pod. Having the readiness initial delay too high will prevent Pods that are able to handle traffic from receiving any. So let’s create the new and improved data tier.</p>
<p>Now we can watch the GET output for the deployment to observe the impact of the probes. Note the ready column. This will show one of one replicas when the readiness check passes. With the watch option, new changes are appended to the bottom of the output. So we can see from the bottom line, the Pod transitions to the ready state after the number of seconds in the age column in the bottom line of the output. Watch the deployment for a while to make sure things are running smoothly. If no new lines appear, there are no changes and everything has stayed up and running. However, if something did go awry, I’d recommend using a combination of the described and logs commands to debug the issue.</p>
<p>Unfortunately, failed probe events don’t show in the events output but you can use the Pod restart counter as an indicator of failed liveness probes. Logs are always the best direct way to get at them. We will add some debug logging to the service so that you can see all the incoming probe requests after this. Onto the app tier. Notice that the debug environment variable has been added which will cause all the service requests to be logged.</p>
<p>Note that this environment variable is specific to the sample application and not for general purpose settings. Further down, probes are declared and this time they are HTTP GET probes. They send request to end points built at the server specifically for checking its health. The liveness probe endpoint does not actually communicate with redis. It’s a dummy that will always return 200 okay as its response for every request. Your readiness probe endpoint checks that the data tier is available. We’re also gonna be setting the initial delay seconds so the process has adequate time to start.</p>
<p>So let’s create the app tier deployment. And we’ll subsequently watch that deployment to verify containers are alive and ready. It may take some time to start the containers and wait for the initial delay seconds on the readiness probe. But after a short delay, the replica will be ready. We can now stream the logs to see what’s happening behind the scenes.</p>
<p>First, we’re to be getting the Pods to find a Pod in the deployment, then use kubectl logs with the dash f option to follow the log stream. And I’ll use cut to filter down what is going on. We can see that Kubernetes is firing both probes in 10-second intervals. With the help of these probes, communities can take Pods out of service when they aren’t ready and restart them when they enter a broken state.</p>
<p>To summarize what we saw in this lesson, containers in Pods can declare readiness probes to allow Kubernetes to monitor when they’re ready to serve traffic and when they should temporarily be taken out of service. Containers in Pods can declare a liveliness probes to allow Kubernetes to detect when they have entered a broken state and the Pod should be restarted.</p>
<p>Both types of probes have the same format and manifest files and can make use of either command, HTTP GET or TCP socket probe types. Remember that probes kick in after containers are started. If you need to test or prepare things before the container start, there is a way to do that as well. And that is the role of init containers and it is a subject of our next lesson. I’ll see you there.</p>
<h1 id="Init-Containers"><a href="#Init-Containers" class="headerlink" title="Init Containers"></a>Init Containers</h1><p>Sometimes you need to perform some tasks or check some prerequisites before a main application container starts. Some the examples include waiting for a service to be created, downloading files, or dynamically deciding which port the application is going to use. The code that performs those tasks could be crammed into the main application, but it is better to keep a clean separation between the main application and supporting functionality to keep the smallest footprint you can for the images. However, the tasks are closely linked to the main application and are required to run before the main application starts.</p>
<p>So Kubernetes provides us with an init container as a way to run these tasks that are required to complete before our main container starts. Pods may declare any number of init containers. They run in a sequence in the order they are declared. Each init container must run to completion before the following init container begins. And once all of the init containers have completed the main containers in the pods can start.</p>
<p>Init containers use different images from the containers in the pod, and this can provide some benefits. They can contain utilities that are not desirable to include in the actual application image for security reasons. They can also contain utilities or custom code for setup that is not present in the application image. For example, there is no need to include utilities like sed or awk or dig in an application image if they are only used for setup.</p>
<p>Init containers also provide an easy way to block or delay the start-up of an application until some pre-conditions are met. They are similar to readiness probes in this sense but only run at pod startup. It can perform other useful work. All of these features together make init containers a vital part of the Kubernetes toolbox. There is one more important thing to understand about it init containers. They run every time a pod is created.</p>
<p>This means they will run once for every replica in a deployment. And if a pod restarts, to say, due to failed live-ness probes the init containers would run again as part of that restart. Thus, you have to assume that init containers run at least once. This usually means that init containers should be unique. Running it more than once should have no additional effect.</p>
<p>Let’s add an init container to our app tier that will wait for Reddis before starting any application. We’ll see that the init containers have the same field as regular containers in a pod spec. The one exception is init containers do not support readiness probes because they must run to completion before the state of the pod can be considered ready. You will receive an error if you try to include a readiness probe in an init container.</p>
<p>Let’s see what the manifest looks like in our case. We’ll just be updating the app to your deployment, so we won’t make a new namespace. I’m comparing the deployment from the previous lesson with our new version with init containers. You can see that the fields are the same as what we have seen with regular containers. I’ve used the same image as the main application for simplicity, and it has everything we need in it. The command field is used to override the image’s default entry point command.</p>
<p>For this init container, we want to run a script that waits for a successful connection with Reddis. The script is already included in the image and is executed with the NPM run script await Reddis command. This command will block until the connection is established with the configured Reddis URL provided as in an environment variable.</p>
<p>Now let’s apply those changes to the existing deployment. After that, describe the deployments pod. And observe the event log, as it now shows the entire lifecycle with init containers. The await Reddis init container runs the completion before the server container is created. You can also view the logs of init containers using the usual logs command and specifying the name of the init container as the last argument after the pod name. This is specifically important when debugging init containers which prevents the main container from ever being created.</p>
<p>This concludes our tour of init containers. They give you another mechanism for controlling the lifecycle of pods. You can use them to perform some tasks before the main containers have an opportunity to start. This could be useful for checking preconditions, such as checking that depended upon services are created or preparing dependent upon files. The files use case requires knowledge of another Kubernetes concept, namely volumes which can be used to share files between containers. We’ll discuss all we should know about volumes in the next lesson. So continue on when you’re ready.</p>
<h1 id="Volumes"><a href="#Volumes" class="headerlink" title="Volumes"></a>Volumes</h1><p>Containers in a pod share the same network stack, but each has their own file system. It could be useful to share your data between containers. For example, having an init container prepare some files that the main container depends upon. The file system of containers are also limited to the lifetime of the container, so this could present some undesirable effects. For example, if the data tier container we are using in our examples crashes or fails a likeness probe, it will be restarted, and all of our data will be lost forever.</p>
<p>So this lesson is gonna cover the different ways Kubernetes hands non-ephemeral data that bring data from containers, Kubernetes volumes, and Kubernetes persistent volumes. Our goal for this lesson is to deploy the data tier from our sample application, using a persistent volume so the data can outlive the data tier pod. Again, this lesson builds on the code from the previous lessons, so let’s first discuss more about the options for storing persistent data and then apply them to our data tier.</p>
<p>Kubernetes includes two different data storage types. Both are used by mounting a directory in one container, and then that could be shared by containers in the same pod. Pods can also use more than one volume or persistent volume. Their differences are mainly in how their lifetime is managed. One type exists for the lifetime of a particular pod and the other is independent from the lifetime of the pods. Volumes are tied to a pod and their life cycle. Volumes are used to share data between containers in a pod and to tolerate container restarts.</p>
<p>Although you can configure volumes to use durable storage types that survive pod deletion, you should consider using volumes for non-durable storage that is deleted when the pod is deleted. Default type of volume is called emptyDir and it creates an initially empty directory on the node running the pod to back the storage used by the volume. Any data written to the directory remains if a container in the pod is restarted.</p>
<p>Once the pod is deleted, the data in the volume is permanently deleted. It’s worth noting that since the data is stored on a specific node, if a pod is rescheduled to a different node, that data will be lost. If the data is too valuable to lose when a pod is deleted or rescheduled, you should consider using persistent volumes. Persistent volumes are independent from the lifetime of pods and are separately managed by Kubernetes. They work a little bit differently.</p>
<p>Pods may claim a persistent volume and use it throughout their lifetime. The persistent volumes will continue to exist outside of their pods. Persistent volumes can even be mounted by multiple pods on different nodes if the underlying storage supports multiple readers or writers. Persistent volumes can be provisioned statically in advanced by a cluster admin or dynamic for a far more flexible self-serve use case.</p>
<p>Pods must make a request for storage before they can use a persistent volume. The request is made using a persistent volume claim, or PVC. A PVC declares how much storage the pod needs, the type of persistent volume, and the access mode. The access mode describes the persistent volume and whether it is mounted in read-only, read-write, or read-write many. There are three supported access modes to choose from, read-write once, read-only many, or read-write many. If there isn’t a persistent volume available to satisfy the claim and dynamic provisioning isn’t enabled, the claim will stay in a pending state until such persistent volume is ready. The persistent volume claim is connected to the pod by using a regular volume with the type set to persistent volume claim.</p>
<p>Both volumes and persistent volumes may be backed by a wide variety of volume types. It is usually preferable to use persistent volumes for more durable types and volumes for more ephemeral storage needs. Durable volume types include the persistent disks in many cloud vendors, such as Google Cloud Engine persistent Disks, Azure Disks, and Amazon Elastic Block Store. There’s also support for more generic volume types, such as network file system or NFS and iSCSI. That is quite a lot to take in, but everything should solidify with an example.</p>
<p>So our objective is to use a persistent volume for the sample application’s data tier, since we want the data to outlive the pod. In our example, the cluster has an Amazon Elastic Block Store volume statically provisioned and ready for us to use. To see dynamic provisioning in action, I’d encourage you to complete the lab on CloudAcademy entitled “Deploy a Stateful Application in a Kubernetes Cluster.”</p>
<p>Before we get into volumes, I want to cement the issue we are trying to solve. We can illustrate the issue of a pod computer losing data when they restart by forcing a restart of the data tier pod. First off, let’s look at the counter that’s been running since our deployments lesson. That is the value of our counter at the moment, and every second, it will keep increasing.</p>
<p>Now, if I force the pod to be restarted, we can observe the impact on the counter. One way to do that is to kill the Redis process, which’ll cause the data tier container to exist, and the data tier pod will automatically restart. We can use the exec command and run a command inside of the container the same way Docker exec does. So let’s open a bash shell inside of that container.</p>
<p>The change of a command prompt tells us we’re in the container now. We can now use the kill command to stop the main process of the container. But what is that ID? The ID of the main process, which is Redis in this case, will always be one, since it is the first process that runs inside of the container. We can see the command prompt change back, since the container terminated, so our shell was also terminated.</p>
<p>Now we can look at the counter value through the poller logs again and see that it is much lower than before because the data tier was completely wiped out when the pod restarted. This is what we want to avoid. So let’s create our new namespace and get on to the volumes. Now on to the data tier. There are three additions to the manifest, a persistent volume, a persistent volume claim, and a volume to connect the claim to the pod.</p>
<p>First is our persistent volume. This is the raw storage where the data is ultimately written to by the pod’s container. It has a declared storage capacity and other attributes. Here we’ve allocated one gibibyte. The access mode of read-write once means this volume may be mounted for reading and writing by a single node at a time. Note that it is a limit on the node attachment, not pod attachment.</p>
<p>Persistent volumes may list multiple access modes in the claim that specifies the mode it requires. The persistent volume can only be claimed in a single access mode at any time. Lastly, we have an AWS Elastic Block Store mapping, which is specific to the type of storage backed by the PV. You would use a different mapping if you were not using an EBS volume for storage. And the only required key for AWS’s Elastic Block Store is the volume ID, which is uniquely identified by the EBS volume. It will be different in your environment than mine, so I’ve added an insert volume ID placeholder that we will place before we recreate the PV. </p>
<p>Next, we have the persistent volume claim. The PVC spec outlines what it is looking for in a persistent volume. For a persistent volume to be bound to a PVC, it must satisfy all the constraints in the claim. We are looking for a persistent volume that provides the read-write once access mode and has at least 128 mebibytes of storage. The claim request is less than or equal to the persistent volume’s capacity and the access mode overlaps with the available access modes in the persistent volume. This means that the PVC request is satisfied by our persistent volume and will be bound to it.</p>
<p>Lastly, the appointment’s template now includes a volume which links the PVC to deployments pod. This is accomplished by using the persistent volume claim mapping and setting the claim name to the name of the persistent volume, which is data tier volume claim. You will always use persistent volume claim when working with PVs. If you wanted to use an ephemeral storage volume, you would replace it with an emptyDir mapping or other types that don’t connect to a persistent volume.</p>
<p>Volumes can be used in the pods containers and init containers, but they must be mounted to be available in the containers. The volume mounts list includes all the volume mounts for given container. The mount pass for different containers could be different even if the volume is the same. In our case, we only have one, and we are mounting the volume at slash data, which is where the Redis is configured to store its data. This will cause all of the data to be written to the persistent volume.</p>
<p>Now we are left with replacing the volume ID placeholder with the actual ID of the Amazon EBS volume. You can get it from the EC2 console in your browser, but we’ll just use the AWS CLI in this example. The volume can be obtained from the AWS EC2 describe command as follows. The full command is available to copy in the introduction to Kubernetes playground lab. The filter only selects the persistent volume which is labeled with the type equals PV tag, and the query outputs only the volume ID of the property volume.</p>
<p>The introduction to the AWS CLI on CloudAcademy explains this more so in greater detail. We only need to get the volume ID in this case. Then we can use the stream editor or set to substitute the occurrence of insert volume ID with the volume ID stored in vol_id. And with that, we are ready to create the data tier using a persistent volume. We’ll also create the app and support tiers, which don’t have anything new compared to the previous versions.</p>
<p>Let’s get the persistent volume claim, which has a short name of PVC in kubectrl, to confirm the claims request is satisfied. The status of bound confirms that the persistent volume claim is bound to the persistent volume. Now, if we describe the data tier pod, we’ll see that the pod initially failed to schedule because the claim needs to wait a while before it is bound to the persistent volume.</p>
<p>Once it is bound, the pod is scheduled and we can see a successful attachment with the volume event. Not only can our new design tolerate data tier pod container restart, but the data will persist even if we delete the entire data tier deployment. If everything goes to plan, we should be able to recover the Redis data if we then replace the deployment. That is because the deployment template is configured to use the PVC, and the PVC is still bound to the persistent volume storing our original Redis data.</p>
<p>So let’s verify all of this. Before we delete the data tier deployment, let’s get the last log line from the poller to see where our counter is at. If we delete the deployment and then replace it, we should see a number higher than this if the data is persistent. So let’s do that. And now we’re going to confirm that there are no data tier pods left running, and we’ll now recreate the data tier deployment.</p>
<p>Now, it takes a couple of minutes for all the readiness checks to start passing again and for some old connections to time out. This is mainly a side effect of the example application not being particularly good at handling this type of situation. So after a minute or two, we can get the poller’s last log, and voila, the counter has kept on ticking upward from where we left off before deleting the deployment.</p>
<p>Our persistent volume has lived up to its name. This concludes our lesson on volumes. We’ve covered volumes, persistent volumes, and persistent volume claims. In our example, we’ve shown how to use a persistent volume to avoid data loss by keeping the data independent from the life cycle of the pod or the pod’s volume. We also saw how kubectrl exec allows us to run commands in existing containers when we demonstrated how container restarts cause data loss when volumes aren’t used. You now have a solid foundation for volumes and persistent volumes.</p>
<p>In our next lesson, we’re gonna be covering two other useful Kubernetes features that you should keep in your toolbox. They also have a nice tie-in with volumes. There’s a few more lessons to go. Keep it up and I’ll catch you in the next one.</p>
<h1 id="ConfigMaps-and-Secrets"><a href="#ConfigMaps-and-Secrets" class="headerlink" title="ConfigMaps and Secrets"></a>ConfigMaps and Secrets</h1><p>Up until now, the deployment template has included all of the configuration required by Pod containers. This is a big improvement over storing the configuration inside the binary or container image. Having configuration in the Pod spec, also makes it a lot less portable. Furthermore, if the configuration involves sensitive information, such as passwords, API keys, this also presents a security issue.</p>
<p>So Kubernetes provides us with ConfigMaps and Secrets, which are Kubernetes resources that you can use to separate the configuration from the Pod specs. This operation makes it easier to manage and change configurations. It also makes for more portable manifests. ConfigMaps and Secrets are very similar and used in the same way when it comes to Pods. One key difference is that Secrets are specifically for storing sensitive information. Secrets reduce the risk of their data being exposed. However, the cluster admin also needs to ensure that all the proper encryption and access control safeguards are in place to actually consider Secrets being safe. We’ll focus on Secrets and leave out the security details from this introductory course.</p>
<p>Another difference is that Secrets have specialized types for storing credentials, such as requiring to pull images from registries. They also are good at storing TLS private keys and certificates. But I’ll refer you to the official documentation we need to make use of those capabilities. ConfigMaps and Secret store data as key value pairs. Pods must reference ConfigMaps or Secrets to use their data. Pods can use the data by mounting them as files through a volume or as environment variables. We’ll see examples of these in the demo.</p>
<p>We’re going to be using a ConfigMap to configure Redis using a volume to mount a Config file will use and a Secret to inject sensitive environment variables into the app tier. First, let’s create a Config namespace for this demo. With key control create -f 10.1.namespace.yaml.</p>
<p>Now let’s see how the ConfigMap manifest looks. First notice that there is no spec rather than we have key value payers of the ConfigMap stores and they’re under a mapping named data. Here we have a single key named Config. You can have more than one but one is enough for our purpose. The value of Config is a multiline string that represents the file contents of a Redis configuration file. The bar or pipe symbol after ConfigMap is Yaml for starting a multiline string and causes all the following lines to be the value of Config including the Redis Config comment. The configuration files set the TCP keep alive in max memory of Redis. These are arbitrarily chosen for this example. Separating the configuration makes it easy to manage configuration separately from the Pod spec. we will have to make some initial changes to the Pod to make use of the ConfigMap but after that, the two can be managed separately.</p>
<p>Let’s take a look at the updated data tier. I’m comparing what the data tier from our probes lesson which doesn’t include the persistent volume to avoid not being able to satisfy the persistent volume claim. Starting from the volumes, a new ConfigMap type of volume is added and it references the Redis config ConfigMaps we just saw. Items declare which key value pair we want to use from the ConfigMaps. We only have one in our case, and that is Config.</p>
<p>If you have multiple environments, you could easily do things like referencing a dev configuration in one environment and a production configuration in another. The path sets the path of the file that will be mounted with a Config value. This is relative to the mount point of the volume. Up above in the container spec, the volume mounts mapping declares the use of the Config volume but amounts of that etc Redis. So the full absolute path of the Config path, will be etc Redis, Redis Comf.</p>
<p>The last change that we need is to use a custom command for the container so that Redis knows to load the Config file when it starts. We do that by setting the Redis server, etc Redis comp as the command. With this setup, we can now independently configure Redis without touching the deployment template. As a quick side note, before we create the resources, if we’re dealing with the Secret rather than a ConfigMap, the volume type would be Secret rather than ConfigMap. And the name key would be replaced with secret name. Everything else would be the same.</p>
<p>Let’s create the resources. Now let’s start a shell in the container using Q control exec to inspect the effect of our ConfigMap. Start by cutting out the contents of the etc Redis, Redis Conf file. See that the contents match the ConfigMap value that we specified. Now to prove that Redis actually loaded the Config, we can output TCP, keep alive configuration value and make sure it matches the two 40 value in the file. And there we have it, separation of configuration and Pod spec is complete, so let’s exit out of the container.</p>
<p>Before we move on, I wanna highlight how changes to the ConfigMap interact with volumes and deployments. So let’s use Kube control edit to update the ConfigMap. And let’s change the TCP keep alive value from 240 to 500. Within around a minute, the volume will reflect the change we made to the ConfigMap. That is pretty slick, but Redis only loads the configuration file in start-ups so it won’t impact the running Redis process. And because we never updated the deployments template, we’d never triggered a rollout.</p>
<p>So let’s confirm the TCP keep alive Redis hasn’t been updated using the Redis CLI. There is something to keep in mind when you separate the configuration from the Pod spec to cause the deployments Pods to restart and have Redis supply the new configuration changes, we can use kube control rollout, namespace Config, restart deployment data-tier. This will cause a rollout using the current deployment template, and when the new Pod start, the Redis containers will use the new configuration. We can verify that with the Redis CLI.</p>
<p>Now we can quickly see how secrets work and see the similarities they have with ConfigMap. We will add a secret to the app here using an environment variable. It won’t have any functional impact but it will show the idea. Here is our Secret manifest, I mentioned upfront that you usually don’t wanna check in secrets to source control given their sensitive nature. It makes more sense to have secrets managed separately. You could still use manifest files as we are here, or the Secret could be created directly with kube control.</p>
<p>The command at the bottom of the file shows how to create the same Secret without a manifest file. Focusing on the manifest file, we can see that itself has a similar structure as our ConfigMap, except for the kind being Secret rather than ConfigMap. And Secrets can use a string data mapping in addition to the data one we use in our ConfigMap. As part of the effort to reduce the risk of Secrets being exposed in plain text, they are stored as base-64 encoded strings and Kubernetes automatically decodes them when using a container. I also have to point out that basics and foreign coding does not really offer any additional security. It’s not encrypting the values, and anyone can decode base-64, so to continue to treat the encoded strings as sensitive data.</p>
<p>With that cautionary statement out of the way, the stream data mapping allows you to specify Secrets with first encoding because kubernetes will coding them for you. It’s simply a convenience. If you use the data mapping, you must specify in coded values. In the API key secret is the one that we will use in the app tier, but I’ve included the encoded and decoded key value pairs to illustrate the basics for encoding.</p>
<p>In the data mapping, the encoded value is hello, base-64 encoded. So let’s create the Secret to see this. Now, if we describe the Secret, we can only see the keys. The values are hidden as part of the best effort to shield Secret values. We can see what the values are with kubecontrol edit. From here, we can see that string data mapping is not actually stored. The values are based 64 encoded and then added to the data mapping. The decoded value we entered in string data was hello, but now it is the base-64 encoded string beginning with AGV.</p>
<p>Shifting over to the app tier deployment, the API key environment variable is added. If value from mapping is used to reference it from the source for the value. Here, the source is Secret, so the secret key ref is used. If you need to get the environment variable from a ConfigMap rather than a Secret, you would use the ConfigMap key ref instead of Secret key ref. The name is the name of the Secret, and the key is the name of the key and the Secret you want to get the value from.</p>
<p>So let’s create the app tier now. And we can use the ENV command and the container dump all the environment variables. We can find the API key variable amid the wash of variables and observe the value as the decoded value that we entered in string data of our Secret manifest file and not encoded value. There is no need to decode the value inside of the container. I’ll just mention before wrapping up, that just like with using volumes to reference Secrets or Config maps, you should restart a roll out to how the deployment pods restart with a new version of the environment variables. Environment variables do not update on the flight like volumes. So actively managing the rollout is must.</p>
<p>This concludes the lesson on ConfigMaps and Secrets. So let’s recap what we’ve learned. ConfigMaps and Secrets are used for separating configuration data from Pod specs or what would otherwise be stored in container images. ConfigMaps and Secrets both store groups of key and value data. Secret should be used when storing sensitive data. Both can be accessed in the Pod containers by the referencing them using volumes or environment variables.</p>
<p>This was our last hands-on lesson for course. And the last hands-on lessons have prepped you to get started with managing and deploying applications in Kubernetes. For our next lesson, we’re gonna be highlighting some of the areas that the Kubernetes ecosystem is particularly strong with and that I think you should know about. You’re almost at the finish line, so join me in our next lesson and we’ll cross it together.</p>
<h1 id="Kubernetes-Ecosystem"><a href="#Kubernetes-Ecosystem" class="headerlink" title="Kubernetes Ecosystem"></a>Kubernetes Ecosystem</h1><p>The Kubernetes ecosystem is a very vibrant and healthy mixture of tools that can help you get more done and work more efficiently with Kubernetes. So I wanted to give you a sense of what’s happening around the core of Kubernetes. There are really way too many tools and topics to choose from, so I’ve only selected a few that are popular and worth knowing about. So, just know that I’m touching the surface of this ocean. </p>
<p>The first tool that I’ll mention is Helm. Helm is Kubernetes’ package manager. You write packages called charts. Then you use the helm CLI to install and upgrade charts as releases on your cluster. Charts contain all the resources like services and deployments required to run a particular application. Helm charts make it easy to share and complete applications built for Kubernetes. Helm charts can also be found on the Helm hub, similar to how you would find Docker images on Docker hub. </p>
<p>As an example of how you might use Helm. In our sample application, we used Redis for our data tier. So rather than build the data tier up from scratch and managing the resources ourselves we could take advantage of the register charts available for home. There is a highly available Redis chart that comes from the single point of failure in our example application. Charts can be installed with just a single Helm CLI command. Using available charts for running common applications can really free you up to focus on the applications that are core to your business. So definitely take a look at what’s available before deciding to roll your own solution.</p>
<p>Here’s another example. You can create a chart for your entire microservice application and pack up onto the services, deployments, everything and then subsequently publish it so other members of your organization or the public could use it. The next tool I want to mention is Kustomize with a K. Kustomize allows you to customize YAML manifests in Kubernetes. It can help you manage the complexity of your applications running inside of Kubernetes.</p>
<p>An obvious example is managing different environments such as tests and stage. We saw how we could use config maps and secrets to help with that. But Kustomize makes it even easier. Kustomize works by using customization dot YAML file that declares rules for customizing or transforming resource manifest files. The original manifests are untouched and remained usable as they are, which is an, a massive benefit compared to other tools that required templating in the manifest, rendering them unusable on their own.</p>
<p>Some examples of the kinds of rules you can create with Kustomize are generating config maps and secrets from files. In our data tier we had to write the contents of the Redis config file instead of the config map data. With Kustomized you can generate it directly from the config file rather than trying to keep track of the config file and config map and keep them tied together and in sync. You can also configure common fields across multiple resources. For example, you can set the namespace, labels, annotations, and name prefixes and suffixes using Kustomize itself. That makes it easy to customize around your organization’s conventions without polluting the original manifest files. The original manifest remain pristine and easy to share, and also reusable in other situations.</p>
<p>In our example, we had to keep our tier labels and prefixes manually synchronized across all of the resources. And then we had to specify the namespace at the command line every time to avoid hard coding in any space in the manifest. Kustomize can contain all of that complexity for us. The other thing that Kustomize can do is apply patches to any field in a manifest. Kustomize allows you to find a base group of resources and apply an overlay to customize to a base. This is an easy way to manage separate environments by applying a dev name prefix and a label for development environments.</p>
<p>Kustomize has been directly integrated with kubectl since Kubernetes 1.14. The kubectl customized command prints the customized resource manifests with customization defined in customization dot YAML. To accept the customization and then realize them in your cluster you can include the –kustomize or - k option to cube control create or apply.</p>
<p>The next one I’d like to discuss is Prometheus. Prometheus is an open source monitoring and alerting system. Prometheus’s built on top of many components, but at its core is a server for pulling in time series metric data and storing it. Prometheus was originally inspired by an internal monitoring tool at Google called borgmon. Similar to how Kubernetes itself was inspired by the board project at Google. Given that history it should come as no surprise that Prometheus is the de facto standard solution for monitoring Kubernetes.</p>
<p>Kubernetes components supply all their own metrics and Prometheus format makes it easy to integrate. You can collect a lot more metrics in Prometheus in the basic metric server we used in this course. That includes metrics outside of Kubernetes. There’s also adapters that allow Kubernetes to get metrics from Prometheus so you can do things like auto-scale pods based on custom metrics in Prometheus instead of only the CPU utilization that we saw in the course. </p>
<p>Prometheus has some built in options for visualization but it is commonly paired with Grafana to to create visualizations and dashboards. Prmoetheus also lets you define alert rules to send out notifications. It’s incredibly easy to install Prometheus in a cluster, and one way to do it is by using a Helm chart. </p>
<p>I’ll round out our talk about all these tools with two members of the Kubernetes ecosystem that relate to the types of applications that you deploy on top of Kubernetes. The first is Kubeflow. Kubeflow is aimed at making deployment of machine learning workloads on Kubernetes simple, scalable, and portable. </p>
<p>Kubeflow is a complete machine learning stack. You can use it for complete end to end machine learning including building models, training them, and serving them all within Kubeflow. Being built on Kubernetes, you can deploy it anywhere and get all of the nice features that Kubernetes provides like auto-scaling. Definitely check out Kubeflow if your requirements involve machine learning. </p>
<p>And the last one is Knative. Knative is a platform built on top of Kubernetes for building, deploying, and managing serverless workloads. Serverless has gained a lot more momentum because it allows developers and companies to focus more on the code and less on the servers that run it. This trend started with AWS Lambda which is synonymous with serverless. However, as the industry shifts to multi-cloud and avoiding vendor lock-in solutions built on top of Kubernetes can be deployed anywhere. This gives you the portability that you would get with containers, but for your entire serverless platform. Knative is not the only game in town when it comes to serverless but it does have the support of industry heavyweights like Google, IBM, and SAP. </p>
<p>This concludes our lesson about the Kubernetes ecosystem. We only touched on a few topics but I hope you can see the breadth of the ecosystem. I hope you can use some of these tools that we’ve seen but know that there’s a lot more to explore on your own.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>This is the last lesson in this course. I wanna congratulate you for completing this course in Kubernetes. In this lesson, I’m gonna be summarizing the high-level learning outcomes you’ve achieved and gonna be providing you with the next step for your Kubernetes journey. We’ve covered a lot of ground together. We started by explaining what Kubernetes is in our overview lesson. We followed that up with a summary of the architecture of Kubernetes and some of your options for deploying Kubernetes itself. </p>
<p>Next, we discussed the options available to you for interacting with Kubernetes. No doubt you’ve gotten to know kubectl much better than since when we first introduced it. Then we moved into the technical basics and built an application from a single pod all the way up to a self-healing, auto-scaling multi container, multi-tier application. Along the way, we’ve learned the following, Kubernetes terminology, how any number of deployments with service discovery work. Triggering, pausing, resuming, restarting, and undoing deployment roll-outs. Monitoring container liveness and readiness with probes. Preparing pods within init containers. How to configure persistent data storage which involves persistent volumes, persistent volume claims in volumes. And lastly, how to separate configuration and sensitive data from pod specs with containers using ConfigMaps and Secrets.</p>
<p>These learning outcomes should give you the confidence to start deploying applications with Kubernetes. The previous lesson took a step back and allowed you to see some of the exciting tools available to you in the vibrant Kubernetes ecosystem. So now that you’ve built a solid foundation in Kubernetes, where should you go next? Well, I’ve got a few recommendations. Cloud Academy has a lot more content on Kubernetes. In addition to the Kubernetes learning path, I’d encourage you to take a look at the certified Kubernetes administrator learning path or the certified Kubernetes application developer learning path. Both get into more details and describe more about topics that we’ve already covered but just didn’t have the time to fully explore. You can decide which one is best for you based on your current role or the kind of role you want to pursue, whether that’s more on the administrative side or more on the developer side. Both learning paths include tips on how to become a kubectl ninja and do things like generating manifests and explaining field values amongst other tips.</p>
<p>I’d also recommend that you convert your own application to run on Kubernetes. We did a similar exercise in this course, but I recommend taking an application that you know well and modeling it out to be a Kubernetes application. This will reinforce everything we have learned, and it can help you find different solutions to already existing challenges. You can also try to package your solution as a Helm chart if you want something you can share with others.</p>
<p>My last recommendation is to participate in the Kubernetes community. There are many ways to get involved. And on GitHub, you can report issues, help others solve their own issues, contribute updates and improvements to documentation, or submit pull requests for new features. If there’s a specific area of Kubernetes you are interested in, you can consider joining a special interest group, which is how Kubernetes organizes the community. There are also conferences, Slack channels, and Google groups to follow what’s happening so you can connect with other members of the community.</p>
<p>KubeCon is the flagship conference and is hosted virtually every year, and when available, in person. And with that, our journey through Kubernetes comes to an end. Please share your feedback so I can continue to improve and deliver just what you’re looking for. It may be the end of this course, but it’s only the beginning of a long and productive relationship with Kubernetes. Thank you for taking this course. And until next time, I’m Jonathan Lewey with Cloud Academy.</p>
<h1 id="1Introduction"><a href="#1Introduction" class="headerlink" title="1Introduction"></a>1<strong>Introduction</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/intro-to-k8s">Course GitHub repo</a></p>
<h1 id="11Autoscaling"><a href="#11Autoscaling" class="headerlink" title="11Autoscaling"></a>11<strong>Autoscaling</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/metrics-server">Metrics Server GitHub repo</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-Playground-10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-Playground-10/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Introduction-to-Kubernetes-Playground-10</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:40" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:40-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:25:30" itemprop="dateModified" datetime="2022-11-20T22:25:30-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-Playground-10/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-Playground-10/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Software-Development-Testing-and-Delivery-with-Docker-9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Software-Development-Testing-and-Delivery-with-Docker-9/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Software-Development-Testing-and-Delivery-with-Docker-9</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:38" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:38-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:24:52" itemprop="dateModified" datetime="2022-11-20T22:24:52-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Software-Development-Testing-and-Delivery-with-Docker-9/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Software-Development-Testing-and-Delivery-with-Docker-9/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Managing-Applications-with-Docker-Compose-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Managing-Applications-with-Docker-Compose-8/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Managing-Applications-with-Docker-Compose-8</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:37" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:37-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:23:42" itemprop="dateModified" datetime="2022-11-20T22:23:42-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Managing-Applications-with-Docker-Compose-8/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Managing-Applications-with-Docker-Compose-8/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Welcome to managing applications with Docker Compose.</p>
<p>About Me<br>I’m Logan Rakai and I’ll be your instructor for this course. I’m a content researcher and developer here at Cloud Academy. I’ve mostly worked on developing Labs, but I’m excited to be your instructor for this course. I’ve put a lot of thought into it and I hope you enjoy it. I have over ten years of experience in software research and development including five years in the cloud. I’m an AWS Certified DevOps Engineer Professional and a Microsoft Certified Solutions Expert: Cloud Platform and Infrastructure. You can connect with me on LinkedIn or on Twitter.</p>
<p>Who this course is for<br>This course is for anyone who could find themselves working with Docker containers. Among the roles that might be in that situation are <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/devops/">DevOps</a> engineers, developers, cloud engineers, and test engineers.</p>
<p>Prerequisites<br>In order to get the most out of this course, you should have experience with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-docker-2/course-intro-1/">Docker</a>. You probably have enough experience if you have ever written a Dockerfile or if you can answer questions like when should you use a volume? And when should you use a user-defined network? The course includes some development demos that are most beneficial if you have some software development experience. You can follow along and I’d encourage you to. You will need Docker version 1.13 or greater installed. I’ll be using a Mac with Docker for Mac installed but you can follow along in Linux or Windows. The code I’ll be using is all available on GitHub. A clickable link is available at the bottom of the transcript for this lesson. You’ll benefit from a good integrated development environment or IDE. I’ll use Visual Studio Code which is available for free on mac, Linux, and Windows.</p>
<p>What we’ll cover<br>In this Course, we’ll go over what <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/docker-compose-overview-1/">Docker Compose</a> is and why you would use it. Then we’ll explore the two parts of Docker Compose: Docker Compose files and the Docker Compose command-line interface. Next, we’ll get into demo-focused lessons beginning with running a web app with Compose. After that, we’ll see how to build images in a development scenario with Compose. Lastly, we’ll see how to use Compose to adapt an application to multiple different environments. In particular, we’ll see how to use Compose to manage an application in development and production. Wow, after seeing all those exciting topics I need a second to Compose myself.</p>
<p>Learning Objectives<br>After completing this course, you will be able to:<br>• Understand the anatomy of Docker Compose files<br>• Configure your application using Docker Compose files<br>• Use the Docker Compose CLI to manage the entire lifecycle of applications<br>• Build your own images from source code with Docker Compose<br>• Extend Docker Compose files to adapt applications to multiple environments</p>
<p>Feedback<br>I’m happy to hear from you. I make content for you and I want it to be as good as it can be. If you have any feedback, please get in touch with me by leaving a comment on the Comments tab below the video, by emailing <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, or by connecting with me on Twitter where my handle is @LoganRakai.</p>
<p>All right, that’s all for the introduction. In the next lesson, we’ll start to get a better idea of what Docker Compose is. Continue on to the next lesson whenever you are ready.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/docker-compose-training">https://github.com/cloudacademy/docker-compose-training</a></p>
<h1 id="Docker-Compose-Overview"><a href="#Docker-Compose-Overview" class="headerlink" title="Docker Compose Overview"></a>Docker Compose Overview</h1><p>Thanks for joining me. We will start to peel back the outer layers of Docker Compose in this overview lesson.</p>
<p>I will begin by looking at how you might accomplish a task without Docker Compose. This will highlight some of the issues that Docker Compose was made to solve and give motivation for this lesson and really the entire course on Docker Compose.</p>
<p>Then I will define Docker Compose at a high level in terms of what it can do and how it does it.</p>
<p>Lastly, I will introduce the two parts that make up Docker Compose.</p>
<p>By the end of this lesson, you will understand what Docker Compose is, why you would use it, and get to know a bit about the parts that make up Docker Compose.</p>
<p>Ok, to start off with, I want to share the motivation for Docker Compose and it will give an opportunity to review some core Docker concepts. Let’s say that you are working on developing an application with Docker. The application is relatively simple, consisting of two services that communicate with one another. Each service corresponds to a container in the diagram. One service, let’s call it service A, is entirely stateless and should be accessible from the host machine on a port. The other service, service B, is required to persist data. You have Dockerfiles for both services already.</p>
<p>The task at hand is to spin up a temporary environment with the application running to perform some tests and tear it down when you are finished. How do you create this environment?</p>
<p>In a world before Docker Compose, you might go about achieving that with the following series of Docker commands. You want to follow best practices in isolating your application’s containers from other containers that are running on the Docker host. To give you the most control over that you create a user-defined bridge network.</p>
<p>Using a user-defined network also gives you access to automatic DNS resolution from container names to IP addresses that your application might take advantage of.</p>
<p>For service B that persists data, you want to follow best practices again by choosing an option that is easy to back up and migrate, that can be managed using the Docker commands, can be safely shared among multiple containers, and have the flexibility to be stored on remote hosts or in the cloud. You naturally decide to create a volume to achieve this.</p>
<p>Now you build the Docker images using docker build with the –f option to specify the different Dockerfiles for each service.</p>
<p>Almost there. You only need a couple more commands to start running the containers using the images you built. The docker run command creates the containers and starts running them. You use the –network option so that both containers are in the app_network in order to communicate. You use the -p option so that Service A is accessible on a host port. You use the –mount option for service B to mount the volume into the container.</p>
<p>Now everything is up and running so you can perform some tests.</p>
<p>After your tests are completed you decide to tear down everything you created to keep the environment pristine.</p>
<p>Start by stopping the service A and service B containers. Once the containers are stopped, you are able to remove them. Next you can remove the images you built for service A and service B. After that, you’re free to delete the volume for service B’s persisted data. And finally, you can remove the network that enabled communication between the two containers. And that’s it.</p>
<p>Now let’s take a moment to discuss the solution in the grand scheme of things. Setting up and tearing down the environment required about ten Docker commands. Relatively speaking, it is not too bad compared to a solution using virtualization and even better when compared to what would be involved using bare metal. Docker has made great progress in creating environments quickly and without having to worry about nasty issues like configuration drift.</p>
<p>Now, the series of commands used isn’t the minimal number you could use to achieve the same result. For example, you might decide that it is acceptable to have Docker automatically create the volume for you as a side effect of the –mount option in the run command instead of explicitly creating the volume with docker volume. But even after some optimizing, the fact remains, there is a lot of typing involved to accomplish a fairly common task. Not to mention there could easily be more options involved for configuring each command. </p>
<p>However, it is natural to ask the question, can we do better? One option that could be useful when performing the commands more than once is to put all the commands in a script. That also allows you to check the script into version control to better manage changes and collaborate with other team members. But to write the script you still need to know all of the docker commands required and the sequence to put them in. You are in essence telling Docker how to do something with the commands in the script. This is sometimes referred to as the imperative paradigm in DevOps where you give explicit steps to perform.</p>
<p>As an alternative, wouldn’t it be nice to only have to declare what you want to make instead of the explicit steps to perform to create what you want? That is to take a declarative, as opposed to an imperative, approach and let some tooling figure out the steps to create what you want.</p>
<p>That is in essence what Docker Compose gives you with respect to defining and managing multi-container environments in Docker. You still get the benefits of being able to use source control, but the emphasis shifts to describing what you want instead of how to create it. By way of analogy, Docker Compose is similar to using Dockerfiles. You can run a container, attach to it, run some commands, and use Docker commit to create a new image from the container. But in most situations, you want the enhanced documentation and maintainability that a Dockerfile gives you for accomplishing the same task along with Docker build. Analogously, you usually want to use Docker Compose instead of running a series of Docker commands.</p>
<p>That gives you a high-level understanding of what Compose is and why you might use it. In the context of Docker, you can refer to Docker Compose simply as Compose. You will see a lot of examples of Compose in action throughout this course to develop a more robust understanding of Compose. </p>
<p>It is also worth noting that Docker Compose files can be used to manage multi-container applications that are distributed over a cluster of computing resources. To natively manage a cluster in Docker, you run Docker in swarm mode. Swarm mode is outside of the scope of this course. You can learn more about swarm mode in other excellent content on Cloud Academy. I just want you to know that the time you spend learning Docker Compose in a single host environment will pay dividends later on when you start running applications on a Docker swarm cluster.</p>
<p>Docker Compose consists of two parts: a specially formatted file called a Compose file, and a command-line interface.</p>
<p>A Compose file is where you declare services that comprise your application. You can do a lot inside a Compose file. The Docker commands you use for creating containers, volumes, and networks have equivalent declarative representations in Compose files. Knowing Docker commands makes writing Compose files quite easy given their close connection.</p>
<p>There is an entire lesson devoted to the details of Compose files in this course. To get a sneak peak of what’s to come, take a look at this example Compose file. The services section declares two services: web and redis. Each service has a set of options underneath it. For the web service, there’s an option for specifying the image, which ports to make expose on the host machine, and a volume to mount in the container created for the service. There is also the depends_on option which isn’t something that has an equivalent docker command option. It becomes necessary in the context of Compose because you need a way to specify the order services come up. You can no longer issue commands in a specific order as you would with docker commands. But I’m getting ahead of myself. We’ll cover a lot more details on Compose files in an upcoming lesson.</p>
<p>The other part of Compose is a command-line interface. It has a familiar feel to the docker command-line interface. Many of the commands you use with docker exist in docker-compose but generalized to multi-container applications. The name of the Compose binary is appropriately docker-compose. As an example of the power of docker-compose, the series of docker commands that was presented in the Motivation section can be performed with just two commands in Docker Compose. One for bringing the application up, and another for tearing everything down. The simplicity of creating isolated environments with Docker Compose makes automated testing one of its key use cases. As with Compose files, there is an entire lesson in this course devoted to the Compose command-line interface. You will also see a lot of both Compose files and command-line interface in all of the lessons that present examples showing you how to use Compose for different tasks.</p>
<p>It can be difficult to manage multiple container applications. In the example at the beginning of this lesson, many commands and options are required to start and stop a relatively simple multi-container application. The difficulty in managing multi-container applications grows with the number of containers involved.</p>
<p>Docker Compose lets you specify services in a multi-container application using a declarative paradigm. You declare what you want and Compose figures out how to create it.</p>
<p>There are two parts to Compose. Compose files are where you declare the services you want, and the docker-compose command-line interface is how you manage the multi-container application declared in a Compose file.</p>
<p>That’s all for this overview lesson on Docker Compose. We’ll dive deep into Compose files in the next lesson. Whenever you are ready, continue on and start to see your multi-container applications through the Docker Compose lens.</p>
<h1 id="How-to-Create-Docker-Compose-Files-Using-YAML"><a href="#How-to-Create-Docker-Compose-Files-Using-YAML" class="headerlink" title="How to Create Docker Compose Files Using YAML"></a>How to Create Docker Compose Files Using YAML</h1><p>It’s time to start digging into the details of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/course-introduction-18/">Docker Compose</a>. We’ll start by taking a close look at Compose files in this Lesson.</p>
<p>Agenda<br>I will start by giving you a brief introduction to the file format used for Compose files: YAML. If you haven’t used YAML before, you’ll learn enough to understand the Compose file examples used in this course.</p>
<p>Next, I will teach you about the root elements in a Compose file document. These are the top-level element of a Compose file and include: Compose file version, services in the application, volumes used by the services, and networks to be created. There is a lot of similarity between these sections of a Compose file and Docker commands that you are familiar with.</p>
<p>I will finish the lesson with a couple special topics in Compose files.<br>Let’s get started with YAML.</p>
<p>YAML</p>
<p>YAML is a data serialization language. Data serialization languages can be employed for a broad variety of programming scenarios including internet messaging, object persistence, or, in the case of compose, configuration files. Some of the design principles of YAML are that it should be human-friendly, and that it should work with any programming language. When YAML is stored in a file, the file can have a .yaml or .yml extension. Both are recognized as YAML. The capabilities outlined in the YAML specification are quite extensive. We’ll only really scratch the surface of what you can do with YAML.</p>
<p>Another, perhaps more common, data serialization language is JavaScript Object Notation (JSON). JSON formatted files are supported by Docker Compose. However, it is rare to see JSON Docker Compose files in practice. Comparing a YAML file to its JSON equivalent shows the cleanliness and fewer characters needed to represent the contents. In this example, Part of the reason why it can cut down on the line count is because it is whitespace sensitive. That means if you insert an extra space at the wrong place, the file will be corrupted. JSON on the other hand isn’t whitespace sensitive, meaning that you could squash all the whitespaces out and not harm the integrity of the file. However, readability would not fare so well. For that reason, JSON files tend to be formatted with abundant whitespace resulting in a less compact representation than YAML. It can take some getting used to working with a whitespace sensitive language, but IDEs tend to have support for formatting YAML files making it quite painless to work with. Some features of YAML, which is actually a superset of JSON, further enhance the compact representation of Compose files. You will see an example of this later in this lesson.</p>
<p>Data Types<br>Let’s take a look at a few basic data types in YAML. This list isn’t comprehensive but is enough to understand what usually goes into Compose files.</p>
<p>The first YAML data type we’ll consider are integers. Integers are whole numbers like zero or 1. You can also include a leading plus or minus sign to indicate positive or negative integers.</p>
<p>Strings are a sequence of characters that aren’t interpreted as a different data type. Strings can include spaces and the use of quotes to indicate the start and end of a string. Quotes are optional unless you use symbols that have a special meaning. Use single quotes around strings that use YAML syntactic characters like the pound symbol or colon. Use double quotes if you want to escape control characters like backslash n for newlines. If you want an integer to be interpreted as a string, you need to enclose it in quotes because it will be interpreted as an integer otherwise.</p>
<p>The null type is used to represent the absence of a value, or no value. It isn’t very common to see in Compose files but is a recognized data type in Compose. You use a tilde or the word null to represent it.</p>
<p>Booleans<br>Booleans are the last data type we’ll discuss. They indicate one of two values: true or false. Booleans can be represented with true, false, yes, no, on, off as well as the same words with the first letter capitalized or all the letters capitalized. In YAML any matches of a pattern including yes, and on get converted to true. This can cause unintended consequences if you have conditions testing Boolean values. For example, if you had a condition that was checking if a variable has a Boolean value of yes but it automatically got converted to true. Compose simply disallows the use of Booleans in contexts where such issues can arise to be on the defensive side.</p>
<p>You’ll see an error message similar to this if you use a Boolean value. In this particular case, I tried to use a Boolean as an environment variable. As the error message hints, only strings, number, or a null can be used. If you want to use true or false, yes or no, on or off as values, you need to use the string representation by wrapping them in quotes. If you ever encounter an error message involving true or false, this is probably what it relates to.</p>
<p>Collections<br>Collections are data structures that allow you to collect basic data type values in an organized manner. The first YAML collection we’ll consider is the mapping. Mappings are also known as dictionaries or hashes in different programming languages. Maps consist of keys mapped to values. The syntax for a mapping is a key followed by a colon, a space and then the value. The space is important. A mapping can have multiple key value pairs.</p>
<p>Mappings can also have mappings as values. Using a mapping as the value for a mapping is referred to as nested mappings. In Compose files the inner mapping is usually located on a new line with indentation.</p>
<p>There is also an inline syntax that lets you write a nested mapping on a single line. You use braces to wrap the inner mapping in this case. You may see this from time to time but I think it tends to hurt readability and should be avoided.</p>
<p>Sequences<br>The other kind of collection is a sequence. Sequences are also called lists or arrays in other languages. Sequences are simply lists of values.</p>
<p>You use dashes to indicate items in a sequence. Each item goes on its own line at the same level of indentation.</p>
<p>Sequences can also be nested. To represent an inner sequence, you use an indented dash on a new line below the outer sequence.</p>
<p>As with mappings, there is an inline syntax to represent a sequence on a single line. You use brackets to wrap a comma-separated list of items to use inline syntax. You see this inline syntax used for command options in Compose files.</p>
<p>Combos<br>You can also combine the sequence and mapping collections. For example, you can have a sequence as the value of a mapping. The dash in a sequence as a mapping value also counts as indentation, so you don’t have to use spaces for indentation when including a sequence in a mapping. That means both of these examples are valid YAML. You might prefer to indent for consistency, but they are not required and you will see both styles used in practice.</p>
<p>Similarly, you can use mappings in a sequence. There is again two ways to represent a mapping in a sequence. You can use a line with only a dash followed by the indented mapping, or you can include the first line of the mapping on the same line as the dash. Both styles get used and you should be aware of each.</p>
<p>The last thing I want to mention about YAML is that is supports inline comments. This makes it much more useful in terms of documenting Compose files than JSON which doesn’t support commenting. In YAML, a comment starts when a pound character is encountered and continues to the end of the line. The exception to a pound character starting a comment is if it’s within a quoted string.</p>
<p>That is enough YAML to get through the example Compose files used in this course. It’s also enough to understand most examples you might find online and enough for you to write your own Compose files. As I mentioned before, using an IDE that can automatically format YAML can save you some headaches. Compose also includes a command for verifying configuration files in case you need to verify your YAML syntax and configuration declared in a Compose file.</p>
<p>Compose files<br>Now that we’ve built up a foundation in YAML, we can focus on Compose file specifics. As mentioned earlier, YAML Compose files are the focus and make up the vast majority of Compose files in use. We will only consider version 3 Compose files.</p>
<p>From this compatibility table, you can see that version 3 Compose files require a Docker Engine of version 1.13 or higher. The <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/docker-compose-cli-1/">Docker Compose command-line interface</a> has a different release schedule than the docker engine, and requires version 1.10 or high for version 3 Compose files. Docker recommends using version 3 Compose files. There are multiple minor version numbers for version 3, for example 3.0 and 3.4. Unless otherwise noted, examples used in this course follow the 3.0 Compose file format. This covers versions of Docker released since the beginning of 2017.</p>
<p><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/compose-file">https://docs.docker.com/compose/compose-file</a> The Compose file reference is a great reference for understanding all the configuration options available to you in Compose files. This course covers many frequently used configuration options, but leaves out many that might be useful in certain situations. Let’s take a quick look at it now.</p>
<p>Here we are at the Compose file reference page. It defaults to showing reference material for the latest major version which is 3 at this time. There is a handy navigation bar on the right to see all the available configuration options</p>
<p>Just note that it can be confusing at times because some configuration options only apply to Docker swarm mode (deploy), some only apply when not running in swarm mode (security_opt), or specific Compose file minor versions (Extension fields), and sometimes different options are available for Docker running on Linux or windows based systems (isolation).</p>
<p>Version<br>A YAML Compose file is a mapping with several keys at the root or top level. The first one we’ll discuss is the version. The value for the version must be a string. The string can specify a major version number only, for example 3, or a major and minor version number, such as 3.1. If a only major version is included, the minor version is implied to be 0, or the earliest release. So if you want to use a feature that came out in the latest minor version release, you need to include it in the version string. The version string tells Compose how the contents of the file should be parsed.</p>
<p>Services<br>The services mapping is where you configure the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/containers/">containers</a> created for services in your application. Each service is configured in a nested mapping under the services key. You can assign an arbitrary name for each service. In the image, the service names are web and redis. Each service has a nested mapping that declares the configuration for containers started for the service. The configuration is the main piece of declaring services in a Compose file, so let’s focus in on that.</p>
<p>Configure the container for the service.<br>Inside of the service configuration mapping, you declare the configuration options for service containers in a way that is similar to how you would configure containers using docker run command parameters.</p>
<p>This table shows how you would configure a container using the docker run command and the corresponding configuration key in a Compose file. The only required argument for docker run is the image name, which you specify using the image key in a Compose file. There are a few ways to configure a volume with docker run, but they all map to the volumes key in Compose files. There are different syntaxes in Compose to support the different volume configurations that you use different parameters for with docker run. The -p parameter to publish ports on the host corresponds to the ports key in Compose files. The -e parameter for setting environment variables in a container maps to the environment key in Compose files. With docker run you have two parameters for setting up logging and both parameters go into a nested mapping under the logging key. The last one that I’ll mention is security-opt for setting security options which only differs in the use of an underscore in Compose files. There are many more, but aside from a using YAML and slightly different names your experience with docker run will make writing Compose files easy.</p>
<p>Caveats<br>There are some points to be aware of for the correspondence between docker run and Compose file service configuration. Some docker run parameters that you might expect to be able to use in Compose only work in swarm mode. This is the case for setting runtime constraints such as -m for memory limit, or –cpus for number of cpus. It’s possible to run Docker in swarm mode with a single machine so it isn’t a significant barrier. However, Swarm mode is outside of the scope of this course so that’s all I will say about it.</p>
<p>Other docker run parameters such as -d to run in detached mode or –rm to clean up the container when it exits are specified through the command-line interface and not in the Compose file configuration.</p>
<p>Dependencies<br>Because Compose supports multi-container applications, there are additional options for configuring services that don’t exist with docker run.</p>
<p>The depends_on key provides a way to list the services a service depends on. Docker Compose can use the dependency relationships to determine the order to start services. If you tell Compose to start a specific service instead of the entire application, Compose can also use the information to automatically start any dependencies of the service. However, it’s important to note that Compose won’t wait for the dependencies to be ready before starting a service. For example, Compose can start a database before a web service but it can’t account for the time it takes the database process to be ready to handle connections. Because of this, it’s best practice to write your applications in a way that can tolerate connection failures. If that isn’t an option, you can use scripts that poll the dependencies to wait until they are ready. One such script is called wait-for-it.sh.</p>
<p>The other key that expresses dependencies is links. Links correspond to the link parameter of docker run allowing you to grant access to a container to access an exposed port on a private interface and to provide aliases to reach containers. Links in Compose carry the same meaning but additionally determine startup order of services, the same way depends_on does. Generally, networks are a better way to express communication relationships. We’ll discuss more about networks in a bit.</p>
<p>Examples<br>To get a taste of service configuration in Docker Compose, let’s look at some examples. I’ll arbitrarily use the redis image for the examples. Starting off simple, this docker run command will start a container named app-cache using the redis image.</p>
<p>The equivalent service configuration in a Compose file would like this. In the first line we need to specify that we are using version 3 of compose files. The services mapping is a pretty simple conversion of the docker run command parameters. The container name used as the service key, and the redis image argument used as the image key-value pair.</p>
<p>Now, if you specify a tag to pull a specific version of the image,<br>you add the same tag to the image value. Note that although the colon is a special character in YAML, you don’t need quotes around the string because colon only takes on special meaning when followed by a space.</p>
<p>If you want to the redis server port of 6379 available on the Docker host, you include the -p argument like so.</p>
<p>The corresponding Compose file includes a ports key which has a sequence of port strings. Just like with Docker run you can specify host and container port, or just the container port to allow Docker to choose an available host port. When specifying host and container port like in the example, it’s a good idea to put quotes around the string because YAML will parse numbers separated by a colon as sexagesimal or base 60 numbers if the numbers are less than 60.</p>
<p>In this last example, a command with arguments is added to override the default command.</p>
<p>The same string can be used as the value of the command mapping in a Compose file.</p>
<p>Or you can use the same syntax you would use in a Dockerfile for setting the default command of an image. In this case you do need quotes around the last argument “yes” otherwise it gets treated as a Boolean value. It’s best to always quote as you would in a Dockerfile. You might also recognize that syntax as the inline syntax of a sequence.</p>
<p>That means you can also express the command in the normal sequence syntax. This form can make long commands more readable. You get the idea of how to work with service configuration in Compose files through these examples. You might need to consult the Compose file reference to get the correct key names but it’s usually a fairly straightforward exercise to write the configuration.</p>
<p>Volumes<br>The next root key in the Compose file mapping is volumes. It is an optional key. You use the volume mapping in a way that is similar to how you use docker volume create. Services can reference volumes in each service’s volumes configuration key.</p>
<p>It’s a good time to point out that you can use volumes in the service’s configuration even if you don’t have a volumes key in your Compose file. The use case for the root volumes key is to use named volumes and to share volumes across services.</p>
<p>You can also declare external volumes that have been created outside of the context of the Compose file. For example, a volume created by docker volume create, or a different compose file. In a volume’s nested configuration mapping, you can set the external key to true to declare an external volume. If the external volume doesn’t exist, an error will be reported.</p>
<p>Take a look at this example Compose file using the root volumes key. There are two named volumes declared on lines 13 and 14. The first, called named-volume, doesn’t have any nested configuration. This will create a volume using the default local volume driver. YAML sees the absence of any value and represents it as a null. You could equivalently write a tilde or the word null for the value on line 13. The other named volume is called external-volume and is configured as an external volume.</p>
<p>In the app-cache service’s volumes configuration starting on line 5, you can see a few ways to declare volumes in Compose. The first is using a named volume in the root volumes key and will be mounted at &#x2F;data in the container. The next example on line 9 uses a relative path to set the source of the mount. Relative paths are relative to the location of the Compose file. The last example on line 11 will have the Docker Engine create a volume automatically to mount at &#x2F;tmp&#x2F;stuff in the container.</p>
<p>Lastly for volumes, you can configure a custom volume driver using the driver and driver_opts keys. The named volume in the example called ebs-volume uses the convoy docker volume plugin by rancher. If you want to specify any driver-specific options, you can do so under the driver_opts key.</p>
<p>Networks<br>Networks are declared under the top-level networks key. By now it will come as no surprise, that network configuration in Compose files aligns closely with the docker network create command. However, there are a few new concepts to networking in Compose.</p>
<p>By default, Compose will automatically create a new network using the default bridge driver for an application in a Compose file. The name of the network is based on the name of the directory the Compose file is in with default appended on the end. All containers created for services in the Compose file join the default network and can be reached and discovered by the corresponding service name. This is slightly different from running containers with docker run and not specifying a network. In that case, the containers get added to the default network named bridge.</p>
<p>To review how you can use a bridge network, consider this example Compose file. There are two services, web, and cache. The Compose file doesn’t declare any networks so all service containers will join the default network created for the app declared in the Compose file.</p>
<p>In the default network, the cache container can reach the web container by using web as the hostname for the container.</p>
<p>Similarly, web can reach cache by resolving the cache hostname. Because web is inside the network, it uses the container port of 6379 to connect.</p>
<p>From the Docker host machine, cache can be reached at the host port of 36379. What about web? How can the host reach web?</p>
<p>It cannot reach web, because no ports are published making it accessible to the host.</p>
<p>Besides the default network, you can declare custom networks under the root networks key. This gives you more control and allows you to create more complex network topologies. Custom networks can be external to the application, similar to external volumes worked. You add the external: true mapping to tell Compose to verify the network already exists and join services to that external network.</p>
<p>This example Compose file illustrates how custom networks are used. The file declares two networks under the top-level networks key beginning at line 18. The frontend network uses the default network configuration, while the backend network refers to an external network created outside of the Compose file. The networks mapping for each service shows the proxy and app service are part of the frontend network, and the app and db service are in the backend network. With this configuration, the db and proxy services are isolated from one another. This approach of limiting communication between services to allow only what is necessary is a best practice. The db service further configures the alias database for itself on the backend network. The app container could resolve the hostname db or database as a result of the alias. The mapping syntax must be used for specifying aliases.</p>
<p>Special Topics: Variable Substitution<br>I will finish of the lesson by discussing a couple special topics in Compose files, starting with variable substitution. Variable substitution allows you to generalize your Compose files to create different environments without having to modify the Compose file. Docker Compose will substitute shell environment variables in place of variable placeholders in a Compose file. You indicate a variable by using a dollar sign before the variable name, and optionally surrounding the variable name in braces. If the variable is not defined in the environment where Docker Compose is running, an empty string is substituted for the variable. The example on the bottom half of the slide shows a snippet of a Compose file that uses a variable named REDIS_TAG for the image tag. In the shell environment, the REDIS_TAG environment variable is set to 4.0.5. When Docker Compose creates the application, the environment variable is substituted into the image value string.</p>
<p>Special Topics: Extension Fields<br>The other special topic I wanted to mention is extension fields. Extension fields let you reuse configuration blocks and move important configuration fragments to the root of the Compose file. Extension fields only work with version 3.4 or higher Compose files, so you will need to include a minor version number to get this to work in version 3. To use extension fields, add a root key that begins with x- and add the configuration to reuse under it. To insert the configuration somewhere else in the file, you use YAML anchors. Anchors allow you to create an alias for the configuration that effectively inserts the configuration fragment wherever you reference the anchor. The example on the right shows how an extension field called x-logging is used for configuring logging in two services. The anchor is indicated with an ampersand and is named default-logging. In the service definitions, the asterisk precedes the anchor name to indicate that an anchor is being used as the logging value. The extension field mapping including both options and driver are inserted at the proper indentation level where the default-logging anchor is referenced with an asterisk. The example illustrates how extension fields are useful for removing configuration clones.</p>
<p>Recap<br>This lesson began with a crash course in YAML. You learned about the string, integer, null, and Boolean data types. You also learned about the two collections in YAML: mappings, and sequences. This was just enough YAML to understand and write Compose files.</p>
<p>Next you understood the anatomy of a Compose file. The configuration in compose files falls under the top-level mapping keys of version, services, volumes, and networks. The configuration for services, volumes, and networks are similar to parameters you pass for docker run, volume create, and network create commands, but formatted in YAML syntax.</p>
<p>We finished the lesson by covering a couple special topics in Compose files. Variable substitutions allow you to generalize a Compose file using environment variables. Extension fields let you reuse configuration fragments to cut down on clones.</p>
<p>Up until now, you’ve only seen how to declare what’s in your multi-container applications. In the next lesson, you will see how to use the docker-compose command-line interface to run and manage the applications. When you are ready to learn how to start running your Compose file applications, continue on to the next lesson.</p>
<p><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/compose-file">https://docs.docker.com/compose/compose-file</a></p>
<h1 id="Features-and-Commands-of-Compose-Command-Line-Interface"><a href="#Features-and-Commands-of-Compose-Command-Line-Interface" class="headerlink" title="Features and Commands of Compose Command-Line Interface"></a>Features and Commands of Compose Command-Line Interface</h1><p>In this lesson, we’ll see how to use the Docker Compose command-line interface to turn the multi-container applications described in Compose files into actual running environments in Docker.</p>
<p>Agenda<br>I’ll start by reviewing some of the high-level features of the Compose CLI.</p>
<p>Next, I will go through some of the installation options available for different platforms.</p>
<p>Lastly, I’ll finish the lesson by looking at how to use the docker-compose CLI by the reviewing common commands and parameters</p>
<p>Features<br>One of the features of the Compose CLI that I want to highlight is its ability to run multiple isolated environments on a single host. Some scenarios where this is extremely useful is on a continuous integration server where you need to run automated tests for each build version. The ability of Compose to run multiple isolated environments means you don’t have to sequentially iterate through each version. A development scenario where this comes in handy is when you may need to create multiple copies of an environment for different feature branches. You can use variable substitution in the Compose file to create the desired branch environment. We’ll talk more about development scenarios in later lessons in this course.</p>
<p>The Compose CLI uses a parallel execution model to perform tasks for creating and deleting an application environment. Not everything can run in parallel due to dependencies and limitations in Docker, but when possible parallel execution is used to reduce the time it takes to manage applications.</p>
<p>Another useful feature to be aware of is the change detection capabilities of Compose. Every time you start a container for a service in Compose, the configuration is cached. If you later restart a Compose application, Compose will reuse any <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/containers/">containers</a> that haven’t changed configuration. This is a bit like how layers are cached when building images from a Dockerfile. Just like with Dockerfiles, you can instruct Compose not to use the existing containers and instead force all containers to be rebuilt.</p>
<p>The last feature, using the term loosely, is that <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/course-introduction-18/">Docker Compose</a> is an open-source project on Github with an active community. You can report issues and make feature requests there. If you are familiar with the Python programming language, you can fork the project and modify the source to better suit your needs. Maybe even make a pull request to have your improvements included into the project.</p>
<p>Installation<br>Before we get into using the Compose CLI, I want to say a few words about getting Compose installed on your system.</p>
<p>For mac users,<br>Compose comes installed with the Docker for Mac application and Docker Toolbox for older systems.</p>
<p>For Windows users,<br>If you obtained Docker through Docker for Windows, or Docker Toolbox<br>Compose came included with that.<br>If you are running the native Windows Docker Daemon, on Windows Server 2016 or Windows 10 with the Anniversary Update<br>You need to install Compose separately. You can choose the appropriate version of Compose and download an installer from the Compose Github releases page. For example, you could download version 1.17.0 to get the version of Compose I’m using for this course.</p>
<p>For Linux systems,<br>Docker Compose is included in many distribution repositories<br>For example, on CentOS or RedHat distributions you can use yum or dnf, and apt on Debian-based systems.<br>If Compose isn’t available through the distribution’s package repo or you want a specific version,<br>you can get Compose from the Github release page.</p>
<p>Usage<br>All right! With that out of the way, we can look at how to use the Compose CLI. I’ll show a couple slides to cover the Compose CLI basics and then hop over to my terminal to briefly illustrate using the Compose CLI.</p>
<p>docker-compose follows similar patterns to the docker CLI. You specify options to Compose, followed by a command, and add arguments for the command at the end. You can always use the –help argument to print a help page for any command.</p>
<p>Compose will use the Docker Daemon running on the host by default.</p>
<p>You can connect to a Docker Daemon running on a remote host using the -H option. Along with that, you can secure the connection to the remote host using transport level security options. This requires the remote host to have been configured to use tls for the Docker Daemon.</p>
<p>For commands that reference a Compose file, the default Compose files that the CLI tries to find in the current directory are named docker-compose.yml or docker-compose.yaml.</p>
<p>It can be restrictive using only the default Compose file, so the -f option is provided to allow you to specify a path to any file that you want Compose to use as the Compose file for a command.</p>
<p>Each isolated application is associated with a project in Compose. The project is given a name and that name appears in resources that get created by Compose. For example, the names of networks and containers created by Compose begin the project name followed by the arbitrary name key declared in the Compose file.</p>
<p>The default project name is the name of the directory containing the Compose file.</p>
<p>You can assign a custom project name by using the -p option.</p>
<p>Commands<br>As we have seen, Compose tries to make adoption easy for users already familiar with Docker. Most of the commands in the Compose CLI are familiar Docker commands that are generalized to work with multi-container applications.</p>
<p>This is the list of commands that exist in both Docker and Compose CLIs as of Compose version 1.17. As an example of how a command is generalized to multi-container applications, consider the stop command. In Docker, you use the stop command to stop one or more running containers passing the container names as arguments to the command. In Compose, the stop command will stop all containers declared in a Compose file, unless you provide the names of individual services to stop. Most commands generalize as you would expect. Some commands like config are not related to the Docker command. Config is useful for validating the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/anatomy-of-a-compose-file-1/">YAML</a> and configuration in a compose file. It’s important to note that you can still use the docker CLI to work with resources created by Docker Compose. For example, containers created by Compose are listed by docker ps since they are created by the Docker daemon. Compose is just wrapping commands to generalize them to how you would expect them to work with multi-container applications.</p>
<p>After removing the commands that exist in Docker, there are currently only two non-deprecated commands that are unique to Compose and they are big ones.</p>
<p>The first is up.</p>
<p>The up command performs the actions required to instantiate the application described in a Compose file. It starts by creating default and named networks as applicable, and any named volumes.</p>
<p>It then takes the actions required to bring up service containers. This includes building images if required, then creating and starting containers, and finally attaching to the containers to aggregate output and error streams from the containers. When the command exits, the containers are all stopped. However, you can use the -d option to let the containers run in detached mode.</p>
<p>The up command is also responsible for performing change detection when you bring up an application that has already been brought up. It will recreate containers with changed configuration and join them to the appropriate networks. Any connections that were established with the original container are closed. There are several options for configuring how Compose does this if the default behavior isn’t what you want.</p>
<p>The other command unique to Compose is down. Down is a partial opposite of up.</p>
<p>What I mean by partial opposite is that down will only remove containers, as well as any named and default networks by default.</p>
<p>It won’t delete volumes or images that up created, unless you pass arguments instructing the command to do so. Up and down make it easy to perform integration tests in a continuous integration pipeline. You can simply wrap a test script between up and down to have the tests run in the isolated environment. Ok, with that, we’ve covered enough of the Compose CLI to try it out and to use the help argument to find out more information when needed.</p>
<p>@Terminal docker-compose<br>Here we are at my terminal. I want to demonstrate using the Compose CLI just to give a first look at it. We will cover more in-depth examples showing Compose in action in the remaining lessons in this course.</p>
<p>To start with, you can always get the usage information by appending –help on any command or docker-compose itself. I’ll pipe it into more to page through the output. I won’t read through the output since we’ve discussed most of what is shown. The help output finishes with a list of all the available commands.</p>
<p>To get more information on the up command, I’ll enter docker-compose up –help. I’ll jump down to the options to see what’s available for configuring the behavior of the up command. Just as an example, –no-deps can be used to prevent starting dependent services. This doesn’t sound very useful when you first bring an application up, but if you later modify the configuration of one service, it can be useful to not restart the services that the one service depends on. As another example, adding the –remove-orphans option can clean up any services that are no longer declared in a compose file. This can happen if you delete a service outright from a Compose file or if you rename one.</p>
<p>To finish up, I’ll demonstrate how to use docker-compose’s config command to debug any YAML or configuration errors. You will also see how config shows you the effective configuration that is used by Compose after variable substitutions and extension field references.</p>
<p>If I switch over to VS Code, I have a Compose file open called 1-extension-fields.yml. It follows an example shown in the slides for using extension fields.</p>
<p>It’s using version 3.4 which is good because that’s the earliest version that supports extension fields. To see if everything is ok with the file, run the config command on it. At the terminal, I’ll use the -f option to specify that file as the Compose file to use. So, Compose reports an error about services.cache.command contains true which is not valid. If I jump back to Code, it seems strange at first because there is no instances of true in the file. But remember that multiple words get mapped to true in YAML. Yes is one of them. Code has even changed the color to indicate that it isn’t a string value. I’ll add quotes around it to correct the error.</p>
<p>Running config again reveals a different error. The cache service doesn’t set an image or build command so it can’t be created. I’ll set the image to redis, but I want to use variable substitution to set the tag, like so. Now when I run the config command again there are no errors and the effective configuration is displayed. Here you can see the default-logging YAML references have been replaced with the associated configuration under each services logging key. Compose reports a helpful warning at the top about the REDIS_VERSION variable not being set so an empty string is substituted. You can see that in the displayed configuration. That will need to be corrected. I’ll export the variable<br>Export REDIS_VERSION&#x3D;4.0.6<br>And the config command output confirms the variable is substituted into the configuration. I’ll leave it at that for now. We’ll see several more examples of Compose at the command-line in upcoming lessons.</p>
<p>Recap<br>This lesson started by outlining some of the features of the Compose CLI including, how it can create multiple isolated environments on the same host. This makes it appealing for continuous integration, testing, and development scenarios. It also has built-in Compose file change detection support to only do what is required to bring the application to the desired state described in a Compose file.</p>
<p>We saw that the docker-compose CLI follows the same pattern as the docker CLI. Most of the commands in docker-compose are analogous to ones you find in docker.</p>
<p>There are however, two important commands that are unique to compose. Up does everything required to bring an application described in a Compose file up, and down brings an application down, removing containers and networks, but leaving images and volumes untouched by default.</p>
<p>In the next lesson, we’ll go a step farther and bring up a web application with Compose using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/run-a-web-app-1/">pre-built images from Docker Hub</a>. When you are ready to see more of Compose in action, continue on with the next lesson.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/docker/compose/releases">https://github.com/docker/compose/releases</a></p>
<h1 id="Deploying-and-Configuring-a-Web-Application-with-Compose"><a href="#Deploying-and-Configuring-a-Web-Application-with-Compose" class="headerlink" title="Deploying and Configuring a Web Application with Compose"></a>Deploying and Configuring a Web Application with Compose</h1><p>Welcome back. This lesson will go through deploying a web application with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/course-introduction-18/">Compose</a>. This lesson is more applied than the previous ones. We’ll spend most of our time at the command-line and making changes to a Compose file.</p>
<p>Agenda<br>I’ll start by briefly introducing the web application.</p>
<p>Then we’ll get right into the demo.</p>
<p>Wordpress<br>The web application that we’ll use is WordPress. WordPress is a popular content management system or CMS. You can create websites and blogs in WordPress. WordPress is written in PHP and uses MySQL as a database. The images for WordPress and MySQL are maintained by Docker. Both images have over 10 million pull on Docker Hub. This scenario relates to operating the application. This is where images have been created and you pull them from a registry, possibly Docker Hub, or your own corporate image registry. The following lesson gets into developing applications with Compose. Now the stage is set, so let’s hop over to Visual Studio Code to look at the Compose file I’ve prepped for the application.</p>
<p>@Terminal<br>Here is the Compose file, wordpress.yml. All of the contents just fit on the screen. Let’s take a moment to go through it since it ties together a lot of what we’ve seen in the course so far. There are two services, one for WordPress which is where the PHP application code exists and is served up by an apache web server, and one for the MySQL database. Both services use a specific tag on the image to have more control over the environment and to prevent unexpected changes from creeping in. Both services also have a restart key with the value of always. This is the same as the restart option for docker run. To make the application more production-worthy it’s a good idea to have the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/containers/">container</a> restarted automatically if it exits for some reason. You definitely want to persist data for a CMS, so the database service is using a named volume called db_data. This mounts into &#x2F;var&#x2F;lib&#x2F;mysql in the container and is where MySQL stores its database files. Each service has a set of environment variables configured. The db service uses a mapping for its environment variables to create users and a database called wordpress. The wordpress service uses a sequence of strings to configure the database user and host with equal signs separating the variable names from their values. Both syntax styles are allowed and equivalent. One variable to highlight is WORDPRESS_DB_HOST in the wordpress service. It configures the database hostname. The value that is assigned is db on port 3306, the default port for MySQL. There are no named networks in this file. How can the wordpress service connect to the db? Both services will be added to the default network that Compose will create. The last bit of configuration, is the publishing of the wordpress port. The app will be available on port 8000 on the Docker host. The string is enclosed in quotes as a best practice although not strictly necessary in this case because the container port, 80, wouldn’t mistakenly be interpreted as a base-60 number.</p>
<p>Switching over to my terminal, I’m in the webapp directory which contains the wordpress.yml compose file. I’m starting with a clean Docker environment. No containers, no volumes, and only the default Docker networks. I’ll bring the wordpress application up now, using the -f option to specify a custom compose file. The db’s mysql image gets pulled first followed by the wordpress image. I’ll speed this up while the image layers get pulled. Now you can see in the output that the container for the db service is created first followed by the container for the wordpress service. This is guaranteed because the db is in the wordpress services depends_on sequence. The webapp at the beginning of the container name is the project name and it defaults to the current directory name which is webapp. The up command then attaches to the containers and aggregates their output. Compose uses color to distinguish between the output from different containers. You can see WordPress attempting to connect to the database and failing. Recall that depends_on doesn’t wait until the database is ready, it only sequences the order containers are started in. Fortunately, WordPress follows the best practice of having the application handling failed connections and retrying until a connection is made. At this point, the db and wordpress services have finished initializing. I’ll stop the docker-compose command with ctrl+z instead of exiting with ctrl+c so the containers don’t get stopped. After clearing the screen, I’ll list the containers with docker ps and confirm that the containers made by compose are like any others. Checking on the volumes, we can see the db_data named volume created by Compose. The unnamed volume comes from the WordPress image. It declares a volume for the WordPress web assets that get served up by the Apache web server in the &#x2F;var&#x2F;www&#x2F;html directory. Next, we can see the default network Compose created in the networks list.</p>
<p>To verify the application is functioning correctly, I’ll jump over to a browser and navigate to port 8000 on localhost where the WordPress service published its web server container port. The first time you use wordpress, you need to configure the language, a site title, and some user information. I’ll set the title to Composing. The other details aren’t important. With those details set, I can log in and see the admin dashboard. Up in the upper right corner I can navigate to the public site that’s hosted by default. Here it is with the Composing title that I specified earlier. Everything is working as expected. We successfully ran a web application in Compose!</p>
<p>Let’s see how the Compose change detection works. Say we decide to accept the risks of using the latest tag for the wordpress image. I’ll change the tag, save the file, and go back to the terminal.</p>
<p>I’ll repeat the up command except using the -d argument to run the containers in detached mode so the shell prompt will be returned to me after the command finishes. It starts by pulling down the latest version of the wordpress image. After that, it checks and sees that the db service container already running matches the configuration in the Compose file. There is no need to restart it. It then detects that the wordpress container doesn’t match the configuration in the Compose file and recreates it using the updated configuration. You can change the behavior of up to suit your needs in different scenarios though. If you were uncertain if any other services had changed configuration and wanted to avoid recreating the db container at all costs, you can specify the –no-deps argument to up along with the service you want to bring up. In the output, notice that no check of the db service is made. If you want to recreate all containers even if their Compose configuration hasn’t changed, you can pass the –force-recreate argument. The output indicates each container is being recreated now. To be certain, check the output of docker ps and see the containers have just been created. Now I will demonstrate bringing the application down with the down command. The output describes the steps Compose is taking, stopping containers, then removing containers, and lastly the default network Compose created. Docker ps -a verifies there is no trace of any service containers. I can bring the application back up again very quickly having previously downloaded the images. Now, if I load WordPress in the browser, what do you think I will see?</p>
<p>We don’t see the first-time configuration page, we see the same composing site as before. That’s because docker-compose down leaves the volumes by default.</p>
<p>You can change that default behavior though. Add –rmi all to remove all the images used in the Compose file, –volumes to delete the named volumes declared in the Compose file as well as well as any anonymous volumes attached to service containers, and –remove-orphans to delete any project containers that are no longer defined in the Compose file. This time you can see the removal of volumes and images in the output. There is still the old 4.9.0 wordpress image kicking around however. I’ll use the image prune command to remove any images that are left over.</p>
<p>Closing<br>That was pretty awesome! Managing the multi-container web app with Compose was painless. In the next lesson, we’ll see how Compose works when we need to build the images using Dockerfiles.</p>
<h1 id="Using-Compose-Configurations-and-Commands-to-Build-Images"><a href="#Using-Compose-Configurations-and-Commands-to-Build-Images" class="headerlink" title="Using Compose Configurations and Commands to Build Images"></a>Using Compose Configurations and Commands to Build Images</h1><p>Up until now, we have seen <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/course-introduction-18/">Compose</a> working with images pulled from a Docker registry. Can you use Compose in development scenarios when the code isn’t ready to be sealed in an image? How do you build images with Compose? These are the questions I’ll answer in this lesson.</p>
<p>Agenda<br>I’ll begin by getting into the Compose file configuration and Compose commands needed for building images.</p>
<p>With that foundation in place, I’ll finish the lesson with a demo that illustrates how to use Compose to build an image in a development scenario. Compose will bring the application up and your code changes will be reflected in the running <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/containers/">container</a> without needing to rebuild or stop the container. This provides a similar experience to developing on your local machine without Docker, but the server and all of the application dependencies are running inside a container.</p>
<p>Building in Compose<br>When you build images in Compose, you make use of the same tried and true Dockerfiles that you use when building images with the docker build command. We won’t get into the details of Dockerfiles in this lesson, but I’ll quickly review one in the demo.</p>
<p>To instruct compose to build an image, add the build key in a service’s configuration. There can be more than one service in a Compose file with a build mapping.</p>
<p>The Docker-compose up and docker-compose build commands can be used to build images. We’ll take a closer at the build key and these commands in the next few slides.</p>
<p>Build Key<br>If a service has a build key present, Docker Compose will build the image for the service. There are two forms of build configurations in a Compose file. The short form sets the build value to the path of the build context which is where the Dockerfile is located.</p>
<p>The longer form uses a nested mapping. The context is a required key and it has the same meaning as the context for the short form. The Dockerfile key is optional. If specified, the value is the name of the Dockerfile to use. If it isn’t specified, Dockerfile will be used as the name for the file containing the image build instructions. Args are optional as well, and can be used to pass Arg values at build time. The Dockerfile should have corresponding ARG instructions.</p>
<p>The built image will be given a name that follows the pattern of Compose project name followed by the service name. If you want to use a different name, or want to specify a tag for the built image, beside the default latest tag, you can do so by using the image key. The image specified the image to pull from a Docker registry before, but if a build configuration is present for a service, the image is interpreted as the name of the built image.</p>
<p>Docker-compose up will build any image for services that don’t have one already built. Subsequent up commands won’t rebuild the image, unless you pass the –build option. This might not give enough control over built images, so there is another command for building.</p>
<p>Docker-compose build will build images or rebuild them if they already exist. Just like with the docker build command, there are a couple options to customize the behavior of docker-compose build. The –no-cache option will prevent using the layer cache causing all layers to be rebuilt. The –pull option will always attempt to pull a newer version of a base image described in the Dockerfile. That’s all there is to building in Compose.</p>
<p>Demo<br>Now, we’ll get into a demo to illustrate building in Compose. The demo will use a NodeJS project that uses MongoDB for persistence. The image shows the app. It simply accumulates whatever messages users enter. The goal of the demo is to build an image with Compose that will allow on the fly updates as you modify the source code. No rebuilds and no stopping the containers. This gives the instant feedback that developers crave. Let’s see how to do that.</p>
<p>Here in VS Code, I have a Dockerfile for the project open. It’s called dev.dockerfile. I’ll try to stay as language-agnostic as possible but the specific RUN instructions are specific to NodeJS development. At a high level, the instructions install the dependencies for developing and running the application. On line 5, nodemon is installed. nodemon is a tool that watches for changes to development files and automatically restarts the server to reflect the changes. On line 17, nodemon is set as the default command for running a container using the image. On lines 8 through 11, the src directory is created and set as the working directory. Then the application dependencies file, package.json, is added to the src directory in the image. The npm install command installs all of the dependencies in the src directory. Note that only the dependency file is added and not any source files. The image has everything the code needs to run but not the code itself. The development server port of 3000 is exposed on line 14. So how will this image be used to develop the code? The default command is expecting a file at &#x2F;src&#x2F;app&#x2F;bin&#x2F;www to start the server but it doesn’t exist in the image. How will that work? The answer to both questions is by mounting a volume. Specifically, the source will be mounted at &#x2F;src&#x2F;app. The default command will then start a server using the code in your development environment. Let’s take a look at the Compose file, that I’ve called dev.docker-compose.yml.</p>
<p>There are two services, app and app-db, that are in the backend network. App publishes the port of 3000 so that the host can access the development server. There are a couple environment variables to configure NodeJS for development and to pass the hostname of the database. What’s most important for this lesson is the build configuration. Because the dockerfile doesn’t have the default name, the mapping syntax is required. The context is ., representing the directory of the Compose file which is also where the dockerfile is. The volumes key also plays an important role. The src directory on the host is mounted at &#x2F;src&#x2F;app where the development server expects to find it. Let’s bring up the application using the Compose CLI. Thanks to the image configuration mapping, the built image will be named accumulator and will receive the default tag of latest.</p>
<p>I’ll use the up command which builds the image since there is no prior image to use. I’ll skip ahead to the build part. Each of the instructions in the Dockerfile are executed just like with docker build. I’ll jump ahead to when the image is ready. There are some harmless warnings because some optional dependencies are specific to macs but the image is Linux. The output reports that the accumulator:latest tag is used. Compose also gives a helpful warning telling you to use the build command or pass the –build option to rebuild the image. Let’s verify the app is up and running.</p>
<p>I’ll point my browser to localhost on port 3000 and voila, the accumulator app is up and running. I’ll enter some messages and refresh the page to ensure they are persisted in the database. Everything looks to be working.</p>
<p>I’ll hop back to VS Code, and edit one of the views by adding a colon after Enter messages to accumulate, to confirm that the change gets updated in the browser. I’ll save that change, and refresh the browser.</p>
<p>And there is the colon. That change was to a file that doesn’t require restarting the server. To confirm that nodemon is correctly watching for changes, I’ll modify a server-side JavaScript file.</p>
<p>I’ll add some exclamation marks at the end of the development environment notice that appears in the upper right corner. Going back to the browser and refreshing</p>
<p>We see the changes reflected. No stopping the container and no build command required.</p>
<p>To show that nodemon detected the change, let’s look at the app service’s logs. There it is in green, restarting due to changes. That’s pretty cool. You can use the development image to bundle up all the dependencies and all you need on your machine is the source files. You don’t need the dependencies installed locally. By relying on the image for dependencies, you are a step closer to having parity between development and production because the production image would be using the same dependencies. The chance of the code working on your machine but not in production is greatly reduced. We’ll look more at dev-prod parity in the next lesson.</p>
<p>I’ll take the application down now. And if I bring it back up, do you think the test messages I entered into the application will still be there?</p>
<p>Let’s refresh the page and see. In this case, the messages are gone. Recall that the db image isn’t using a volume, so once the container is removed, everything is gone. That give an easy way to start fresh when developing this app, but you could easily add a volume if you wanted to persist the messages.</p>
<p>Recap<br>This lesson illustrated how to build images and develop with Compose. The next lesson will further build upon what we’ve learned in this Lesson to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/extending-compose-1/">adapt Compose to multiple environments</a> so you can share common configuration between development and production. When you are ready, continue on to the next lesson to see how it’s done.</p>
<h1 id="How-Compose-Handles-and-Combines-Multiple-Files"><a href="#How-Compose-Handles-and-Combines-Multiple-Files" class="headerlink" title="How Compose Handles and Combines Multiple Files"></a>How Compose Handles and Combines Multiple Files</h1><p>We saw how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/building-in-compose-1/">use Compose to build an image</a> in a development scenario where the code is not ready to be sealed into the image. But what about once the code is ready? How can you use Compose to make the production image? Do you need to use two independent Compose files and Dockerfiles? This lesson will clear up these questions.</p>
<p>Agenda<br>I’ll start with a discussion of how Compose handles multiple Compose files.</p>
<p>Then I’ll mention a few considerations for using Compose for production environments.</p>
<p>I’ll finish by reviewing the concepts we discuss in a demo. The demo extends the app from the previous lesson to use Compose for development and production environments.</p>
<p>Multiple Compose Files<br>Although it’s an option to maintain completely separate compose files for each environment you maintain,</p>
<p>Compose has a useful feature that can combine Compose files.</p>
<p>The semantics of combining Compose files is to treat the first file as a base configuration and each additional file overrides configuration specified in the base configuration. The overrides can add configuration that isn’t present in the base configuration as well, not only strictly overriding existing values in the base configuration.</p>
<p>By default, Compose is set up to read two Compose files, the familiar docker-compose file, as well as an optional override file called docker-compose.override.yml.</p>
<p>The -f Compose option can be used multiple times to specify non-default override files. Each override file overriding the previous ones.</p>
<p>The Docker Compose config command is useful when writing and debugging multiple Compose files. It will display the effective Compose file after everything is combined.</p>
<p>The -H option of Compose allows you to manage the application on different Docker hosts, since the different environments are probably on different machines.</p>
<p>Multiple Compose Files Example<br>Let’s consider an example that uses Compose in two environments, one that is not intended for development, and another that is for development. The Dockerfile for the application is shown here. It’s follows the non-development image pattern of copying all of the source code into the image, and installing the dependencies. Note that the source files are in the &#x2F;src directory.</p>
<p>Here is the docker-compose.yml file, which plays the role of the base configuration in our example. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/course-introduction-18/">Compose</a> is instructed to build the web image using the current directory as the context, along with publishing a port on the host, and running a redis container. During the build the source files in the current directory get added to the image. The image can then create containers that don’t have any dependency on source files outside of the container. This is what you want in a non-development scenario. Let’s see how an override Compose file can extend the application to work in development scenarios. Can you guess?</p>
<p>On the right, I’ve shown a development override Compose file. To use source files on your local machine instead of inside the image, you can use a volume. The example mounts the current directory at &#x2F;src in the container. Because of the way that layering works in images, the files that are in the container are effectively overwritten by the volume. With this override, you can modify the source files on your local machine and see the changes reflected in a running container. It isn’t always possible to use a common Dockerfile for each environment you intend to use the container. In that case, you can override the Dockerfile for each environment using the build’s dockerfile configuration in each Compose file.</p>
<p>Production Considerations<br>Moving to production environments is worthy of a course of its own. I will mention a few Compose file considerations for moving to a production environment, but know that there is more to it.</p>
<p>Remove any volumes for source code. You want the code to be frozen inside of a production image.</p>
<p>Consider using the always restart policy so services will automatically bring themselves back up if they exit.</p>
<p>Avoid host port conflicts that can prevent an application from coming up by letting Docker choose the host ports to use. You do this by only specifying the container port in the ports sequence.</p>
<p>Usually the runtime and the application have environment variables to configure production mode. This may reduce verbosity of logs and disable debug information.</p>
<p>The last one that I’ll mention, is consider additional services that may be useful in production. For example, monitoring and log aggregation services. Now let’s wrap up with a demo illustrating some of the concepts of using multiple compose files for development and production environments.</p>
<p>Demo<br>I have a project open that is similar to the example in the previous lesson, except I’ve modified it to use multiple Compose files. The development Dockerfile is the same as before. This is the file, dev.dockerfile, to refresh your memory. Because the application uses different ports and default commands for development and production, I’ve written a separate production Dockerfile.</p>
<p>This is it here, prod.dockerfile. It doesn’t install any development dependencies like nodemon and it copies all of the source files into the image, not just the dependency file. The exposed port has changed from 3000 to 8080 as well.</p>
<p>Now let’s get to the main subject, the Compose files. This is the base Compose file, docker-compose.yml, that has configuration that is common to both environments. Then each environment has its own override file to add its own unique configuration. Alternatively, you could use the base configuration for the production environment, and have a single override file for development that not only adds but actually overrides settings in the base configuration. The configuration in this file is similar to the Compose file in the previous lesson’s demo with the development specific configuration removed. One change is that the image has been given a registry URL. This allows you to later use docker-compose push to push the production image to a corporate registry, for example.</p>
<p>Looking at the development override Compose file now. This is the development specific configuration extracted from the single configuration in the previous lesson. When you tell Compose to use the base configuration plus this configuration as an override, it effectively reproduces the single development environment Compose file in the previous lesson. We’ll actually see how they combine with the docker-compose config command in awhile.</p>
<p>And over to the final file we’ll look at, the production override Compose file. I’ve use the name prod but the image could and probably should be used for automated testing in a continuous integration system and&#x2F;or staging before going into production. First, note the build configuration is set to use the prod dockerfile. We can also see several of the production considerations manifested in this override file. There is no volume for the app, both services are configured with the always restart policy, no specific host port is set to avoid port conflicts, and a production environment variable is set to configure the application for production. The last override is that the database is set to use a named volume to persist its data. In development it was considered optional, but we definitely want a volume in the production environment. Now let’s take a look at how to use multiple Compose files on the command-line.</p>
<p>I’ll focus on using the config command to show the effective configurations when you specify override files. I’ll start by validating the configuration for the development environment. The command just adds an extra -f for the development override file. The output shows the combined configuration that Docker Compose will use. The output of config is more explicit than what was in the Compose files, in terms of using absolute paths and not having implicit null values. But we can clearly see that both configuration’s options are there. For example, the base configuration’s image and the development override’s source code volume. I’ll clear that and have one more go at it, this time specifying the production override file. And here we can see the effective configuration for the production environment. For example, the always restart policy on both services.</p>
<p>Closing<br>In this Lesson, you saw how multiple Compose files and Docker Compose’s override feature makes it easy to manage your multi-container applications in multiple environments. We also discussed some considerations for when one of those environments is production. When you are ready, continue on to the next <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/summary-6/">lesson</a> where we’ll wrap up the course.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Congratulations! You made it to the end of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/course-introduction-18/">course</a>. I hope you enjoyed the course and learned a lot along the way. Let’s talk a walk down memory lane together.</p>
<p>Course Review<br>We began the course by introducing the problems that Compose aims to solve. Namely, simplifying the process of managing multi-container applications in Docker. Then we dove in to study the anatomy of Compose files. We covered a relatively small but powerful subset of YAML that I said was enough to understand all the examples in the course. Looking back, it was enough wasn’t it. I think you’ll find that it’s enough to understand almost any Compose file you come across. The YAML mapping used to declare multi-container applications in a Compose file has four top-level keys: version, services, volumes, and networks. We went into the details of each and drew upon experience with Docker commands to make it easier to write our own Compose files. We then covered the other part of Compose, the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/docker-compose-cli-1/">Compose CLI</a>. In addition to many familiar commands that are generalized to work with multi-container applications, Compose introduces two new commands: up and down. We then demonstrated how to use Compose to manage WordPress, a popular content management system. The lessons learned applied to managing applications made up of pre-built images. We then looked at using Compose in development scenarios when you need to build the image yourself and still be able to modify the source code. This is possible with the build configuration that works with plain old Dockerfiles and strategically mounting volumes. Lastly, we saw how Compose can be used to manage applications that target multiple environments, such as development and production. Compose override files make it possible to do without duplicating common configuration shared between environments.</p>
<p>Learning Outcomes<br>By taking this course, you have achieved the following learning outcomes:<br>• Understand the anatomy of Docker Compose files<br>• Configure your application using Docker Compose files<br>• Use the Docker Compose CLI to manage the entire lifecycle of applications<br>• Build your own images from source code with Docker Compose<br>• Be able to extend Docker Compose files to adapt to multiple environments</p>
<p>Learning More<br>There are a few places I’d recommend for learning more. Try out some of the labs, quizzes or other courses on Cloud Academy. There is content on Compose as well as Docker swarm mode which lets you run multi-container applications on a cluster of computers instead of a single host.<br><a target="_blank" rel="noopener" href="https://cloudacademy.com/">https://cloudacademy.com</a></p>
<p>The Docker Compose docs are a great place to learn all the intricacies of Compose and to stay on top of release changes.<br><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/">https://docs.docker.com/compose/</a></p>
<p>The Docker Compose GitHub repository is also a good place to learn about what’s coming next for Compose and learn from discussions around reported issues. Of course, you can file your own issues and even contribute to the code base.<br><a target="_blank" rel="noopener" href="https://github.com/docker/compose">https://github.com/docker/compose</a></p>
<p>The broader GitHub community can also be a great place to learn more about using Compose. There are around a quarter of a million Compose files using the default docker-compose.yml or .yaml name on GitHub.<br><a target="_blank" rel="noopener" href="https://github.com/search?q=filename:docker-compose.yml+filename:docker-compose.yaml&amp;type=Code">https://github.com/search?q=filename:docker-compose.yml+filename:docker-compose.yaml&amp;type=Code</a></p>
<p>Feedback<br>I’m happy to hear from you. I make content for you. If you have any feedback, please get in touch with me by leaving a comment on the Comments tab below the video, by emailing <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, or by connecting with me on Twitter where my handle is @LoganRakai.</p>
<p>Thank you<br>That’s all for this course on managing applications with Docker Compose. I want to end by thanking you for going through the course with me. It’s been a blast! Now go on and put what you’ve learned here into action. Until next time, I’m Logan Rakai with Cloud Academy.</p>
<h1 id="1Course-Introduction"><a href="#1Course-Introduction" class="headerlink" title="1Course Introduction"></a>1<strong>Course Introduction</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/docker-compose-training">Course Github Repo</a></p>
<h1 id="8Summary"><a href="#8Summary" class="headerlink" title="8Summary"></a>8<strong>Summary</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/">Docker Compose docs</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/docker/compose">Docker Compose GitHub</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/search?q=filename:docker-compose.yml+filename:docker-compose.yaml&type=Code">Docker Compose YAML files</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Docker-Swarm-Playground-7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Docker-Swarm-Playground-7/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Docker-Swarm-Playground-7</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:36" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:36-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:25:54" itemprop="dateModified" datetime="2022-11-20T22:25:54-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Docker-Swarm-Playground-7/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Docker-Swarm-Playground-7/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Manage-Your-Cluster-Using-Docker-Swarm-Mode-6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Manage-Your-Cluster-Using-Docker-Swarm-Mode-6/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Manage-Your-Cluster-Using-Docker-Swarm-Mode-6</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:34" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:34-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:25:20" itemprop="dateModified" datetime="2022-11-20T22:25:20-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Manage-Your-Cluster-Using-Docker-Swarm-Mode-6/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Manage-Your-Cluster-Using-Docker-Swarm-Mode-6/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Container-Orchestration-With-Docker-Swarm-Mode-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Container-Orchestration-With-Docker-Swarm-Mode-5/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Container-Orchestration-With-Docker-Swarm-Mode-5</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:33" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:33-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 03:02:06" itemprop="dateModified" datetime="2022-11-21T03:02:06-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Container-Orchestration-With-Docker-Swarm-Mode-5/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Container-Orchestration-With-Docker-Swarm-Mode-5/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Welcome to Container Orchestration with Docker Swarm Mode.</p>
<p>About Me<br>I’m Logan Rakai and I’ll be your instructor for this Course. I’m a content researcher and developer here at Cloud Academy. I’ve been thinking a lot about how to maximize the return on your time invested in this course. I’m confident that you’ll be confident in your ability to orchestrate containers with Docker Swarm mode after completing the course. I have over ten years of experience in software research and development including five years in the cloud. I’m an AWS Certified DevOps Engineer Professional and a Microsoft Certified Solutions Expert: Cloud Platform and Infrastructure. You can connect with me on LinkedIn or on Twitter.</p>
<p>Who this course is for<br>This course is for anyone that is interested in orchestrating distributed systems at any scale.<br>DevOps Engineers<br>Site Reliability Engineers<br>Cloud Engineers<br>Software Engineers</p>
<p>Prerequisites<br>In order to get the most out of this course, you should have experience with Docker. You should have a solid understanding of images, networks, volumes, and have experience using Docker compose for managing multi-container applications. If you need to brush up on any of those topics, Cloud Academy has some great courses for that. “Introduction to Docker” by Ben Lambert covers fundamental Docker concepts and “Managing Applications with Docker Compose” by yours truly for working with multi-container applications using Docker Compose.</p>
<p>You can follow along with the course examples, and I’d encourage you to. You will need Docker version 1.13 or greater installed. I’ll be using a Mac with Docker for Mac version 17.12 installed but you can also follow along in Linux. You should have VirtualBox installed as well. Swarm mode in Windows has some additional limitations mainly around network encryption. I’ll mention the limitations when we cover the relevant topic. Almost everything we discuss will apply to Windows environments but I’ll be using Linux containers in the demos.<br>I’ve put resources that I use for the demos on GitHub. A clickable link is available at the bottom of the transcript for this lesson. Most of the work will happen at the command-line although we will work with some files near the end of the course. I’ll be using Visual Studio Code for working with the files but you could use whatever you are comfortable with.</p>
<p>Learning Objectives<br>After completing this course, you will be able to:<br>“ Describe what Docker swarm mode can accomplish<br>“ Explain the architecture of a swarm mode cluster<br>“ Use the Docker CLI to manage nodes in a swarm mode cluster<br>“ Use the Docker CLI to manage services in a swarm mode cluster<br>“ Deploy multi-service applications to a swarm using stacks</p>
<p>Feedback<br>I’m happy to hear from you. I make content for you and I want it to be as good as it can be. If you have any feedback, please get in touch with me by leaving a comment on the Comments tab below the video, by emailing <a href="mailto:&#115;&#x75;&#x70;&#112;&#x6f;&#114;&#x74;&#x40;&#99;&#108;&#x6f;&#117;&#x64;&#97;&#99;&#97;&#100;&#x65;&#x6d;&#x79;&#x2e;&#99;&#111;&#x6d;">&#115;&#x75;&#x70;&#112;&#x6f;&#114;&#x74;&#x40;&#99;&#108;&#x6f;&#117;&#x64;&#97;&#99;&#97;&#100;&#x65;&#x6d;&#x79;&#x2e;&#99;&#111;&#x6d;</a>, or by connecting with me on Twitter where my handle is @LoganRakai.</p>
<p><a target="_blank" rel="noopener" href="https://www.virtualbox.org/wiki/Downloads">VirtualBox</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/docker-swarm-mode-training">Course resources on GitHub</a></p>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>Welcome to this overview lesson on Docker swarm mode. We’ll get a conceptual understanding of swarm mode in this lesson before understanding its architecture and diving into the details and demos in following lessons.</p>
<p>Agenda<br>We’ll start the lesson by getting an understanding of why we need swarm mode. After that, we’ll highlight some features of Docker swarm mode to understand what swarm mode can do for you. Next, we’ll learn about the main concepts of Docker swarm mode. Lastly, I’ll touch on the universal control plane which is Docker’s enterprise product built on top of swarm mode.</p>
<p>Why Swarm?<br>Docker has made great strides in advancing development and operational agility, portability, and cost savings by leveraging containers. You can see a lot of benefits even when you use a single Docker host. But when container applications reach a certain level of complexity or scale you need to make use of several machines. Container orchestration products and tools allow you to manage multiple container hosts in concert. Docker swarm mode is one such tool.</p>
<p>Swarm Mode<br>Swarm mode is a feature built into the Docker Engine providing native container orchestration in Docker. Swarm mode is something you need to enable and when you do, the Docker Engine is said to be running in swarm mode. With swarm mode you can control a cluster of machines in a way that is similar to running and about as easy as running a single Docker Engine. Of course there are some differences and we’ll see them in this course.</p>
<p>Calling swarm mode a container orchestration feature doesn’t quite do it justice. It encompasses cluster management, container orchestration, and more. Some of the main features of swarm mode include:<br>“ Integrated cluster management within the Docker Engine without any additional software<br>“ A declarative service model that allows you to declare what you want and Docker can create it for you. There is no need for you to specify the sequence of commands to realize what you want.<br>“ Swarm mode is able to monitor the cluster state and reconcile any differences between the desired state and the actual state. (Desired state reconciliation)<br>“ Swarm mode uses certificates and cryptographic tokens to secure the cluster<br>“ As well as features you’d expect in a container orchestration offering such as service scaling, multi-host networking, resource-aware scheduling, load balancing, rolling updates, restart policies, and more.</p>
<p>Name disambiguation<br>Docker actually has two cluster management solutions. Both are open source and live on GitHub. Surprisingly, they are both called swarm. Docker Swarm, with a capital S, was the first container orchestration project by Docker. It uses the Docker API to turn a pool of Docker hosts into a single, virtual Docker host using a proxy system. To reduce confusion, Docker Swarm is now referred to as Docker Swarm standalone in documentation.</p>
<p>Although Docker Swarm standalone project is still maintained, the newer container orchestration tool is called Swarmkit. It is what is built into the Docker Engine since Docker version 1.12. You might see swarmkit mentioned from time to time, but this is the most commonly referred to as swarm mode. Docker recommends Swarm mode unless you have a specific reason to use Swarm standalone.</p>
<p>So now you know that there are two swarms, Swarm standalone and swarm mode. It’s useful to be aware of the distinction. You might search for Docker swarm online and stumble upon something related to Swarm standalone when you wanted swarm mode. To avoid any confusion, this course deals exclusively with swarm mode. In the remainder of the course, if I refer to swarm, I’m referring to Docker running in swarm mode. In practice, it’s pretty common to drop mode from the name although it can potentially lead to misunderstandings. In the remainder of the lesson, we’ll cover the architecture of swarm mode.</p>
<p>Swarm Mode Concepts<br>Before going too far, we’ll cover some of the main concepts and swarm mode terminology.</p>
<p>A swarm consists of one or more Docker Engines running in swarm mode. Each instance of the Docker Engine in the swarm is referred to as a node. It is possible to run multiple nodes on a single machine. For example, by using virtual machines. In production environments, you should use multiple machines to ensure availability of the swarm if a machine goes down.</p>
<p>Nodes can participate in a swarm by taking on specific roles: managers and workers. Every swarm requires at least one manager. Managers have several responsibilities, but we’ll start simple and consider one main responsibility. Managers accept specifications from users and drive the actual state of the swarm to the specified desired state. They do so by delegating units of work to workers in the swarm. Workers are primarily responsible for running the delegated units of work. Workers also run an agent which reports back to managers on the status of their work. A node can be either a manager, or a worker.</p>
<p>The specifications that users submit to managers are called services. This is the same concept as a service in Docker Compose. The service configuration declares its desired state, which includes the networks and volumes it uses, the number of replicas, resource constraints, and other details. A manager will ensure the actual state of the swarm matches the service configuration. if it is possible to realize in the swarm. There may not be enough available resources in the swarm which would prevent the desired state from being achieved. Docker will also make the changes necessary to reconcile the actual state with the desired state if you update a service.</p>
<p>There are two kinds of services: replicated and global. You specify the number of replicas for a replicated service based on the scale you desire. A global service allocates one unit of work for each node in the swarm. Global services can be useful for monitoring services, for example.</p>
<p>The units of work delegated by managers to realize a service configuration are referred to as tasks. The tasks correspond to running containers that are replicas of the service. Managers schedule the tasks across nodes in the swarm. If a node leaves the swarm, the tasks that the node was running will be scheduled onto the remaining nodes in the swarm.</p>
<p>By default, manager nodes also run tasks like workers. You can configure managers to participate exclusively in managing the cluster and that is probably a good idea in production. Allowing managers to run tasks by default enables easy to setup and functional single node swarms.</p>
<p>Universal Control Plane<br>The last topic I want to cover in giving an overview of swarm mode is the Universal Control Plane (UCP). UCP is only relevant for the enterprise edition of Docker so I will only briefly touch on it.</p>
<p>Working with swarm mode is similar to working with Docker. You interact with it through the Docker CLI. That is great, but sometimes it can be nice to have a web interface to manage and visualize the cluster and containers. UCP is Docker’s enterprise offering that is built on top of swarm mode to provide a web interface for cluster management and role-based access control. Because UCP is built on swarm, what you learn in this course applies to UCP as well.</p>
<p>Closing<br>All right, now we have a basic understanding of swarm mode. We will take closer look at how swarm mode works by understanding main components of its architecture in the next group of lessons.</p>
<h1 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h1><p>Thanks for joining me for this lesson on Docker swarm mode architecture. You heard about the great benefits swarm mode provides in the previous lesson. In these architecture lessons, we’ll understand more about the parts of swarm mode that enable it to accomplish all those great benefits, starting with networking. This lesson and the following architecture lessons build the foundations for using swarm mode. I promise we’ll be seeing swarm mode in action in the demos of the next lesson group in the course.</p>
<p>Agenda<br>This lesson will cover everything that is unique to swarm mode and networking:<br>“ (Overlay networks) Starting with a Docker network type exclusive to swarm mode, the overlay network.<br>“ (Service discovery) After that, we’ll discuss how services in a swarm can be discovered across multiple host swarm networks.<br>“ (Load balancing) On a related note, we’ll see how load is balanced across all the replicas of a service.<br>“ (External access) Then the mechanisms for accessing the swarm services from outside the swarm will be explored.</p>
<p>Networking<br>The networking requirements in a swarm are much more complex than using a single Docker host. Services need to communicate with one another and the replicas of the service can be spread across multiple nodes. Fortunately, Docker includes a network driver that makes multi-host networking reliable, secure, and a breeze to set up.</p>
<p>Overlay Networks<br>The driver I’m referring to is the overlay network driver. With the overlay driver a multi-host networking in a swarm is natively supported. There is no need to perform any external configuration. You can attach a service to one or more overlay networks, in the same way you would attach a container to one or more user-defined networks when not running in swarm mode.<br>Overlay networks only apply to swarm services and can’t be connected to by containers that aren’t part of a swarm service. Managers automatically extend overlay networks to nodes that run tasks requiring access to a given overlay network.</p>
<p>Network isolation and firewalls<br>It’s a good time to review Docker network isolation and firewall rules. These rules apply to overlay networks just as they do for bridge networks.<br>Containers within a Docker network are permitted access on all ports of containers in the same network.<br>Access is denied between containers that don’t share a common network.<br>Traffic originating inside of a Docker network and not destined for a Docker host is permitted. For example, access to the internet. However, any network infrastructure outside of Docker may still deny the traffic.<br>Ingress traffic, or traffic coming into a Docker network, is denied by default. Ports must be published in order to grant access form outside of Docker.</p>
<p>Service Discovery<br>With services distributed across multiple nodes, a service discovery mechanism is required in order to connect to the nodes running tasks for a service. Swarm mode has an integrated service discovery. It is based upon the domain name system (DNS). The DNS is internal to Docker and implemented in the Docker Engine. It is used for resolving names to IP addresses.</p>
<p>Actually, the same service discovery system is used when not running in swarm mode. Service discovery in Docker is scoped to a network. When you are in swarm mode, the network can be an overlay spanning multiple hosts. But the same internal DNS system is used. All nodes in a network store corresponding DNS records for the network. Only service replicas in the network can resolve other services and replicas in the network by name.</p>
<p>Internal Load balancing<br>There are some unique service discovery considerations for Swarm mode. Each individual task is discoverable with a name to IP mapping in the internal DNS. But because services can be replicated across multiple nodes, which IP address should a service name request resolve to? Docker assigns a service a single virtual IP (VIP) address, by default. Requests for the virtual IP address are automatically load balanced across all healthy tasks spread across the overlay network. By using a virtual IP, Docker can manage the load balancing allowing clients to interact with a single IP address without considering load balancing. It also makes the service more resilient since the service can scale and tasks can change the nodes that they are scheduled on but clients are sheltered from the changes.</p>
<p>Internal load balancing example<br>To illustrate how service discover and load balancing work in swarm mode, consider two services deployed in a swarm service A and service B. Service A has a single replica while service B has two replicas. When service A makes a request for service B by name, the virtual IP of service B is resolved by the DNS server. Service A uses the virtual IP to make a request for service B. Using support for ip virtual servers (IPVS) the request for the virtual IP address is routed to one of the two nodes running service B tasks.</p>
<p>DNS Round Robin<br>Besides the default virtual IP, you can configure load balancing using DNS round robin (DNS RR). You can configure the load balancing on a per service basis. When DNS round robin is used, the Docker Engine’s DNS server resolves a service name to individual task IP addresses by cycling through the list of IP addresses of node’s running a task in the service. If you need more control over load balancing than a virtual IP can give you, DNS round robin should be used for integrating your own external load balancer.</p>
<p>External Access<br>We’ve covered access to services within a Docker network, but what about accessing a service from the outside? With a single Docker host, you would publish a container port on the host to permit access to a container. Similar functionality is still available in swarm. But there are actually two modes for publishing ports in swarm.</p>
<p>Host mode<br>The first is the same as you would expect when publishing a port when not running in swarm mode. The container port is published on the host that is running the task for a service. This mode is referred to as host mode service publishing. You need to be careful with specifying a host port in host mode. If you have more tasks than available hosts, tasks will fail to run because the host port can only be bound to one task. You can omit a host port to allow Docker to assign an available port number in the default port range of 30000-32767. However, this can make it more difficult to work. Also, there isn’t load balancing unless you configure it externally. Obviously, that is useful when you don’t want load balancing, but what about when you do?</p>
<p>Ingress mode<br>Because services can be replicated and tasks can be rescheduled onto different nodes as the state of the swarm changes, it is useful to have the option to load balance a published port across all tasks of a service. This is referred to as ingress mode service publishing. For convenience, all nodes in the swarm publish the port. This is different from host mode where a port is only published if the node is running a task for the service. In ingress mode, requests are round robin load balanced across the healthy instances of the service’s tasks regardless of the node that receives the request.</p>
<p>Ingress mode is the default service publishing mode. It’s ideal when you have multiple replicas of a service and need to load balance between them. Host mode publishing is useful when you have an external service discovery service and potentially for global services where one task for a service runs on each node. For example, a global service that monitors each node’s health shouldn’t be load balanced since you want to get the status of a specific node.</p>
<p>Routing Mesh<br>At this point, you might be wondering how ingress mode publishing work. The magic happens in what is called the routing mesh. The routing mesh combines two of the swarm components that we discussed earlier: an overlay network, and a service virtual IP.</p>
<p>When you initialize a swarm, the manager creates an overlay network named ingress. Every node that joins the swarm is in the ingress network. The sole purpose of the ingress network is to transport traffic from external clients that is destined to published service ports to the service inside the swarm.</p>
<p>When a node receives an external request on the ingress network the node resolves the service name to a virtual IP address. This process is carried out using the same internal DNS server as we discussed in the internal load balancing. The IP virtual server then load balances the request to a service replica over the ingress network.</p>
<p>Because every node is in the ingress network, every node can resolve the external requests can handle the external requests. The nodes need to have a couple of ports open for all of this magic to work:<br>o Port 7946 for both TCP and UDP protocols to enable container network discovery.<br>o Port 4789 for the UDP protocol to enable the container ingress network.</p>
<p>It’s worth mentioning that you could add an external load balancer on top of the load balancing provided by the routing mesh. For example, if you have nodes running in the cloud, you can have the nodes in a private subnet so they aren’t directly accessible from the internet. You could provision a cloud load balancer to handle requests from the internet and load balance them across nodes in the swarm. The swarm nodes then load balance again across the nodes running tasks for the service.</p>
<p>As a final note on the routing mesh, if you are planning to use the routing mesh on Windows, you need to be running version 17.09 or greater.</p>
<p>docker_gwbridge<br>Besides the ingress network, Docker also creates a second network when running in swarm mode called docker_gwbridge. The docker_gwbridge is a virtual bridge that connects the overlay networks (including the ingress network) to an individual Docker daemon’s physical network. This interface provides default gateway functionality for all containers attached to the network. Docker creates it automatically when you initialize a swarm or join a Docker host to a swarm, but it is not a Docker device. It exists in the kernel of the Docker host. You can see it if you list the network interfaces on your host.</p>
<p>Recap<br>There was quite a few topics related to networking in swarm mode. Let’s recap the main points:<br>“ Swarm mode includes a new type of Docker network, the overlay network. Overlay networks make it easy to use multi-host networking in a swarm.<br>“ The same internal DNS service discovery mechanism used when not running in swarm mode is used in swarm mode. The internal DNS naturally extends to multi-host networks.<br>“ The services in a swarm can be load balanced by using a virtual IP address or by DNS round robin.<br>“ External access to the swarm is made possible by publishing ports. There are two modes for publishing in swarm mode: host and ingress.<br>o In host mode each service replica publishes it’s container port on the host. No load balancing is used.<br>o In ingress mode, every node in the swarm publishes the port and requests are load balanced across all the replicas of a service. Any node can handle requests for the service even if the node doesn’t have a replica of the service itself.<br>“ Ingress mode is made possible by the swarm routing mesh which uses two default swarm networks: the ingress overlay network and docker_gwbridge network</p>
<p>Closing<br>In the next lesson, we’ll look into swarm mode container orchestration features including rolling updates and scheduling constraints. When you’re ready continue on to the next lesson to see how swarm can orchestrate containers.</p>
<h1 id="Orchestration"><a href="#Orchestration" class="headerlink" title="Orchestration"></a>Orchestration</h1><p>Swarm mode is made to be familiar to single host Docker users. When you deploy a service, it is similar to running a container. You can specify an image, volumes, networks, published ports, After all, service tasks ultimately run containers. But there are container orchestration features of swarm mode that are unique to running services in a swarm.</p>
<p>Agenda<br>We’ll look at the following orchestration features of swarm mode:<br>“ (Service placement) Which nodes service tasks are placed on<br>“ (Update behavior) how service updates are rolled out, and<br>“ (Rollback behavior) how services can be rolled back to a previous version.</p>
<p>Service placement<br>As we’ve discussed, services can declare a set number of replicas as a replicated service or can be started on every worker node in a cluster as a global service. For replicated services, decisions need to be made by swarm managers for where service tasks will be scheduled, or where the service will be placed. A replicated service’s tasks will be spread across nodes by default. That is to promote high availability in case a node fails. But there are three ways that you can influence where a service is placed:<br>\1. CPU and Memory reservations<br>\2. Placement constraints<br>\3. Placement preferences<br>You can specify each at service creation time. Global services can also be restricted to a subset of nodes with these conditions. Although a node will never have more than one task for a global service. Let’s take a closer look at each.</p>
<p>CPU and Memory reservations<br>Similar to running individual containers, you can declare CPU and memory reservations for services. Each service task can only be scheduled on a node that has enough available CPU and memory to meet the given reservations. Any tasks that remain stay in a pending state until a node with sufficient resources becomes available. Global services will only run on nodes that meet a given resource reservation.</p>
<p>Setting sufficient memory reservations for services is important when there isn’t an abundance of CPU and memory available for the applications you are running. If services attempt to use more memory than is available, the container or Docker daemon could get killed by the out of memory or OOM killer.</p>
<p>Placement constraints<br>Placement constraints allow you to restrict the placement of tasks by providing equality and inequality conditions. The conditions compare node attributes to a string value. There are a few built-in attributes for each node<br>\1. node.id matches the ID of a node<br>\2. node.hostname matches a node’s hostname<br>\3. node.role matches a node’s role, either manager or worker</p>
<p>You can also define your own labels. You can configure labels on a Docker engine or on a node. Engine labels are usually used to indicate things like operating system, system architecture, available drivers. An example is engine.labels.operatingsystem and values could be Ubuntu 14.04 or Windows Server 2016. Node labels are added by Swarm administrators for operational purposes. Node labels can indicate they type of application a node is intended to run, the datacenter location a node is in, the server rack a node is in, et cetera. An example is node.labels.datacenter and values could be north, south, east, or west.</p>
<p>When you provide multiple placement constraints for a service, all constraints must be satisfied by a node in order to be scheduled a service task. If resource reservations are also provided, all constraints and resource reservations must be met. This is true for replicated and global services.</p>
<p>Placement Preference<br>Placement preference is not required as was the case for resource reservations and placement constraints. Instead, placement preferences influence how tasks are distributed across appropriate nodes. Currently the only distribution option is spread which will evenly spread tasks.<br>Labels are again used as the attribute for spreading tasks. For example, assume every node in a swarm has a datacenter label with either east or west as the value. Using the datacenter label and the spread placement preference, half of the tasks will be scheduled on east datacenter nodes and the other half on west datacenter nodes.</p>
<p>Multiple placement preferences can be specified. In this case a hierarchy of preferences is created. For example, if the first preference is datacenter and the second Is server-rack, tasks will be evenly spread across nodes in each datacenter, and within each datacenter tasks are spread evenly across racks.</p>
<p>Nodes that are missing a placement preference label are included in the spread and receive tasks in proportion equal to all other label values. They are treated as the group having the null value for the label. Placement preferences are ignored by global services.</p>
<p>That’s all that there is to influencing service placement in swarm.</p>
<p>Update Behavior<br>You can also configure the way that swarm applies updates to services. Swarm supports rolling updates where a fixed number of replicas are updated at a time until all service replicas have been updated.</p>
<p>You can configure several update parameters:<br>\1. Update parallelism, which sets the number of tasks the scheduler updates at a time<br>\2. Update delay, which sets the amount of time between updating sets of tasks, and<br>\3. Update failure action, which can be set to pause, continue or automatically rollback if an update fails. The default is to pause.<br>These are the three main settings. There are also settings to configure what qualifies as failure. You can set a ratio for the number of failed task updates to tolerate before failing a service update, and set the frequency for monitoring for a failure.</p>
<p>These parameters give you some flexibility in how aggressively or conservatively you roll out an update to the swarm.</p>
<p>Rolling Back Updates<br>Docker swarm keeps track of the previous configuration for services. This allows you to rollback manually at any time or automatically when an update fails, as we discussed.</p>
<p>The same options available for configuring update behavior are available separately for configuring rollbacks. For example, rollback parallelism sets how many nodes to roll back at a time.</p>
<p>Recap<br>In this lesson, we saw how you can influence the nodes that swarm schedules services on by using resource reservations, placement constraints, and placement preferences. Resource reservations and placement constraints must be satisfied, while placement preferences won’t prevent a task from being scheduled. We also discussed how rolling updates and rollbacks can be configured in Swarm. Updates and rollbacks share the same available configuration options.</p>
<p>Closing<br>In the next lesson, we’ll see how swarm mode keeps a consistent view of the swarm. An important topic for any distributed system. When you are ready, continue on to the next lesson to learn about swarm mode consistency.</p>
<h1 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a>Consistency</h1><p>Consistency is an important consideration for any distributed system. In this lesson, we’ll look at the consistency model of swarm mode and how it can impact how you operate a swarm.</p>
<p>Agenda<br>To kick things off we’ll discuss:<br>“ (Consistency) the consistency problem and<br>“ (Raft) how swarm mode goes about solving it, in particular the Raft Consensus algorithm.<br>“ (Tradeoffs) We’ll cover just what you need to know of Raft to understand key tradeoffs that you should consider when deciding on the composition of your swarm.<br>“ (Raft Logs) Lastly, we’ll talk about the raft logs where the cluster state is stored.</p>
<p>Consistency<br>We have seen that swarm mode can include several manager and worker nodes in a swarm. This provides fault tolerance if a node were to go down and ensures services are highly available. But with multiple managers, how does swarm make decisions regarding the state of the cluster? Do nodes in the swarm share a consistent view of the cluster or could one node have a different view than the other? And if so, for how long? These questions all touch on the issue of consistency.</p>
<p>In swarm mode, managers all share a consistent internal state of the entire swarm. This avoids any potential issues that could arise if managers were allowed to eventually converge to a shared state. Workers, on the other hand, do not share a view of the entire swarm. That is exclusively a manager responsibility.</p>
<p>The managers maintain a consistent view of the state of the cluster by using a consensus algorithm. There are several consensus algorithms to choose from and the implementation details are outside the scope of this course. But the consensus algorithm has an impact on how the swarm operates. We’ll look at the basics of swarm modes consensus algorithm so we can understand the implications in operating a swarm.</p>
<p>Raft Consensus<br>The consensus algorithm used by managers to maintain a consistent view of the state of the cluster is called Raft. Raft achieves consensus by electing one manager as the leader. The elected leader makes all of the decisions for changing the state of the cluster to bring it to the desired state. For example, the leader accepts new service requests and service updates and also decides how to schedule tasks.</p>
<p>In order to maintain a consistent view across the managers, the decisions aren’t acted upon until a majority of managers agree on the proposed changes to the cluster. A manager “agrees” simply by receiving a proposed change and acknowledging they received it. When the leader is certain a majority of managers have received the proposed change, the change can be implemented. In this context, the majority of managers are referred to as a quorum.</p>
<p>The reason why a quorum is enough to proceed is because Raft limits how many managers failures it can tolerate. If you have N managers in a swarm, Raft allows for (N-1)&#x2F;2 failures. In the case of a three manager swarm, that means 3 minus 1 divided by two is one, so one manager can fail and the swarm can continue to operate as usual. If two managers were to fail, the cluster state would freeze until a quorum of managers again became available. In the absence of a quorum, currently running services will continue to run but no new scheduling decisions take place.</p>
<p>Regarding leader elections, when a swarm is initialized the first manager is automatically the leader. If the currently elected leader fails or voluntarily steps down, say to perform system updates, an election between remaining manager nodes takes place. Until a newly elected leader is chosen, the cluster state is frozen.</p>
<p>Manager Tradeoffs<br>After that overview of Raft consensus, you might be tempted to add a lot of managers to your swarm. The more managers, the more failures your swarm can tolerate and remain fully operational. Although, that is true, the more managers that are in the swarm also increases the amount of managerial traffic required for maintaining a consistent view of the cluster and the amount of time it takes to achieve reach consensus with every state change. Although increasing managers does increase fault-tolerance it generally decreases performance and scalability.</p>
<p>There are some general rules for setting the number of managers:<br>You should usually have an odd number of managers. Having an even number of managers doesn’t improve the fault tolerance compared to having one less manager and increases communication overhead.<br>A single manager swarm is acceptable for development and test swarms. Because a single manager swarm can’t tolerate any failures, it is not something you should use in production.<br>A three manager swarm can tolerate one failure, while a five manager swarm can tolerate two.<br>Docker recommends a maximum of seven managers which can tolerate three manager failures. Above seven has too much of an impact on performance to be beneficial.</p>
<p>However many managers you settle on, you will want to distribute them across availability zones to maintain a fully operational swarm in the event of a datacenter outage. Docker recommends distributing across at least three availability zones in production.</p>
<p>Working Manager<br>There is another tradeoff when considering managers in a swarm. Have you heard the bad joke that goes “Don’t stand around doing nothing. People will think you’re the boss.” In swarm mode, you need to consider whether or not you let the boss, or the managers, do work. By default managers perform worker responsibilities, namely running tasks. But that has more to do with enabling single node swarms than anything.</p>
<p>Because managers participate in the Raft consensus process, it can be detrimental to the performance of the swarm if managers are overly utilized. You can use conservative resource reservations to make sure that managers won’t become starved for resources. To be on the safe side, you can also prevent any work from being scheduled on manager nodes by draining them. Draining essentially removes any tasks currently on a node and preventing new tasks from being scheduled to it.</p>
<p>Worker Node Tradeoffs?<br>You might be wondering if there are any tradeoffs to consider when adding worker nodes to a swarm. There really isn’t much to worry about in the case of adding more worker nodes. More workers give you more capacity for running services and improves service fault tolerance. More workers don’t affect the manager’s raft consensus process so the swarm performance isn’t harmed.</p>
<p>Workers actually do participate in a consensus process. To exchange overlay network information nodes participate in a weakly-consistent, highly scalable gossip protocol called SWIM. The details are outside the scope of this course and the performance implications are negligible. The protocol is an example of an eventually consistent model where the network state is allowed to differ between nodes but eventually they converge on a consistent view.</p>
<p>Raft logs<br>The last topic we’ll discuss in this lesson is Raft logs. If you arrived at this lesson from a search for log rafts, I’m afraid you’ll need to continue your search. The logs we’re talking about are where the leader manager records the Raft consensus state changes, such as creating a new service or adding a new worker. These logs are what get shared with other managers to establish a quorum.</p>
<p>The Raft logs are persisted to disk. The logs are stored in the raft subdirectory of your Docker swarm data directory. This is &#x2F;var&#x2F;lib&#x2F;docker&#x2F;swarm on Linux by default. As part of a disaster recovery strategy, you can back up a swarm cluster by backing up the entire swarm directory which includes certificates and other files in addition to the raft change logs in the raft subdirectory. You can restore a new swarm from a backup by replacing the directory swarm directory with the backed-up copy.</p>
<p>Recap<br>That’s everything for this lesson. We started by understanding how swarm mode solves consistency challenges by electing a leader manager and ensuring a majority of managers acknowledge swarm changes. This strategy comes from the Raft consensus algorithm. We understood the tradeoffs between fault tolerance and performance when choosing the number of managers in a swarm, as well as whether or not managers should do work. We finished by discussing the raft logs which are persisted on disk and record all the changes the leader makes to a swarm.</p>
<p>Closing<br>In the next lesson, we will cover the security measures included in swarm mode. Continue on to the next lesson when you are ready to learn about security in Docker swarm mode.</p>
<h1 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h1><p>Docker takes security seriously. All the security features that you can use when not running in swarm mode can be used in swarm mode. This includes using trusted images, encrypted communication with docker engines, and kernel security features leveraged by Docker. This lesson covers the security provisions in Docker swarm mode.</p>
<p>Agenda<br>We’ll begin by covering the<br>“ (Cluster Management) cluster management aspects of swarm security<br>“ (Data Plane) Next, we will discuss security of data communicated between services<br>“ (Secrets) After that, we’ll see how swarm secrets are kept secure.<br>“ (Locking a Swarm) Lastly, the locking functionality of a swarm will be described.</p>
<p>Cluster Management<br>Docker swarm mode uses public key infrastructure (PKI) to secure swarm communication and state. Swarm nodes encrypt all control plane communication using mutual transport level security (TLS). When you initialize a swarm, Docker assigns the node that executed the command as a manager. The manager automatically creates several resources for security:<br>“ A root Certificate Authority (CA): This plays the standard role of a CA in PKI by being a trusted entity that issues certificates verifying the identity of certificate holders.<br>“ A key pair: This is the public and private key used for secure communication between nodes in the swarm.<br>“ A worker token: This token is used to by nodes to join the swarm as a worker node. The token is a digest of the root CA and a secret.<br>“ A manager token: This is similar to the worker token but used to join nodes as managers in the swarm.<br>Whenever a new node joins the swarm, the manager issues a new certificate identifying the node. The node uses the certificate for communicating in the swarm. New manager nodes also get a copy of the root CA certificate so that they can take over leadership in the event of an election.</p>
<p>You can use an alternate CA instead of allowing Docker to automatically handle their creation for you. The CA can also be rotated out whenever your security policies require it. Rotating the CA will automatically rotate the TLS certificates of all swarm nodes in the cluster.</p>
<p>Data Plane<br>As for the data plane, you can enable encryption of overlay networks at the time of creation. Any time traffic leaves a host an IPSec encrypted channel is used to communicate with a destination host where the traffic is decrypted. The swarm leader periodically regenerates and distributes the key used for encrypting IPSec data plane traffic. Overlay network encryption is not supported for Windows as of Docker version 17.12.</p>
<p>Raft Logs&#x2F;Secrets<br>The Raft logs are encrypted at rest on the manger nodes. This protects against intruders that gain access to the raft logs on disk. The encryption is particularly important because swarm secrets are stored in the raft logs. Secrets are a feature of swarm that allows you to securely store secrets that can be used by services. This could include passwords, API keys, or any other information you wouldn’t want to be exposed over the network or in a Dockerfile. In Windows, secrets are supported in version 17.06 an above.</p>
<p>Locking a Swarm<br>A challenge with encrypting the raft logs is that the keys used to encrypt the logs needs to be stored somewhere a manager has access to. By default, the keys are stored on disk along with the raft logs. If an attacker gains access to the raft logs, there is a good chance they could gain access to the keys used to encrypt them. They could then decrypt the logs and expose any secrets therein.</p>
<p>For an extra layer of security, a swarm allows you to take control of the key used for encrypting the logs. This allows you to implement strategies where the key is never persisted to disk. This works with a swarm feature called autolock. When a swarm is autolocked, you must provide the key when starting a docker daemon. For greatly improved security, you have to pay that price of requiring manual intervention when a manager is restarted. You can rotate the key at any point or disable autolock so managers can be restarted without intervention.</p>
<p>Recap<br>In this lesson, we saw the security measures that are in place in out of the box when using swarm mode. This included several security layers with regards to managing a cluster. We also saw how overlay network communication can optionally be encrypted to secure communication between services. Swarm supports sharing secrets and uses encryption to protect the secrets on disk in the manager Raft logs. The keys for decrypting the logs are stored on disk by default, but you can use autolocking to take control of the keys and improve the defense of your swarm.</p>
<p>Closing<br>We have now covered all of the architecture topics related to swarm mode. In the following lesson group, we will get hands-on with a swarm and see how to operate and use a swarm from the command line and by describing applications in stack files.</p>
<h1 id="Setting-Up-a-Swarm"><a href="#Setting-Up-a-Swarm" class="headerlink" title="Setting Up a Swarm"></a>Setting Up a Swarm</h1><p>All right, this lesson and remaining lessons focus more on getting hands-on with Docker swarm mode. You will see a lot of the concept knowledge that you’ve built up in the previous lessons in action. These lessons will focus more on the commands you need to use to accomplish tasks related to swarm mode, starting with setting up a swarm.</p>
<p>We will begin by laying out the options available to you for setting up a swarm mode cluster. After that we’ll show two ways to set up a swarm locally on your machine: as a single node swarm and as a multi-node swarm using virtual machines.</p>
<p>There are several options for creating a swarm mode cluster. You should consider factors such as the workloads you want to deploy on the swarm, the management complexity, and cost when determining which option to choose.</p>
<p>The simplest option is creating a single-node swarm. Recall that swarm mode managers can also perform work by default meaning that you can run swarm workloads with a single node. We will see later in this lesson how easy it is to set up. This may be appropriate in development and test scenarios. With no fault tolerance, it is not something to do in production.</p>
<p>The other options are for multi-node clusters. There are unmanaged options that you put you in charge of maintaining the infrastructure and applying patches, and there are more managed options where you can use the swarm as a service without worrying about hardware or software patches.</p>
<p>For the unmanaged option, you would likely have your own compute cluster or private cloud. You would need to ensure Docker is installed on the bare metal servers or on virtual machines running on top. The network firewall would need to allow traffic on the ports swarm mode requires (TCP port 7946 and UDP ports 7946 and 4789). The Universal Control Plane that is available through Docker Enterprise edition can set up an on-prem swarm using a graphical interface.</p>
<p>Here is a screenshot of the UCP web interface in action showing a three node swarm.</p>
<p>We will setup a multi-node cluster using VMs on a single physical host later in this lesson. You could run VMs in a public cloud and make a swarm out of the VMs. However, there may be a better option if you are going to leverage the public cloud.</p>
<p>For the more managed options, you could use cloud provider templates that allow you to set a few parameters and have swarm created for you. This is true for Microsoft Azure, Amazon Web Services, and IBM Cloud. You can also leverage Docker’s Docker Cloud offering to create swarms on Azure and AWS through the Docker Cloud graphical interface. Each option is explained in Docker’s own documentation.</p>
<p>Now, it’s time to demo setting up some swarms. I’ll first setup a single node swarm and then a multi-node swarm using virtual machines and the help of the docker-machine command.</p>
<p>I’m here at my terminal on my mac. I have Docker for Mac installed<br>$ docker version<br>To see the current status of the Docker daemon’s swarm mode, you can use the docker info command and look for the Swarm key:</p>
<p>$ docker info | grep Swarm<br>The inactive value means the daemon is not running in swarm mode.</p>
<p>Now we’ll see how easy it is to start running in swarm mode. The commands relevant to managing a swarm are under the swarm subcommand of the Docker CLI.</p>
<p>$ docker swarm –help<br>In the commands list you see everything from rotating the root certificate authority for a swarm to unlocking a locked swarm. The only command needed to start a new swarm is </p>
<p>$ docker swarm init<br>And that’s all that it takes to start running a single-host swarm. The output tells you that the current node is running as a swarm manager and provides a command for joining workers to the swarm. The value of the token argument is the worker join token. A similar looking token is used for joining manager’s to a swarm as seen from the join-token manager output</p>
<p>$ docker swarm join-token manager</p>
<p>Let’s probe around to see some of the changes that occur when you start running in swarm mode. First, let’s revisit the docker info output<br>$ docker info<br>The state has changed to active to indicate that the daemon is indeed running in swarm mode. There is also a bunch of useful tidbits related to the swarm’s configuration: Number of managers, number of nodes, right down to internals of the Raft consensus algorithm. You can even eek out additional information including TLS certificate info by using the format flag and specifying the Swarm field</p>
<p>$ docker info –format ‘‘<br>I’ll clear that because it is quite unsightly and there is no pretty print option.</p>
<p>We can also verify that the networks we learned in the swarm architecture lessons have been created<br>$ docker network ls<br>Here we see the docker_gwbridge local bridge network for connecting overlay networks to the hosts network and the ingress network used for handling external ingress traffic to the swarm.</p>
<p>That’s all there is to the single-node swarm. You could start using it for development and test scenarios as is. For demonstration purposes, I want to use a multi-node cluster so I will tear down our current swarm. To do that, you force leave the swarm<br>docker swarm leave –force<br>The force flag is required because when the last manager in a swarm leaves all the swarm state goes with it. This is what we want to happen in this case.</p>
<p>I’ll set up a multi-node swarm with two workers and one manager for demonstrating various swarm concepts. Remember that one manager is not a good idea in production, but it is going to be enough to illustrate working with a swarm mode cluster. To quickly create Docker-enabled VMs, I’m going to use docker-machine.<br>$docker-machine<br>docker-machine comes installed with Docker for Mac and Docker for Windows. Only a few docker-machine commands are needed so I’ll explain them as they are required. But know that there is a lot more to docker-machine than what I’ll explain in this lesson.</p>
<p>The first command is create, which does exactly what you’d expect. </p>
<p>$ docker-machine create vm1<br>By default it will create a VM in VirtualBox using an image with docker installed. Virtualbox was installed previously on my mac so everything went off without a hitch. I’m using the names vm1, vm2, and vm3 instead of more descriptive names like manger1 because it’s possible for nodes to change their role in a swarm. However, vm1 will be used as the manager in this lesson. I’ll speed this up until it finishes…<br>Now I’ll create vm2 in the same way</p>
<p>$ docker-machine create vm2<br>And finally vm3</p>
<p>$ docker-machine create vm3<br>Now I’ll use the ls command to list the vms and their IP addresses</p>
<p>$ docker-machine ls<br>The machines are at 192.168.99.100, 101, and 102. The VMs are running the 18.01 edge release which doesn’t have any significant changes in swarm mode compared to the 17.12 stable release I have running on my Mac. I’ll connect to vm1 using docker-machine’s ssh command</p>
<p>$ docker-machine ssh vm1<br>And I’ll show docker info to confirm that docker is installed but swarm mode is inactive</p>
<p>$ docker info<br>To initialize a swarm, I’ll use the same init command as with a single-node setup but with an advertise address:</p>
<p>$ docker swarm init –help<br>The advertise address is the IP address other nodes will use to join the swarm.</p>
<p>$ docker swarm init –advertise-addr 192.168.99.100<br>I’ll give the IP address but you could alternatively provide the network interface name. I’ll copy the prepared join command for joining workers. You can always retrieve the join token later using the join-token swarm subcommand.<br>I’ll drop out of vm1 and ssh into vm2 to join the swarm</p>
<p>$ exit</p>
<p>$ docker-machine ssh vm2</p>
<p>$ docker swarm join …<br>The output acknowledges that the node joined the swarm as a worker node. Now I’ll repeat the process for vm3.<br>To confirm the swarm has one manager and 3 nodes in total, I need to run the docker info command on the manager node, which is vm1. </p>
<p>$ docker info<br>There we have it, a 3-node swarm with one manager setup with the help of docker-machine.</p>
<p>In this lesson, we learned some of the options available for setting up a swarm mode cluster. These included some high touch options putting you in charge of the hardware and software patching to fully automated solutions like the one provided by Docker Cloud allowing you to spin up a swarm on Amazon Web Services or Azure from the comfort of a graphical interface.<br>We then saw how to set up a single node swarm using the swarm init command<br>After we set up a multi-node swarm with the help of docker-machine and Virtualbox. The same init command was used with an advertise address for other nodes to use to join the swarm using the join command.</p>
<p>This is a depiction of the swarm that we currently have set up. vm1, vm2, and vm3 are all in the swarm, while my mac is not participating in the swarm. vm1 is the swarm manager, as indicated by the orange tie. We’ll use this multi-node swarm for the remainder of the lessons.</p>
<h1 id="Managing-Nodes"><a href="#Managing-Nodes" class="headerlink" title="Managing Nodes"></a>Managing Nodes</h1><p>We now have a 3-node swarm with one manager. This lesson will demonstrate how to perform swarm node management tasks. For example, promoting a worker to a manager, organizing nodes with labels, and preventing manager’s from doing work.</p>
<p>Agenda<br>I’ll give a brief overview of the swarm node management tasks we’ll be going through. We’ll spend most of the time at the command-line where we’ll execute the tasks on the swarm we stood up.</p>
<p>Node Management<br>Promoting<br>The first task that we’ll go through is promoting a worker node to a manager. You may want to do this to increase your fault-tolerance or in order to take an existing manager out of service without impacting the number of managers available. Remember that if you are going for an increase in fault-tolerance that you should increase the manager count up to the next odd number. For example, going from one manager to three.</p>
<p>Demoting<br>Demoting is the opposite of promoting. It takes a node that is currently in a manager role and demotes the node to a worker role.</p>
<p>Availability<br>The availability of a node refers to the ability to schedule tasks to the node. It isn’t whether a node is up or down which one might logically guess. The availability of a node is configured by managers. The allowed availability states are: active, pause, and drain. Active means that work can be scheduled on a node, pause means no new work can be scheduled but existing work scheduled on the node won’t be canceled, and drain means nothing can be scheduled and any running work is terminated. Setting availability to drain is useful for gracefully taking a node offline to perform maintenance. Draining managers is also useful to prevent them from having work scheduled to them, which is the default behavior.</p>
<p>Labeling<br>The final node management task that we’ll demonstrate is labeling. Recall that labels are useful for influencing where services are placed in a swarm. For example, labels can be used to ensure that tasks are scheduled in different availability zones to provide service availability SLAs.</p>
<p>Demo<br>Now I’ll hop over to the command-line and start with the demo</p>
<p>I’m connected into the vm1 swarm node which is currently the manager of the swarm. In docker, node management tasks are accomplished by using the docker node management command.</p>
<p>$ docker node –help<br>There are some standard commands that are available for most docker management commands: namely ls for listing nodes, ps for listing tasks scheduled to nodes, and rm for removing nodes from a swarm. Inspect is also a familiar docker command that lists detailed information about a node. To get a view of the swarm, I’ll run the ls command</p>
<p>$ docker node ls<br>And we see the three nodes, that all three are available, and that only vm1 is a manager, and is therefore the leader.</p>
<p>To promote vm2 to the manager role in the swarm, I’ll use the promote command:<br>$ docker node promote vm2<br>And easy as that vm2 is now a manager in the cluster</p>
<p>$ docker node ls<br>The manager status of reachable means the node is a manager and is participating in the Raft consensus quorum. The other possible manager status is unavailable, which indicates the manager has a problem communicating with the other managers.</p>
<p>To change vm2 back to the worker role, I’ll use the demote command:<br>$ docker node demote vm2</p>
<p>$ docker node ls</p>
<p>Next up is modifying the availability of a node. You can use the update command for that<br>$ docker node update –help<br>The availability option does what we want. You can also see the label-add and label-rm options which add and remove labels from nodes. There’s also the role option which promote and demote are a short form of updating a node to the role of either worker or manager. Let’s say we don’t want the manager to have any tasks scheduled to it, so I’ll set the availability of vm1 to drain</p>
<p>$ docker node update –availability drain vm1</p>
<p>$ docker node ls<br>and the availability in the ls table reflects the change.</p>
<p>I actually want the manager to be able run tasks so I’ll undo that by setting availability to active<br>$ docker node update –availability active vm1</p>
<p>To finish up I’ll add a fictitious availability zone labels to each node using the label-add update option. I’ll say vm1 is in zone 1, vm2 is in zone 2, and vm3 is in zone 3:<br>$ docker node update –label-add zone&#x3D;1 vm1</p>
<p>$ docker node update –label-add zone&#x3D;2 vm2</p>
<p>$ docker node update –label-add zone&#x3D;3 vm3<br>To see node labels, you need to use the inspect command</p>
<p>$ docker node inspect vm3<br>Here is the labels property in the Spec. You can also filter out everything but the labels by using the format option with a Go template</p>
<p>$ docker node inspect -f ‘&amp;#123;&amp;#123;.Spec.Labels&amp;#125;&amp;#125;’ vm3<br>Here again is the zone label key-value pair in the Labels map.</p>
<p>Recap<br>In this lesson we learned about the node management tasks that are part of managing a swarm. We understood the concepts and demonstrated how to promote and demote a node, set a node’s availability, and label swarm nodes.</p>
<p>This slide shows the current state of our swarm. Each node is now labeled with a zone compared to where we began the lesson.</p>
<p>Closing<br>In the next lesson, we’ll see how to schedule tasks onto the swarm by using services. If you are ready to see swarm mode in action, continue on to the next Lesson.</p>
<h1 id="Managing-Services"><a href="#Managing-Services" class="headerlink" title="Managing Services"></a>Managing Services</h1><p>Services are what make up distributed applications running on a swarm. In this lesson, we’ll get experience running and managing services in our swarm.</p>
<p>Agenda<br>To begin, I’ll give a quick rundown of what services we’ll be running, and then we’ll get into the demo.</p>
<p>The Plan<br>I’ll use two images for demonstrating how to work with services.<br>The first is a swarm visualizer provided by Docker. It allows you to visualize the state of nodes in a swarm and see where service tasks have been scheduled. It requires information that only manager nodes have access to. We’ll constrain the placement of the service to make sure it gets what it needs.<br>The second is a web service that serves a simple web page that displays the name of the node running the task. This will give us a way to verify that requests are load balanced across multiple nodes when using the ingress network in swarm.</p>
<p>Demo<br>Ok, now let’s get to the demo.</p>
<p>When working with services, all of the commands are conveniently located under the docker service management command<br>$ docker service –help<br>There are some familiar commands: inspect, logs, ls, ps, and rm. They do what you would expect given your knowledge of the Docker CLI. We’ll use them as we work through this demo. We’ll give the rest more attention, starting with create.</p>
<p>$ docker service create –help | more<br>This is the equivalent of docker run for swarm services. There are too many options to go through. Several match docker run options and several others are unique to services. We’ll go through some of the unique ones in this lesson and save some for the next.</p>
<p>Let’s start by creating the swarm visualizer. The visualizer must run on managers, so we can use a constraint on the node role to handle that. For demonstration purposes, I’ll make the service global so that every manager will run one task for the service. The service could be load-balanced since the swarm state that the service visualizes is the same regardless of which manager you use. But I will publish the port using host mode so we can compare that to ingress mode. Ingress mode is the default, so the mode&#x3D;host part of the string must be provided. The mount option is required so that the service containers can access the manager node’s docker daemon socket. That is where it pulls the swarm state information from. Finally, we’ll give the service the name viz and specify the latest version of the dockersamples&#x2F;visualizer image.</p>
<p>$ docker service create <br>–constraint&#x3D;node.role&#x3D;&#x3D;manager <br>–mode global <br>–publish mode&#x3D;host,target&#x3D;8080,published&#x3D;8080 <br>–mount&#x3D;type&#x3D;bind,src&#x3D;&#x2F;var&#x2F;run&#x2F;docker.sock,dst&#x3D;&#x2F;var&#x2F;run&#x2F;docker.sock <br>–name&#x3D;viz <br>dockersamples&#x2F;visualizer</p>
<p>The commands can get pretty long and we’ll see how to bettern manage them in the next lesson. I’ll speed things up until it is finished. The service converged message lets us know that the actual state has converged to the desired state in the service spec. We can see the service spec using inspect<br>$ docker service inspect viz –pretty | more<br>This output shows some of the default values that were used, such as the update and rollback config. It also shows that the service mode is global and that the port has been published in host mode. We can use the ps command to confirm the actual state matches the desired state</p>
<p>$ docker service ps viz</p>
<p>Now let’s switch over to a web browser to see the swarm visualizer. The manager is running on vm1 which has an IP address of 192.168.99.100. The visualizer displays a column for each node. The node’s name, role, memory, operating system, and the hard to read text is the node labels we applied earlier. Tasks are shown with squares under each node’s heading. Currently there is only the one visualizer task. If we didn’t have any role constraint there would be one on every node, but because the service was constrained to managers, there is only one. Task borders are color-coded according to their service. Because the service published its port in host mode, I have to use the manager’s IP. If I try vm2’s IP, it won’t be able to reach the service. So I’ll go back to vm1’s IP address.</p>
<p>I’ll promote vm2 to be a manager and that will cause a viz replica to be started on vm2 because the viz service is global. I’ll use the ls command to see the change. Notice the replicas has jumped up to 2. After awhile there will be 2 of 2 tasks running and I can try again to access the visualizer on vm2 in the browser. There it is. Both vm1 and vm2 are manager’s and there are two replicas of the viz service shown. I’ll go back to vm1’s visualizer and demote vm2 back to a worker. Now we’re back to just one viz replica.</p>
<p>Let’s focus in on the 2nd service now. I’ll switch over to VS Code and quickly go through the source. This file, index.php, is doing going to echo back the node name which it gets from an environment variable. That’s all there is to it. Taking a look at the Dockerfile, the base image is an php image with the apache web server installed. The index.php source file is copied into the image and the web server serves it on port 80. There is also a healthcheck embedded in the image. You can of course override the image healthcheck or create a healthcheck if the image doesn’t have one when you create the service. This is the same behavior as with docker run.</p>
<p>Back to the command line. We’ll create the service with a constraint to not schedule any tasks in zone 1. The exclamation mark followed by equals means not equal to. We’ll declare 2 replicas. This implies the service mode is replicated and not global. replicated is also the default. The manager will try to spread the tasks over available nodes by default, but we will specify a placement preference to take control of how it spreads. Next, I’ll add an environment variable for the NODE_NAME and use a Go template to get the hostname of the node. Port 80 will be published in ingress mode by default making the service reachable from any node’s IP address regardless of if a task is running on the node. I’ll give the service the name nodenamer and specify version 1.0.0.</p>
<p>$ docker service create <br>–constraint node.labels.zone!&#x3D;1 <br>–replicas 2 <br>–placement-pref ‘spread&#x3D;node.labels.zone’ <br>-e NODE_NAME&#x3D;’&amp;#123;&amp;#123;.Node.Hostname&amp;#125;&amp;#125;’ <br>–publish 80:80 <br>–name nodenamer <br>lrakai&#x2F;nodenamer:1.0.0</p>
<p>Now the two tasks move through the preparing, and starting states to reach the running state before the service converges to the desired state. We can check on the swarm state in the visualizer. There are two tasks for the nodenamer service and they have a dark grey border. They are deployed evenly across all the zones except zone 1 as we constrained the placement. We can also test the ingress routing capabilities. I’ll send a request to port 80 on vm1 which isn’t running a task for the service. But the page loads thanks to the ingress routing mesh. if I reload a few times, you can see the node name changing. This illustrates the virtual IP load balancing of the service. If I send my requests to vm2, the behavior is the same.</p>
<p>Back at the command-line I can also access the service at localhost on vm1 through the ingress network.<br>$ curl localhost<br>The output doesn’t have new line so the command prompt gets tacked onto the end. Version 1.0.1 of nodenamer fixes this issue. We’ll do a rolling update to version 1.0.1. Before we do, we can inspect the service</p>
<p>$ docker service inspect nodenamer<br>and observe the setting for updates. Parallelism is one by default so only one task will be upgraded at a time. We won’t look at failed updates, so the only other relevant setting is update order which is only available in Docker 17.05 and above. It defaults to stopping existing tasks and then starting a new task. The other value is start first, which starts a new task first and then stop the old one when the new task is running. There’s also update delay which sets the delay between rolling updates. It defaults to zero which is not a problem in this case, we’ll still be able to see the update roll through because it takes some time for new tasks to get to running. Let’s do the update to version 1.0.1</p>
<p>$ docker service update –image lrakai&#x2F;nodenamer:1.0.1 nodenamer<br>And switch over the the visualizer to watch the update roll through. The old task on vm2 is taken down first, then a new task using the 1.0.1 image comes in. As soon as it reaches the running state, the task on vm3 is stopped and a new 1.0.1 image task starts.<br>Now if I curl localhost from vm1, the new line is added to the output.</p>
<p>We can scale up the service to, say 6 replicas using the scale command<br>$ docker service scale replicas&#x3D;6<br>and we will see them come up equally spread across the eligible nodes. Even though there are more than one task on each node there are no port conflicts. This wouldn’t be the case if host mode was used for publishing the service port.</p>
<p>Let’s set the parallelism for rollbacks to 2 so we can rollback faster than we update.<br>$ docker service update –rollback-parallelism 2<br>Then we intentionally update the service back to version 1.0.0 so we can finish the lesson with a rollback to version 1.0.1.</p>
<p>$ docker service update –image lrakai&#x2F;nodenamer:1.0.0 nodenamer<br>And we can see it roll through 1 task at a time.<br>Now we can rollback</p>
<p>$ docker service rollback nodenamer<br>and we should see two tasks at a time being rolled back. There we see it rolling back two at a time.</p>
<p>That’s it for this lesson. You now know how to put the theory we studied earlier into practice by using the docker service management command for managing services.</p>
<p>Closing<br>In the next lesson, we’ll cut down on the lengthy commands and improve the repeatability and maintainability of swarm service deployments by using stacks. Whenever you are ready, continue on to the next lesson.</p>
<h1 id="Working-with-Stacks"><a href="#Working-with-Stacks" class="headerlink" title="Working with Stacks"></a>Working with Stacks</h1><p>Stacks help you manage applications distributed across a swarm. In this lesson, we’ll get some practice working with stacks. I think you’ll find that it’s easy to get the hang of and better than writing sprawling, multi-line docker service create commands.</p>
<p>Agenda<br>We’ll start the lesson by introducing stacks and stack files. It won’t take very long because your prior experience with Docker Compose will serve you well here. Then we’ll get started with a demo. Our last demo of the course.</p>
<p>Stacks<br>In swarm mode, stacks are a group of related services that can be orchestrated and scaled together. It carries the same meaning as a software stack or technology stack in application development.</p>
<p>Stacks are declared using a Compose file. That makes it very easy to start using stacks given your Docker Compose experience. You can use stacks to manage services, networks, and volumes as you would with Docker Compose. For a compose file to work as a stack, you have to be using Compose file version 3 or greater. Although they are the same type of file, when you deploy an application declared in a compose file to a swarm, it’s referred to as a stack. By convention, the file name that is used to indicate a stack is docker-stack.yml.</p>
<p>A lot of the Docker Compose features you know and love work with stacks. These include the declarative configuration, an active community on Github, and the benefits of source-controlled configuration. Similar to Compose, when you deploy a stack a network is created by default to isolate the stack from other services.</p>
<p>However, there are some differences between using Docker Compose and stacks that you should be aware of. Currently, there are several configuration options that are ignored with stacks that you could use in Compose. Some of the most notable ones are build and depends_on. The Compose file reference documentation should be consulted as support is added&#x2F;removed for options over time. A link to the documentation is at the bottom of the transcript for this video.</p>
<p>On the flip side, what’s stacks can use that Compose can’t are mostly gathered under the deploy key. That is where you can specify options for node labels, replication mode, resource reservations, update configuration, and others. Currently, there isn’t support for configuration rollbacks in a stack file, and placement preferences and endpoint_mode for setting virtual IP or DNS round robin service discovery are only available in version 3.3 Compose files or above. Stack files also support swarm secrets by a top-level secrets key. We won’t get into the details but know that they are supported.</p>
<p>With previous experience in Compose, that’s all that we need to go over to start using stacks in swarm mode. We can get started with the demo which will reproduce what we did in the last lesson’s demo except using stacks.</p>
<p>Demo<br>I’ll start by removing the currently running services we deployed with docker service create, since we’re going to recreate them using a stack.<br>$ docker service rm viz nodenamer</p>
<p>Now let’s take a look at the stack file. I won’t dwell on it since it is a one-to-one mapping of the commands we entered to create the services before except with more structure and slightly different option names. The stack specific options are under the deploy keys. Because we have a placement preference, we need to use version 3.3 or higher. It is more pleasant to work with stacks compared to entering all the options at the command-line. We’ll also see next that commands for stacks can limit output to services declared in a stack without having to do any filtering.</p>
<p>The commands for working with stacks are organized under the docker stack management command<br>$ docker stack –help<br>ls lists the current stacks, services lists the services in a stack and ps lists all the service tasks for a given stacks. The deploy and rm commands are similar to docker-compose up and down. In fact, up and down are aliases for deploy and rm so you could use up and down with stacks if you prefer.</p>
<p>Let’s look at the deploy options<br>$ docker stack deploy –help<br>You have options for removing services that are no longer in a stack, controlling when images are resolved, and for passing along credentials if you are using images in a private Docker registry. The one required option is -c to specify a Compose file for the stack.</p>
<p>$ docker stack deploy -c docker-stack.yml demo<br>I’ll call the stack demo. We can see the stack has a network created automatically for the services in the stack. Of course, you can, and often should, exercise more control over the networks you want, just as you would in Docker Compose. The stack has finished deploying so we should see the visualizer on port 8080 of vm1, and there it is.</p>
<p>To do an update you use the same deploy command you used with the original stack deploy. Let’s make an update to the stack so there are 6 replicas, and let’s also put a resource reservation of half a cpu for each nodenamer replica. Each vm only has one cpu so there won’t enough cpu available for all 6 replicas. We’ll use this to verify that swarm respects the resource constraints. Re-run the deploy command<br>$ docker stack deploy -c docker-stack.yml demo<br>The configuration changes will be detected, and the swarm leader will bring the swarm to the new desired state. Let’s watch the visualizer to see what happens. We can see 2 new tasks starting on vm2 and then another on vm3. After all the tasks on vm2 are running one is stopped to respect the resource reservations. We can also see from listing the services in the stack that only 4 of 6 tasks for nodenamer are running while two are left pending.</p>
<p>That’s all for this demo and this lesson. We saw that working with stacks is a natural extension of Compose files and docker commands.</p>
<p>Closing<br>We’ve almost reached the finish line now! Join me for the final lesson when you’re ready to wrap up the course.</p>
<p><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/compose-file">Compose File Reference</a></p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Here we are the of this course about container orchestration with Docker swarm mode. I hope you it has been as fun for you as it was for me. We covered a lot of ground in this course. Let’s take a look back at what we learned.</p>
<p>Course Review<br>We started with a high-level overview of swarm mode and then dove into various components of the swarm mode architecture. The first was networking.<br>We learned about the overlay network type that makes networking swarm services distributed across multiple hosts extremely easy.<br>We saw how swarm accomplishes load balancing by virtual IP or by DNS round robin.<br>Finally, we saw swarm services can be accessed via any node when using ingress port publishing. That is possible thanks to the swarm mode routing mesh.</p>
<p>After that, we looked at the headline architecture component: container orchestration. In that lesson we saw how to influence the placement of services on the swarm,<br>and we also considered rolling updates and rollbacks.</p>
<p>The next architecture component was consistency. Swarm mode uses the raft consensus algorithm for guaranteeing a consistent state of the swarm. Managers participate in elections and elect a single leader responsible for making changes to bring the actual swarm state to the desired state.<br>We examined the fault-tolerance and performance tradeoff related to the number of managers in a swarm. You should consider using 3, 5, or 7 for production workloads.<br>Lastly, we discussed the raft logs where cluster state is persisted.</p>
<p>Security was the last architecture component we looked at.<br>Swarm mode has many security features enabled by default including public key infrastructure and token semantics for joining members to a swarm.<br>Data plane communication on overlay networks is optionally encrypted using IPSec.<br>We also learned how you can improve swarm security by taking control of keys used for decrypting raft logs through a feature called autolocking.</p>
<p>Then we started getting hands-on and learned about options for setting up swarms from single-node, to mutli-node, and from on-prem to in the cloud. We then created a single-node swarm and a multi-node swarm with the help of docker machine.</p>
<p>Following that, we discussed node management in swarm. We demonstrated routine tasks such as promoting, demoting, labeling, and setting a node’s availability to schedule jobs.</p>
<p>The next demo focused lesson was about service management. We saw how to create and update services using various configurations such as host and ingress routing, global and replicated services, resource and placement constraints, and updates and rollbacks.</p>
<p>In the last demo lesson, we explored using stacks for managing distributed applications in Docker swarm mode. Stacks are represented using Compose files and have swarm specific configuration under the deploy key.</p>
<p>Learning Outcomes<br>By taking this course, you have achieved the following learning outcomes. You are able to:<br>“ Describe what Docker swarm mode can accomplish<br>“ Explain the architecture of a swarm mode cluster<br>“ Use the Docker CLI to manage nodes in a swarm mode cluster<br>“ Use the Docker CLI to manage services in a swarm mode cluster<br>“ Deploy multi-service applications to a swarm using stacks</p>
<p>Learning More<br>There are a few places I can recommend for learning more. Try out some of the labs, quizzes or other courses on Cloud Academy. There is more content on swarm as well as other container orchestration tools in case you want to understand the entire ecosystem.<br><a target="_blank" rel="noopener" href="https://cloudacademy.com/">https://cloudacademy.com</a></p>
<p>The Docker Swarm Mode docs are a great place to learn all the intricacies of swarm mode and to stay on top of release changes.<br><a target="_blank" rel="noopener" href="https://cloudacademy.com/admin/clouda/videos/video/2103/change/https:/docs.docker.com/compose/">https://docs.docker.com/compose/</a></p>
<p>The Docker swarmkit GitHub repository is the ultimate place to learn about swarm mode from its source. See what features are coming next for swarm mode and learn from discussions around reported issues. Maybe even report your own or contribute to the code base.<br><a target="_blank" rel="noopener" href="https://github.com/docker/swarmkit">https://github.com/docker/swarmkit</a></p>
<p>Feedback<br>I’m happy to hear from you. I make content for you. If you have any feedback, please get in touch with me by leaving a comment on the Comments tab below the video, by emailing <a href="mailto:&#x73;&#x75;&#112;&#112;&#111;&#114;&#116;&#64;&#99;&#108;&#111;&#117;&#x64;&#97;&#99;&#97;&#100;&#101;&#109;&#121;&#x2e;&#99;&#x6f;&#x6d;">&#x73;&#x75;&#112;&#112;&#111;&#114;&#116;&#64;&#99;&#108;&#111;&#117;&#x64;&#97;&#99;&#97;&#100;&#101;&#109;&#121;&#x2e;&#99;&#x6f;&#x6d;</a>, or by connecting with me on Twitter where my handle is @LoganRakai.</p>
<p>Thank you<br>That does it for another course here on Cloud Academy. You have developed some serious Docker swarm mode skills and should be proud of your accomplishment. Now go and do me the ultimate service by putting what you’ve learned here into practice. Until next time, I’m Logan Rakai with Cloud Academy.</p>
<p>What’s that? All right, swarm twister, take us away!</p>
<h1 id="1Course-Introduction"><a href="#1Course-Introduction" class="headerlink" title="1Course Introduction"></a>1<strong>Course Introduction</strong></h1><p><a target="_blank" rel="noopener" href="https://www.virtualbox.org/wiki/Downloads">VirtualBox</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/docker-swarm-mode-training">Course resources on GitHub</a></p>
<h1 id="7Setting-Up-a-Swarm"><a href="#7Setting-Up-a-Swarm" class="headerlink" title="7Setting Up a Swarm"></a>7<strong>Setting Up a Swarm</strong></h1><p><a target="_blank" rel="noopener" href="https://gitlab.com/gitlab-org/ci-cd/docker-machine">Gitlab Fork</a></p>
<h1 id="10Working-with-Stacks"><a href="#10Working-with-Stacks" class="headerlink" title="10Working with Stacks"></a>10<strong>Working with Stacks</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/compose-file">Compose File Reference</a></p>
<h1 id="11Summary"><a href="#11Summary" class="headerlink" title="11Summary"></a>11<strong>Summary</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/">Docker Swarm Mode docs</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/docker/swarmkit">Docker Swarmkit GitHub repository</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Docker-Basics-Challenge-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Docker-Basics-Challenge-4/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Docker-Basics-Challenge-4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:31" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:31-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:25:42" itemprop="dateModified" datetime="2022-11-20T22:25:42-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Docker-Basics-Challenge-4/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Docker-Basics-Challenge-4/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Getting-Started-with-Docker-on-Linux-for-Azure-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Getting-Started-with-Docker-on-Linux-for-Azure-3/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Getting-Started-with-Docker-on-Linux-for-Azure-3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:30" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:30-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:25:08" itemprop="dateModified" datetime="2022-11-20T22:25:08-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Getting-Started-with-Docker-on-Linux-for-Azure-3/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Getting-Started-with-Docker-on-Linux-for-Azure-3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Docker-Playground-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Docker-Playground-2/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Docker-Playground-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:28" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:28-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:26:04" itemprop="dateModified" datetime="2022-11-20T22:26:04-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Docker-Playground-2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Docker-Playground-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/24/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><span class="page-number current">25</span><a class="page-number" href="/page/26/">26</a><span class="space">&hellip;</span><a class="page-number" href="/page/274/">274</a><a class="extend next" rel="next" href="/page/26/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
